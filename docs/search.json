[
  {
    "objectID": "expandmultinumeric.html",
    "href": "expandmultinumeric.html",
    "title": "Expand Multi Numeric and string columns",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: ExpandMultiNumeric.cs"
  },
  {
    "objectID": "expandmultinumeric.html#multi-numeric-columns",
    "href": "expandmultinumeric.html#multi-numeric-columns",
    "title": "Expand Multi Numeric and string columns",
    "section": "3.1 Multi-numeric columns",
    "text": "3.1 Multi-numeric columns\nSelected multi-numeric columns that should be expanded using the procedure mentioned in the description section above (default: no columns are selected)."
  },
  {
    "objectID": "expandmultinumeric.html#text-columns",
    "href": "expandmultinumeric.html#text-columns",
    "title": "Expand Multi Numeric and string columns",
    "section": "3.2 Text columns",
    "text": "3.2 Text columns\nSelected text columns that should be expanded using the procedure mentioned in the description section above (default: no columns are selected)."
  },
  {
    "objectID": "rank.html",
    "href": "rank.html",
    "title": "Rank",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: Rank.cs"
  },
  {
    "objectID": "rank.html#matrix-access",
    "href": "rank.html#matrix-access",
    "title": "Rank",
    "section": "3.1 Matrix access",
    "text": "3.1 Matrix access\nDefines whether the values in expression columns or rows should be ranked (default: Rows)."
  },
  {
    "objectID": "multiplesampletestprocessing.html",
    "href": "multiplesampletestprocessing.html",
    "title": "Multiple-samples tests",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Tests\nSource code: not public."
  },
  {
    "objectID": "multiplesampletestprocessing.html#grouping",
    "href": "multiplesampletestprocessing.html#grouping",
    "title": "Multiple-samples tests",
    "section": "3.1 Grouping",
    "text": "3.1 Grouping\nSelected categorical row that defines the grouping of columns that should be used in the test (default: first categorical row in the matrix)."
  },
  {
    "objectID": "multiplesampletestprocessing.html#test",
    "href": "multiplesampletestprocessing.html#test",
    "title": "Multiple-samples tests",
    "section": "3.2 Test",
    "text": "3.2 Test\nDefines what kind of test should be applied (default: ANOVA). The test can be selected from a predefined list:\n\nANOVA\nKruskal Wallis\n\n\n3.2.1 S0\nArtificial within groups variance (default: 0). It controls the relative importance of t-test p-value and difference between means. At \\(s0=0\\) only the p-value matters, while at nonzero s0 also the difference of means plays a role. See (Tusher, Tibshirani, and Chu 2001) for details."
  },
  {
    "objectID": "multiplesampletestprocessing.html#use-for-truncation",
    "href": "multiplesampletestprocessing.html#use-for-truncation",
    "title": "Multiple-samples tests",
    "section": "3.3 Use for truncation",
    "text": "3.3 Use for truncation\nDefines on what value the truncation is based on (default: Permutation-based FDR). Choose here whether the truncation should be based on the p-values, on permutation-based FDR-values or, if the Benjamini-Hochberg correction for multiple hypothesis testing should be applied.\n\n3.3.1 Threshold p-value\nThis parameter is just relevant, if the parameter “Use for truncation” is set to “P-value”. Rows with a test result below this value are reported as significant (default: 0.05).\n\n\n3.3.2 FDR\nThis parameter is just relevant, if the parameter “Use for truncation” is set to “Benjamini-Hochberg FDR” or “Permutation-based FDR”. Rows with a test result below this value are reported as significant (default: 0.05).\n\n\n3.3.3 Number of randomizations\nSpecifies the number of randomizations that should be applied (default: 250).\n\n\n3.3.4 Preserve grouping in randomizations\nDefines, whether the grouping specified in a categorical row should be preserved in the randomizations (default: &lt;None&gt;). It can be selected from a list including all available groupings of the matrix."
  },
  {
    "objectID": "multiplesampletestprocessing.html#log10",
    "href": "multiplesampletestprocessing.html#log10",
    "title": "Multiple-samples tests",
    "section": "3.4 Log10",
    "text": "3.4 Log10\nIf checked, \\(-Log_{10}(test\\ value)\\) is reported in the output matrix (default). Otherwise the test-value is reported."
  },
  {
    "objectID": "multiplesampletestprocessing.html#suffix",
    "href": "multiplesampletestprocessing.html#suffix",
    "title": "Multiple-samples tests",
    "section": "3.5 Suffix",
    "text": "3.5 Suffix\nThe entered suffix will be attached to newly generated columns (default: empty). That way columns from multiple runs of the test can be distinguished more easily."
  },
  {
    "objectID": "addannotationtomatrix.html",
    "href": "addannotationtomatrix.html",
    "title": "Add annotation",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. columns\nSource code: AddAnnotationToMatrix.cs"
  },
  {
    "objectID": "addannotationtomatrix.html#source",
    "href": "addannotationtomatrix.html#source",
    "title": "Add annotation",
    "section": "3.1 Source",
    "text": "3.1 Source\nSpecified path to the file containing the annotations that should be added to the matrix (default: first file in Perseus-version /conf/annotations/). The file can be selected from all files in Perseus-version /conf/annotations/.\n\n3.1.1 ENSG column\nSelected text column that contains the base identifier, which are going to be matched to the annotations (default: first text column in the matrix).\n\n\n3.1.2 Annotations to be added\nSelected annotations that should be added (default: no annotations are selected). The annotations can be selected from a list of annotations defined by the tab separated columns in the file specified in “Source”."
  },
  {
    "objectID": "addannotationtomatrix.html#additional-sources",
    "href": "addannotationtomatrix.html#additional-sources",
    "title": "Add annotation",
    "section": "3.2 Additional sources",
    "text": "3.2 Additional sources\nSelected files containing the annotations that should additionally be added to the matrix (default: no files are selected). Additional sources can be selected from all files in Perseus-version conf/annotations."
  },
  {
    "objectID": "andromeda_tutorial.html",
    "href": "andromeda_tutorial.html",
    "title": "Andromeda Tutorial",
    "section": "",
    "text": "Tutorials explaining how to use the Andromeda search tool can be found in our YouTube channel.\nA few examples are listed below:\nConfiguration of the Andromeda search engine\nFrom our online MaxQunat Summer in 2021\n\nAndromeda search engine\nFrom our 2018 MaxQunat Summer in Madison\n\nTutorial - Configuration of the Andromeda search engine\nFrom our 2018 MaxQunat Summer in Barcelona"
  },
  {
    "objectID": "dehyphenateids.html",
    "href": "dehyphenateids.html",
    "title": "De-hyphenate ids",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: DeHyphenateIds.cs"
  },
  {
    "objectID": "dehyphenateids.html#id-column",
    "href": "dehyphenateids.html#id-column",
    "title": "De-hyphenate ids",
    "section": "3.1 Id column",
    "text": "3.1 Id column\nSelected text column containing the IDs, where the hyphens should be removed (default: first text column in the matrix)."
  },
  {
    "objectID": "managecategoricalannotrow.html",
    "href": "managecategoricalannotrow.html",
    "title": "Categorical annotation rows",
    "section": "",
    "text": "1 General =====\n\nType: - Matrix Processing\nHeading: - Annot. rows\nSource code: ManageCategoricalAnnotRow.cs\n\n\n\n2 Brief description\nManage the categorical annotation rows. One important applications is to define a grouping that is later used in a t-test or ANOVA.\nOutput: Same matrix with categorical annotation rows added or modified."
  },
  {
    "objectID": "addnoise.html",
    "href": "addnoise.html",
    "title": "Add Noise",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: AddNoise.cs"
  },
  {
    "objectID": "addnoise.html#standard-deviation",
    "href": "addnoise.html#standard-deviation",
    "title": "Add Noise",
    "section": "3.1 Standard deviation",
    "text": "3.1 Standard deviation\nDefines the standard deviation of the noise distribution that will be added (default: 0.1)."
  },
  {
    "objectID": "sortbycolumn.html",
    "href": "sortbycolumn.html",
    "title": "Sort by Column",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: SortByColumn.cs"
  },
  {
    "objectID": "sortbycolumn.html#column",
    "href": "sortbycolumn.html#column",
    "title": "Sort by Column",
    "section": "3.1 Column",
    "text": "3.1 Column\nSelected expression/numerical column, whose values should be sorted (default: first expression column in the matrix)."
  },
  {
    "objectID": "sortbycolumn.html#descending",
    "href": "sortbycolumn.html#descending",
    "title": "Sort by Column",
    "section": "3.2 Descending",
    "text": "3.2 Descending\nIf checked, the matrix is sorted in descending order (largest to smallest value) by the defined expression/numerical column (default: unchecked). By default the matrix is sorted in ascending order (smallest to largest value) by the defined expression/numerical column."
  },
  {
    "objectID": "transpose.html",
    "href": "transpose.html",
    "title": "Transpose",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: Transpose.cs"
  },
  {
    "objectID": "transpose.html#new-column-names",
    "href": "transpose.html#new-column-names",
    "title": "Transpose",
    "section": "3.1 New column names",
    "text": "3.1 New column names\nSelected text column that specifies the new column names of the transposed matrix (default: first text column in the matrix)."
  },
  {
    "objectID": "filternumericalcolumn.html",
    "href": "filternumericalcolumn.html",
    "title": "Filter rows based on numerical/expression column",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Filter rows\nSource code: FilterNumericalColumn.cs"
  },
  {
    "objectID": "filternumericalcolumn.html#number-of-columns",
    "href": "filternumericalcolumn.html#number-of-columns",
    "title": "Filter rows based on numerical/expression column",
    "section": "3.1 Number of columns",
    "text": "3.1 Number of columns\nThe filtering is based on relations of expression/numerical columns. Up to five columns (default: 1) can be selected. Depending on the number of chosen columns drop down box(es) appear on the pop-up window named “x”, “y”, “z”, “a” and “b”. In these drop down boxes expression/numerical columns can be specified, which can then be used in the relations that should be applied for filtering the matrix."
  },
  {
    "objectID": "filternumericalcolumn.html#number-of-relations",
    "href": "filternumericalcolumn.html#number-of-relations",
    "title": "Filter rows based on numerical/expression column",
    "section": "3.2 Number of relations",
    "text": "3.2 Number of relations\nUp to five relations using the previously specified columns can be included in the filtering process (default:1). Depending on the selected number of relations text fields on the pop-up window appear named “Relation 1”, “Relation 2”, “Relation 3”, “Relation 4” and “Relation 5”.\nIn each text field a relation for the filtering process can be defined using the variables of the parameter “Number of columns”. For the relations numbers with “.” as decimal point, “+”, “-”, “*”, “/” and “^” as well as scientific notation (e.g. “5.4e-12”) can be used."
  },
  {
    "objectID": "filternumericalcolumn.html#combine-through",
    "href": "filternumericalcolumn.html#combine-through",
    "title": "Filter rows based on numerical/expression column",
    "section": "3.3 Combine through",
    "text": "3.3 Combine through\nDefines how the specified relations are combined (default: intersection). Depending on the specified combination mode either rows, which fulfill the “intersection” (default) of the relations are kept or the ones fulfilling the “union”."
  },
  {
    "objectID": "filternumericalcolumn.html#filter-mode",
    "href": "filternumericalcolumn.html#filter-mode",
    "title": "Filter rows based on numerical/expression column",
    "section": "3.4 Filter mode",
    "text": "3.4 Filter mode\nThe “Filter mode” defines, whether the input matrix will be reduced (“Reduce matrix” = default) or a new categorical column called “Filter” will be generated containing the categories “Keep” and “Discard” (“Filter mode” = “Add categorical column”)."
  },
  {
    "objectID": "searchtextcolumns.html",
    "href": "searchtextcolumns.html",
    "title": "Search text column",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: SearchTextColumns.cs"
  },
  {
    "objectID": "searchtextcolumns.html#find-what",
    "href": "searchtextcolumns.html#find-what",
    "title": "Search text column",
    "section": "3.1 Find what",
    "text": "3.1 Find what\nSpecified text/term that is searched in a selected text column (default: empty)."
  },
  {
    "objectID": "searchtextcolumns.html#look-in",
    "href": "searchtextcolumns.html#look-in",
    "title": "Search text column",
    "section": "3.2 Look in",
    "text": "3.2 Look in\nSelected text column that should be searched for the specified term (default: first text column of the matrix)."
  },
  {
    "objectID": "searchtextcolumns.html#match-case",
    "href": "searchtextcolumns.html#match-case",
    "title": "Search text column",
    "section": "3.3 Match case",
    "text": "3.3 Match case\nThe cells of the text column will be searched for a matching substring (default: checked). The results will be in a new generated categorical column called “Search: original column name”. “+” indicates, whether a match was successful."
  },
  {
    "objectID": "searchtextcolumns.html#match-whole-word",
    "href": "searchtextcolumns.html#match-whole-word",
    "title": "Search text column",
    "section": "3.4 Match whole word",
    "text": "3.4 Match whole word\nThe cells of the text column will be searched to match the whole word of the specified term (default: unchecked). The results will be in a new generated categorical column called “Search: original column name”. “+” indicates, whether a match was successful."
  },
  {
    "objectID": "classificationprocessing.html",
    "href": "classificationprocessing.html",
    "title": "Classification (cross-validation and prediction",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Learning\nSource code: not public."
  },
  {
    "objectID": "classificationprocessing.html#items-are-in",
    "href": "classificationprocessing.html#items-are-in",
    "title": "Classification (cross-validation and prediction",
    "section": "3.1 Items are in",
    "text": "3.1 Items are in\nIt specifies if the items that should be used for the cross-validation or the prediction can be found in “Columns” or “Rows” (default: Columns).\n\n3.1.1 Classes\nSelected categorical row or column that contains the class of the items (default: first categorical row/column in the matrix). If items are in columns then the classes are in a categorical row, and if items are in rows the classes are in a categorical column.\n\n\n3.1.2 Sub-classes\nThis parameter is just relevant, if the parameter “Items are in” is set to “Columns”. It specifies whether an additional grouping should be taken into consideration for the cross-validation process (default: &lt;None&gt;). This could for instance be technical replicates.\n\n\n3.1.3 Feature selection\nDefines whether feature selection should be applied by ranking and reducing the features before the classification process (default: None).\n\n\n3.1.4 Feature ranking method\nThis parameter is just relevant, if the parameter “Feature selection” is set to “From feature ranking”. It specifies which features method will be used to rank the features (default: ANOVA). The method can be selected from a predefined list:\n\nANOVA\nHybrid SVM\nMANOVA\nOne-sided t-test\nTwo-way ANOVA\nSVM\nRFE-SVM\nGolub\n\nDepending on the ranking method up to 4 parameters can be specified.\n\n3.1.4.1 S0\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “ANOVA”, “Hybrid SVM”, “One-sided t-test” or “MANOVA”. It defines the artificial within groups variance and controls the relative importance of resulted test p-values and difference between means (default: 0). At \\(s0=0\\) only the p-value matters, while at nonzero \\(s0\\) also the difference of means plays a role. See (Tusher, Tibshirani, and Chu 2001) for details.\n\n\n3.1.4.2 C\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Hybrid SVM”, “SVM” or “RFE-SVM”. C is a penalty constant (default: 100). Large C corresponds to large penalties for misclassification and resembles a hard margin classifier.\n\n\n3.1.4.3 Reduction factor\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Hybrid SVM” or “RFE-SVM”. It defines the factor by what the number of features will be reduced step by step during the ranking process (default: 1.414).\n\n\n3.1.4.4 Number of top ANOVA features\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “MANOVA”. It defines how many of the selected features are top ANOVA features.\n\n\n3.1.4.5 Side\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “One-sided t-test”. It defines the “Left” or “Right” side, where the null hypothesis can be rejected (default: Right).\n\n\n3.1.4.6 Orthogonal grouping\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. It defines the grouping of the data according to a given categorical column or row to distinguish the effects of the groups.\n\n\n3.1.4.7 Min. orthogonal p-value\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. Test results above this p-value are defined as orthogonal (default: 0).\n\n\n3.1.4.8 Min. interaction p-value\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. Test results above this p-value are defined as interacting, hence the effects of one group do not depend on the other group (default: 0).\n\n\n3.1.4.9 Skip if orthog. P-value is better\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. It defines whether features with an orthogonal p-value better than the given value in “Min. interaction p-value” are filtered out (default: unchecked).\n\n\n3.1.4.10 Number of features\nDefines how many features should be selected (default: 100).\n\n\n3.1.4.11 Group-wise feature sel.\nIf checked, for each defined group in the data a different amount of features can be selected, which are then used for the classification (default: unchecked). The numbers can be defined either by typing in the text field in the form \\([Group, number]\\) or by using the Edit button."
  },
  {
    "objectID": "classificationprocessing.html#classification-algorithm",
    "href": "classificationprocessing.html#classification-algorithm",
    "title": "Classification (cross-validation and prediction",
    "section": "3.2 Classification algorithm",
    "text": "3.2 Classification algorithm\nDefines the algorithm that should be used for the classification (default: Support vector machine). The algorithm can be selected from a predefined list:\n\nSupport vector machine\nFisher LDA\nKNN\n\n\n3.2.1 Kernel\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “Support vector machine”. It defines the kernel function that is used to classify items (default: linear). The kernel function can be selected from a predefined list:\n\nLinear:  K(x,y) = x^Ty \nRBF:  K(x,y) = (-|x-y|^2) , &gt; 0 \nPolynomial:  K(x,y) = (x^Ty + r)^d , &gt; 0 \nSigmoid:  K(x,y) = tanh(x^Ty + r) \n\nDepending on the chosen function 1 to 4 parameters must be specified.\n\n3.2.1.1 Sigma\nThis parameter is just relevant, if “Kernel” is set to “RBF”. It defines the slope of the function (see formula above, default: 1).\n\n\n3.2.1.2 Degree\nThis parameter is just relevant, if “Kernel” is set to “Polynomial”. It defines the degree of the polynom (see formula above, default: 3).\n\n\n3.2.1.3 Gamma\nThis parameter is just relevant, if “Kernel” is set to “Polynomial” or “Sigmoid”. It defines the slope of the function (see formula above, default: 0.01).\n\n\n3.2.1.4 Coef\nThis parameter is just relevant, if “Kernel” is set to “Polynomial” or “Sigmoid”. It defines a constant (see formula above, default: 0).\n\n\n3.2.1.5 C\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “Support vector machine”. C is a penalty constant (default: 10). Large C corresponds to large penalties for misclassification and resembles a hard margin classifier.\n\n\n\n3.2.2 Distance\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “KNN”. It defines the selected distance that will be used to assign the nearest neighbours to an item and therefore classify it (default: Euclidean). The distance can be selected from a predefined list:\n\nEuclidean\nL1\nMaximum\nLp\nPearson correlation\nSpearman correlation\nCosine\nCanberra\n\n\n\n3.2.3 Number of neighbours\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “KNN”. It specifies the number of closest neighbours that are taken into account for the classification of an item (default: 5)."
  },
  {
    "objectID": "classificationprocessing.html#cross-validate-assigned-items",
    "href": "classificationprocessing.html#cross-validate-assigned-items",
    "title": "Classification (cross-validation and prediction",
    "section": "3.3 Cross-validate assigned items",
    "text": "3.3 Cross-validate assigned items\nIf checked, cross-validation is applied to items that are already assigned to a class (default: checked)."
  },
  {
    "objectID": "classificationprocessing.html#cross-validation-type",
    "href": "classificationprocessing.html#cross-validation-type",
    "title": "Classification (cross-validation and prediction",
    "section": "3.4 Cross-validation type",
    "text": "3.4 Cross-validation type\nThis parameter is just relevant, if the parameter “Cross-validate assigned items” is checked. It defines the type of cross-validation that should be applied to the data set (default: n-fold). The type can be selected from a predefined list:\n\n//Leave one out:// As many predictors are built as there are items in the data set. Thus for each predictor one item is left out to train the model and the predictor will be evaluated using the left out item. In the end the average prediction performance will be returned.\n//n-fold:// The items of the data set are split into n equally sized chunks. n predictors will be generated. In each of these prediction models the union of n-1 of these chunks are taken as the training set and the remaining chunk is the test set. In the end the average prediction performance will be returned.\n//Random sampling:// The number of predictors is specified by the “Number of repeats” parameter. The number of items taken out to form the test set (and not used for building the predictor) is specified by the “Test set percentage” parameter. In the end the average prediction performance will be returned.\n\nDepending on the cross-validation type 0 to 2 parameters have to specified.\n\n3.4.1 n\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “n-fold”. It defines the number of partitions the data is divided into (default: 4).\n\n\n3.4.2 Test set percentage\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “Random sampling”. It specifies the percentage of the data that is used for testing the trained model (default: 15). The remaining data is used for the training process.\n\n\n3.4.3 Number of repeats\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “Random sampling”. It specifies how often the cross-validation process is repeated (default: 250). In every round the data is again divided according to the previously defined percentage."
  },
  {
    "objectID": "classificationprocessing.html#predict-unassigned-items",
    "href": "classificationprocessing.html#predict-unassigned-items",
    "title": "Classification (cross-validation and prediction",
    "section": "3.5 Predict unassigned items",
    "text": "3.5 Predict unassigned items\nIf checked, unassigned items in the data are predicted using the trained model, which is based on the assigned items (default: checked)."
  },
  {
    "objectID": "classificationprocessing.html#number-of-threads",
    "href": "classificationprocessing.html#number-of-threads",
    "title": "Classification (cross-validation and prediction",
    "section": "3.6 Number of threads",
    "text": "3.6 Number of threads\nDefines the number of threads that should be used for the process (default: 1). The number of threads is limited by number of available cores of the machine Perseus in running on."
  },
  {
    "objectID": "classificationprocessing.html#support-vector-machines",
    "href": "classificationprocessing.html#support-vector-machines",
    "title": "Classification (cross-validation and prediction",
    "section": "5.1 Support vector machines",
    "text": "5.1 Support vector machines\nSupport vector machines (//SVMs//) were largely developed in the 1990s by Vapnik and co-workers on a basis of a separable bipartition problem at the AT & T Bell Laboratories (see S. B. Kotsiantis [[http://citeseer.uark.edu:8080/citeseerx/viewdoc/summary?doi=10.1.1.95.9683|Supervised Machine Learning: A Review of Classification Techniques]]). //SVMs// are a family of data analysis algorithms, based on convex quadratic programming, whose successful use has been demonstrated in classification, regression and clustering problems. Thus, //SVMs// are now the state-of-the-art tools for non-linear input-output knowledge. The following section covers a brief and basic description of //SVMs//, but detailed explanations can be found in V. N. Vapniks [[http://books.google.de/books?hl=de&lr=&id=sna9BaxVbj8C&oi=fnd&pg=PR7&dq=The+nature+of+statistical+learning&ots=ooHfJTilf7&sig=3RFGX9DS8mBTpceDxV-H7UJOhfw#v=onepage&q=The%20nature%20of%20statistical%20learning&f=false|The nature of statistical learning]], N. Cristianinis and J. Shawe-Taylors [[http://books.google.de/books?hl=de&lr=&id=_PXJn_cxv0AC&oi=fnd&pg=PR9&dq=An+introduction+to+Support+Vector+Machines:+and+other+kernel-based+learning+methods&ots=xRNl4BXoXe&sig=isDnY5NnZWQNOccYO1C1z5c2o10#v=onepage&q=An%20introduction%20to%20Support%20Vector%20Machines%3A%20and%20other%20kernel-based%20learning%20methods&f=false|An introduction to support vector machines and other kernel-based learning methods]], V. N. Vapniks [[http://read.pudn.com/downloads161/ebook/733192/Statistical-Learning-Theory.pdf|Statistical Learning Theory]], V. N. Vapniks [[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=788640|An overview of statistical learning theory]] and B. E. Bosers, I. M. Guyons, and V. N. Vapniks [[http://dl.acm.org/citation.cfm?doid=130385.130401|A training algorithm for optimal margin classifiers]].\n//SVMs// are a particular class of supervised learning methods that are well suited for analyses of data in high-dimensional feature spaces. They are computationally efficient and capable of detecting biologically-relevant signals. //SVMs// revolve around the notion of a //margin// - either side of a data separating linear decision boundary (//hyperplane//). Maximizing this //margin// and thereby creating the largest distance between two classes as well as between the //hyperplane// and the instances on either side, is the main task in training //SVMs// (see figure below). Thus, these models have a binary nature to separate classes, but can be extended to multi-class problems by reducing the problem to a set of multiple binary classification problems. The //hyperplane// is defined by:   D(x)  =  &lt;,x&gt;  +  b   where //ω// is the weights vector and //b// is a bias value (or //−b// the threshold).\nIn case an optimal separating //hyperplane// is found, data points on the //margin// are known as //support vectors// and the solution is a linear combination of them (red data points in figure below). Each new data point is then classified according to its optimal position relative to the model’s //hyperplane//. So the model complexity is unaffected by the number of features encountered in the training data, therefore //SVMs// are well suited to deal with learning tasks with a large number of features compared to the number of data points. In case no //hyperplane// can be found, the problem can be addressed using the so-called //soft margin//. The //margin// optimization constraints can be relaxed by allowing some misclassifications or //margin// violations in the training set, to get better generalization of the //SVM// than using a //hard margin//. The choice of appropriate penalties is mandatory:   \\[\\begin{align}\nmin_{w,b,\\xi} ~& \\frac{1}{2} \\ w^{T}w \\ + \\ C\\sum_{i=1}^{l}\\xi_{i} \\\\\n\\text{subject to} ~& y_{i}(w^{T}x_{i}+b) \\ &lt; \\ 1-\\xi_{i} \\ \\ \\text{and} \\ \\ \\xi \\geq 0\n\\end{align}\\]   where //ω// is the weights vector, //b// is a bias value, //C// is a penalty constant, and //ξ// is a slack variable, which is the orthogonal distance between a data point and the //hyperplane//. Large //C// correspond to large penalties for misclassification and resemble a //hard margin// classifier, whereas //ξ// measures the degree of misclassification or //margin// violation. This is a good way to deal with outliers in the data set without destroying the model by tailoring it perfectly to the input data.\nNevertheless, most real-world data sets involve separation problems that are linearly non-separable, which requires the definition of complex functions to build a good classifier. //SVMs// use kernels, a special class of functions to deal with such situations. Mapping the data points to a higher-dimensional space (transformed feature space) using kernels, enables the definition of a linear //hyperplane//, which results in a non-linear //hyperplane// in the original space. The //hyperplanes// in the higher-dimensional space are represented by all points defining a set, whose inner product with a vector is constant in that space. Training the classifier depends only on the data through dot products, which are possible to compute even at a high-dimension at low cost by applying the so-called //kernel trick//. The trick lies in working in an higher-dimensional space, without ever explicitly transforming the original data points into that space, but instead relying on algorithms that only need to compute inner products within that space. These algorithms are identical to kernels and can thus be cheaply computed in the original space. So, everything about linear cases can also be applied to non-linear ones using an appropriate kernel function. It is common practice to find the best suiting function by cross-validation. Some popular kernels, which are all included in Perseus, are:   \\[\\begin{align}\n\\text{linear:} \\ K(x,y)         &= x^{T}y  \\\\\n\\text{sigmoid:} \\ K(x,y)    &= tanh(\\gamma x^{T}y \\ + \\ r) \\\\\n\\text{radial basis:} \\ K(x,y)   &= \\exp(-\\gamma|x \\ - \\ y|^{2}) , \\ \\gamma &gt; 0 \\\\\n\\text{polynomial:} \\ K(x,y)     &= (\\gamma x^{T}y \\ + \\ r)^{d}, \\ \\gamma &gt; 0\n\\end{align}\\]   where //x// and //y// are two data points, //γ// is the slope, //d// is the degree of the polynom, and //r// is a constant.\n{{ perseus:user:activities:matrixprocessing:learning:svm.png?direct |}} Illustration of separating two classes using SVMs. Linear (A.) and non-linear (B.) perfect separation of two classes (green and orange) with a hyperplane (black) and maximal margin (blue and dotted gray lines). Support vectors defining the hyperplane are in red. No misclassifiactions or margin violations are included.\nFor more information you can also consult [[http://en.wikipedia.org/wiki/Support_vector_machine|Wikipedia]].\n\\ \\ \\\n==== Fisher’s linear discriminant analysis ==== Linear Discriminant Analysis (LDA), is a well-known classification technique that has been used successfully in many statistical pattern recognition problems. It was developed by R. A. Fisher, a professor of statistics at University College London, and is sometimes called Fisher Discriminant Analysis (FDA). Its first description was in 1936 and can be found in [[http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/abstract;jsessionid=B987772120350C4E7E8866F341404E7B.f01t01|The use of multiple measurements in taxonomic problems]].\nThe primary purpose of //LDA// is to separate samples of two or multiple distinct groups while preserving as much of the class discriminatory information as possible to classify new unseen instances. The approach of the //LDA // is to project all the data points into new space, normally of lower dimension, which maximizes the between-class separability while minimizing their within-class variability. So the goal is to find the best projection axes for separating the classes. In general the number of axes that can be computed by the //LDA// method is one less than the number of classes in the problem.\n{{ perseus:user:activities:matrixprocessing:learning:lda2.png?direct |}} Illustration of separating two classes using LDA. Classes are separated perfectly and the dimensionality of the problem has been reduced from two features (x1,x2) to only a scalar value y.\nFor more information you can also consult [[http://en.wikipedia.org/wiki/Linear_discriminant_analysis|Wikipedia]].\n\\ \\ \\ ==== k-nearest neighbors ==== K-Nearest Neighbors (kNN) is a simple //lazy learner// algorithm that stores all available data points and classifies new instances based on a similarity measure (e.g., distance functions). It corresponds to the group of supervised learning algorithms and has been used in statistical estimation and pattern recognition already in the beginning of 1970’s as a non-parametric technique. During the training phase the algorithm simply stores the data points including their class labels and all computation is deferred until the classification process. So //kNN// is based on the principle that instances that are in close proximity to another have similar properties. Thus, to classify new unclassified instances, one simply has to look at their k-nearest neighbors, to figure out the classification label. The class membership can be defined by a majority vote of the //k// closest neighbors or the neighbors can be ranked and weighted according to their distance to the new instance. A common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\n{{ perseus:user:activities:matrixprocessing:learning:knn.png?direct&300 |}} Illustration of classifying a new item using kNN. Using a majority vote of the k nearest neighbors, the defined k can change the assigned class of the red star. If k = 3 (purple circle) the star corresponds to the blue polygon class, because the three closest neighbors include two blue polygons and one green rectangle. Whereas, if k = 5 (black circle) the star is assigned to the green class, because the five closest neighbors include more green rectangles than blue polygons (three green rectangles vs. two blue polygons).\nFor more information you can also consult [[http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm|Wikipedia]]."
  },
  {
    "objectID": "multiscatteranalysis.html",
    "href": "multiscatteranalysis.html",
    "title": "Multi scatter plot",
    "section": "",
    "text": "Type: - Matrix Analysis\nHeading: - Visualization\nSource code: not public."
  },
  {
    "objectID": "multiscatteranalysis.html#rows",
    "href": "multiscatteranalysis.html#rows",
    "title": "Multi scatter plot",
    "section": "3.1 Rows",
    "text": "3.1 Rows\nFirst partner(s) of the respective multi scatter plot(s). Selected expression columns that will appear in y-direction in the resulting multi scatter plot. (default: all expression columns are selected)."
  },
  {
    "objectID": "multiscatteranalysis.html#columns",
    "href": "multiscatteranalysis.html#columns",
    "title": "Multi scatter plot",
    "section": "3.2 Columns",
    "text": "3.2 Columns\nSecond partner(s) of the respective multi scatter plot(s). Selected expression columns that will appear in x-direction in the resulting multi scatter plot. (default: all expression columns are selected)."
  },
  {
    "objectID": "volcanoplotanalysis.html",
    "href": "volcanoplotanalysis.html",
    "title": "Volcano plot",
    "section": "",
    "text": "Type: - Matrix Analysis\nHeading: - Misc. (Analysis)\nSource code: not public."
  },
  {
    "objectID": "volcanoplotanalysis.html#grouping",
    "href": "volcanoplotanalysis.html#grouping",
    "title": "Volcano plot",
    "section": "3.1 Grouping",
    "text": "3.1 Grouping\nSelected categorical row that defines the grouping of columns that should be used in the specified “Test” (default: first categorical row in the matrix).\n\n3.1.1 First Group (right)\nFirst selected group that should be used for the specified “Test” (default: first group of the previously selected categorical row in the matrix). The group can be selected from all defined groups of the categorical row defined in “Grouping”.\n\n\n3.1.2 Second Group (left)\nSecond selected group that should be used for the specified “Test” (default: first group of the previously selected categorical row in the matrix). The group can be selected from all defined groups of the categorical row defined in “Grouping”."
  },
  {
    "objectID": "volcanoplotanalysis.html#test",
    "href": "volcanoplotanalysis.html#test",
    "title": "Volcano plot",
    "section": "3.2 Test",
    "text": "3.2 Test\nDefines what kind of test should be applied (default: T-test)."
  },
  {
    "objectID": "volcanoplotanalysis.html#side",
    "href": "volcanoplotanalysis.html#side",
    "title": "Volcano plot",
    "section": "3.3 Side",
    "text": "3.3 Side\nTo apply a two-sided test, where the null hypothesis can be rejected regardless of the direction of the effect “both” has to be selected (default). “left” and “right” are the respective one-sided tests."
  },
  {
    "objectID": "volcanoplotanalysis.html#number-of-randomizations",
    "href": "volcanoplotanalysis.html#number-of-randomizations",
    "title": "Volcano plot",
    "section": "3.4 Number of randomizations",
    "text": "3.4 Number of randomizations\nSpecifies the number of randomizations that should be applied (default: 250)."
  },
  {
    "objectID": "volcanoplotanalysis.html#preserve-grouping-in-randomizations",
    "href": "volcanoplotanalysis.html#preserve-grouping-in-randomizations",
    "title": "Volcano plot",
    "section": "3.5 Preserve grouping in randomizations",
    "text": "3.5 Preserve grouping in randomizations\nDefines, whether the grouping specified in a categorical row should be preserved in the randomizations (default: &lt;None&gt;). It can be selected from a list including all available groupings of the matrix."
  },
  {
    "objectID": "volcanoplotanalysis.html#fdr",
    "href": "volcanoplotanalysis.html#fdr",
    "title": "Volcano plot",
    "section": "3.6 FDR",
    "text": "3.6 FDR\nRows with a test result below this value are reported as significant (default: 0.05)."
  },
  {
    "objectID": "volcanoplotanalysis.html#s0",
    "href": "volcanoplotanalysis.html#s0",
    "title": "Volcano plot",
    "section": "3.7 S0",
    "text": "3.7 S0\nArtificial within groups variance (default: 0). It controls the relative importance of t-test p-value and difference between means. At s0=0 only the p-value matters, while at nonzero s0 also the difference of means plays a role. See (Tusher, Tibshirani, and Chu 2001) for details."
  },
  {
    "objectID": "hawaiiplot.html",
    "href": "hawaiiplot.html",
    "title": "Hawaii plot",
    "section": "",
    "text": "1 General\n\nType: - Matrix Analysis\n\n\n\n2 Brief description\nVisualize the results of a t-test in the form of multiple volcano plots. Determine significant data points with a permutation-based FDR calculation."
  },
  {
    "objectID": "createrandommatrix.html",
    "href": "createrandommatrix.html",
    "title": "Create randome matrix",
    "section": "",
    "text": "Type: - Matrix Upload\nSource code: CreateRandomMatrix.cs"
  },
  {
    "objectID": "createrandommatrix.html#number-of-rows",
    "href": "createrandommatrix.html#number-of-rows",
    "title": "Create randome matrix",
    "section": "3.1 Number of rows",
    "text": "3.1 Number of rows\nSpecifies the number of rows the randomly created matrix should have (default: 100). Rows are called “Row 1”, Row 2”, etc."
  },
  {
    "objectID": "createrandommatrix.html#number-of-columns",
    "href": "createrandommatrix.html#number-of-columns",
    "title": "Create randome matrix",
    "section": "3.2 Number of columns",
    "text": "3.2 Number of columns\nSpecifies the number of columns the randomly created matrix should have (default: 10). Columns are called “Column 1”, “Column 2”, etc."
  },
  {
    "objectID": "createrandommatrix.html#percentage-of-missing-values",
    "href": "createrandommatrix.html#percentage-of-missing-values",
    "title": "Create randome matrix",
    "section": "3.3 Percentage of missing values",
    "text": "3.3 Percentage of missing values\nSpecifies the percentage of missing values the created matrix should contain (default: 0)."
  },
  {
    "objectID": "createrandommatrix.html#mode",
    "href": "createrandommatrix.html#mode",
    "title": "Create randome matrix",
    "section": "3.4 Mode",
    "text": "3.4 Mode\nDefines how many normal distributions should be included in the matrix (default: One normal distribution). The number of normal distributions can be specified from a predefined list:\n\nOne normal distribution (parameter window A)\nTwo normal distributions (parameter window B)\nMany normal distributions (parameter window C)\n\n\n3.4.1 Distance\nThis parameter is just relevant, if the parameter “Mode” is set to “Two normal distributions”. It defines the distance between the two generated normal distributions (default: 2).\n\n\n3.4.2 How many\nThis parameter is just relevant, if the parameter “Mode” is set to “Many normal distributions”. It defines how many normal distributions should be in the generated matrix (default: 3).\n\n\n3.4.3 Box size\nThis parameter is just relevant, if the parameter “Mode” is set to “Many normal distributions”. It specifies the edge length of the hyper cubes in which the centers of the normal distributions are placed at random (default: 2)."
  },
  {
    "objectID": "createrandommatrix.html#parameter-window",
    "href": "createrandommatrix.html#parameter-window",
    "title": "Create randome matrix",
    "section": "3.5 Parameter window",
    "text": "3.5 Parameter window\n\n\n\nCreate random matrix"
  },
  {
    "objectID": "reorderremoveannotrows.html",
    "href": "reorderremoveannotrows.html",
    "title": "Reorder/remove annotation rows",
    "section": "",
    "text": "General\n\nType: - Matrix Processing\nHeading: - Rearrange\nSource code: ReorderRemoveAnnotRows.cs\n\n\n1 Brief description\nAnnotation rows can be removed with this activity.\nOutput: Same matrix but with annotation rows removed or in new order.\n\n\n\n2 Parameters\nA new matrix is generated with the specified columns by selecting/deselecting expression/numerical/multi numerical/categorical/text columns (default: all columns are selected).\n\n\n3 Parameter window"
  },
  {
    "objectID": "troubleshooting.html",
    "href": "troubleshooting.html",
    "title": "Trouble Shooting",
    "section": "",
    "text": "Q: Cannot load raw files\nA: Check if .NET and MSFileReader are installed on your computer\n\nQ: “MSFileReader appears not to be installed” error message\nA: If the software has been successfully installed, but this error message appears, try to uninstall and then re-install the MSFileReader again. Restarting the computer may also be necessary\n\nQ: No spectra are displayed\nA: Check the “Exists” status in the Viewer for each file – if it is false, the location of the raw files has changed and has to be updated. Select all files for which the location has to be updated and use “Change folder” to navigate to the correct location\n\nQ: The MS Feature View is either empty or does not display updated information\nA: Use the refresh button the Feature Controls section\n\nQ: No MaxQuant identifications are loaded\n\nA1: Make sure the raw files are in the same folder as all index files, the mqpar.xml file and all output folders created by MaxQuant during the processing.\nA2: Make sure you are using the same MaxQuant version that was used for processing the raw data\n\nQ: I have a problem that is not listed here\nA: Try asking on one of our google groups\n\nQ: I want to report a bug\nA: Use our bugs tracking system"
  },
  {
    "objectID": "addmodificationcounts.html",
    "href": "addmodificationcounts.html",
    "title": "Add modification counts",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Modifications\nSource code: AddModificationCounts.cs"
  },
  {
    "objectID": "addmodificationcounts.html#modifications",
    "href": "addmodificationcounts.html#modifications",
    "title": "Add modification counts",
    "section": "3.1 Modifications",
    "text": "3.1 Modifications\nSelected modifications for which the corresponding sites are counted (default: all modifications are selected). The list of modifications includes:\n\nAcetylation\nMethylation\nO-GlcNAc\nPhosphorylation\nSumoyalation\nUbiquitination"
  },
  {
    "objectID": "addmodificationcounts.html#uniprot-column",
    "href": "addmodificationcounts.html#uniprot-column",
    "title": "Add modification counts",
    "section": "3.2 Uniprot column",
    "text": "3.2 Uniprot column\nSelected text column that contains the Uniprot identifiers (default: first text column in the matrix)."
  },
  {
    "objectID": "filterquality.html",
    "href": "filterquality.html",
    "title": "Filter quality",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Quality\nSource code: not public."
  },
  {
    "objectID": "filterquality.html#filter-method",
    "href": "filterquality.html#filter-method",
    "title": "Filter quality",
    "section": "3.1 Filter method",
    "text": "3.1 Filter method\nDefines the filter method that will be applied to the values in the expression columns (default: From quality matrix). If “From quality matrix” is selected, the filtering is just based on the previously created quality matrix and a threshold must be specified (see parameter window A). If “Compare main to quality matrix” is chosen, the values of the two matrices are compared to each other in the way it is specified in “Values should be” (see parameter window B).\n\n3.1.1 Threshold\nThis parameter is just relevant, if the “Filter method” is “From quality matrix”. It specifies which values of the matrix will be kept and which ones will be discarded (default: 0). Values lower than the defined “Threshold” will be filtered.\n\n\n3.1.2 Values should be\nThis parameter is just relevant, if the “Filter method” is “Compare main to quality matrix”. It specifies how the values of the main matrix should be compared to the values of the quality matrix (default: Greater than). The operation how to compare the values of the two matrices can be chosen from a predefined list:\n\nGreater than\nGreater or equal\nLess than\nLess or equal"
  },
  {
    "objectID": "perseuspluginstore.html",
    "href": "perseuspluginstore.html",
    "title": "Plug-Ins Store",
    "section": "",
    "text": "You can extend the functionality of Perseus by installing plugins. To install a plugin, just download the plugin .dll file, copy it into the Perseus folder and restart the software.\n\n1 Available Plugins\n\n\n\nPlugin Name\nDeveloper\nDownload\nSource Code\nCompatibility\n\n\n\n\nProteomic ruler\nMarco Hein\nv. 1.6.2\ngitHub\nPerseus 2.0.11\n\n\nPluginInterop\nJan Rudolph\nv. 1.6.15\nPluginInterop\nPerseus 2.0.11\n\n\nDependentPeptides\nJan Rudolph\nv. 1.6.1\nPluginDependentPeptides\nPerseus 2.0.11\n\n\nPluginUpperCase\nJan Rudolph\nv. 1.6.0\nPluginUpperCase\nPerseus 2.0.11\n\n\nPluginPhoton\nJan Rudolph\nv. 1.0\nPhoton\nPerseus 2.0.11\n\n\nPlugInMetis\nDr. Hamid Hamzeiy & Daniela Ferretti\nv. 1.6.6\ngithub\nPerseus 2.0.11\n\n\n\n\n\n\n2 Developers\nIf you want to develop and submit your own Perseus plugin, please read our Plugin Developers’s Guide."
  },
  {
    "objectID": "kinasesubstraterelations.html",
    "href": "kinasesubstraterelations.html",
    "title": "Kinase-substrate relations",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Modifications\nSource code: KinaseSubstrateRelations.cs"
  },
  {
    "objectID": "kinasesubstraterelations.html#uniprot-column",
    "href": "kinasesubstraterelations.html#uniprot-column",
    "title": "Kinase-substrate relations",
    "section": "3.1 Uniprot column",
    "text": "3.1 Uniprot column\nSelected text column that contains the Uniprot identifiers (default: first text column in the matrix)."
  },
  {
    "objectID": "kinasesubstraterelations.html#sequence-window",
    "href": "kinasesubstraterelations.html#sequence-window",
    "title": "Kinase-substrate relations",
    "section": "3.2 Sequence window",
    "text": "3.2 Sequence window\nSelected text column that contains the sequence windows around the sites (default: first text column in the matrix)."
  },
  {
    "objectID": "combinecategoricalcolumns.html",
    "href": "combinecategoricalcolumns.html",
    "title": "Combine categorical columns",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: CombineCategoricalColumns.cs"
  },
  {
    "objectID": "combinecategoricalcolumns.html#first-column",
    "href": "combinecategoricalcolumns.html#first-column",
    "title": "Combine categorical columns",
    "section": "3.1 First column",
    "text": "3.1 First column\nFirst selected categorical column, whose values should be concatenated to the second one (default: first categorical column of the matrix). The new generated categorical column contains the two values concatenated with a “_” in between."
  },
  {
    "objectID": "combinecategoricalcolumns.html#second-column",
    "href": "combinecategoricalcolumns.html#second-column",
    "title": "Combine categorical columns",
    "section": "3.2 Second column",
    "text": "3.2 Second column\nSecond selected categorical column, whose values should be concatenated to the first one (default: first categorical column of the matrix). The new generated categorical column contains the two values concatenated with a “_” in between."
  },
  {
    "objectID": "summarystatisticscolumns.html",
    "href": "summarystatisticscolumns.html",
    "title": "Summary statistics (columns)",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: SummaryStatisticsColumns.cs"
  },
  {
    "objectID": "summarystatisticscolumns.html#columns",
    "href": "summarystatisticscolumns.html#columns",
    "title": "Summary statistics (columns)",
    "section": "3.1 Columns",
    "text": "3.1 Columns\nSelected expression/numerical columns for which the later defined quantities should be calculated."
  },
  {
    "objectID": "summarystatisticscolumns.html#calculate",
    "href": "summarystatisticscolumns.html#calculate",
    "title": "Summary statistics (columns)",
    "section": "3.2 Calculate",
    "text": "3.2 Calculate\nList of quantities that are calculated for the selected columns (default: all of the below listed quantities are selected). The available quantities are:\n\nSum\nMean\nTurkey biweight\nStandard deviation\nCoefficient of variation\nMedian absolute deviation\nMinimum\nMaximum\nRange\nValid values\nInter-quartile range\n1st quartile\n3rd quartile\nSkewness\nKurtosis"
  },
  {
    "objectID": "zscore.html",
    "href": "zscore.html",
    "title": "Z Score",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: ZScore.cs"
  },
  {
    "objectID": "zscore.html#matrix-access",
    "href": "zscore.html#matrix-access",
    "title": "Z Score",
    "section": "3.1 Matrix access",
    "text": "3.1 Matrix access\nSpecifies whether the z-scoring is performed on rows or columns (default: Rows).\n\n3.1.1 Grouping\nThis parameter is just relevant, if the “Matrix access” is set to “Rows”. It specifies, whether the normalization should be applied separately on groups (default: )."
  },
  {
    "objectID": "zscore.html#use-median",
    "href": "zscore.html#use-median",
    "title": "Z Score",
    "section": "3.2 Use median",
    "text": "3.2 Use median\nIf checked, the median and not the mean of each row/column is used for the calculation of the z-score of each matrix cell (default: unchecked)."
  },
  {
    "objectID": "zscore.html#report-mean-and-std.-dev.",
    "href": "zscore.html#report-mean-and-std.-dev.",
    "title": "Z Score",
    "section": "3.3 Report mean and std. dev.",
    "text": "3.3 Report mean and std. dev.\nIf checked, the mean and the standard deviation used for the calculation are reported (default: unchecked). In case the z-scoring is based on rows (“Matrix access” = “Rows”), the calculated mean and standard deviation appear in 2 newly generated numeric columns called “Mean” and “Std. dev.” containing the mean and standard deviation of each row. In case the calculation is based on columns (“Matrix access” = “Columns”), 2 new numeric rows are generated containing the mean and standard deviation of each column."
  },
  {
    "objectID": "andromeda_configurations.html",
    "href": "andromeda_configurations.html",
    "title": "Andromeda Configurations",
    "section": "",
    "text": "The Andromeda search engine1 offers a large variety of already defined modifications, proteases and sequence databases. But sometimes for specialized studies additional configurations are necessary. Therefore it is possible to configure the search engine according to special needs. You can add, remove, duplicate or modify modifications, enzymes and protein sequence databases. Also rearranging the order in the lists is possible to e.g. easier compare things. This rearrangements have no effect on the later analyses.\nOn the following pages are step by step descriptions how to configure…\n\nAndromeda modifications table\nAndromeda Enzymes - the proteases table\nAndromeda Protdatabases - the (protein) sequence databases table\n\nFor question we would love to hear from you under Contact\n\n\n\n\nReferences\n\n1. Cox, J. et al. Andromeda: A Peptide Search Engine Integrated into the MaxQuant Environment. Journal of Proteome Research 10, 1794–1805 (2011)."
  },
  {
    "objectID": "addknownsites.html",
    "href": "addknownsites.html",
    "title": "Add known sites",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Modifications\nSource code: AddKnownSites.cs"
  },
  {
    "objectID": "addknownsites.html#modification",
    "href": "addknownsites.html#modification",
    "title": "Add known sites",
    "section": "3.1 Modification",
    "text": "3.1 Modification\nSelected modification for which the known sites should be added (default: Phosphorylation). The modification can be selected from a predefined list:\n\nAcetylation\nMethylation\nO-GlcNAc\nPhosphorylation\nSumoyalation\nUbiquitination"
  },
  {
    "objectID": "addknownsites.html#uniprot-column",
    "href": "addknownsites.html#uniprot-column",
    "title": "Add known sites",
    "section": "3.2 Uniprot column",
    "text": "3.2 Uniprot column\nSelected text column that contains the Uniprot identifiers (default: first text column in the matrix)."
  },
  {
    "objectID": "addknownsites.html#sequence-column",
    "href": "addknownsites.html#sequence-column",
    "title": "Add known sites",
    "section": "3.3 Sequence column",
    "text": "3.3 Sequence column\nSelected text column that contains the sequence windows around the sites (default: first text column in the matrix)."
  },
  {
    "objectID": "columncorrelations.html",
    "href": "columncorrelations.html",
    "title": "Column Correlations",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: not public."
  },
  {
    "objectID": "columncorrelations.html#type",
    "href": "columncorrelations.html#type",
    "title": "Column Correlations",
    "section": "3.1 Type",
    "text": "3.1 Type\nDefines the measure of correlation that should be calculated between the selected columns (default: Pearson correlation). It can be selected from a list of correlation coefficients:\n\nLog2(Absence-presence enrichment factor)\nAbsence-presence -Log10(p-value)\nNumbers of valid pairs\nValid pairs percentage\nPearson correlation\n-Log10(Pearson p-value)\n-Log10(Pearson p-value) [correlation]\n-Log10(Pearson p-value) [anti-correlation]\nR squared\nSpearman rank correlation\n-Log10(Spearman p-value)\n-Log10(Spearman p-value) [correlation]\n-Log10(Spearman p-value) [anti-correlation]\nKendall rank correlation\nDistance correlation\nMutual information\nEuclidean distance\nManhattan distance\nMaximum distance"
  },
  {
    "objectID": "columncorrelations.html#rows",
    "href": "columncorrelations.html#rows",
    "title": "Column Correlations",
    "section": "3.2 Rows",
    "text": "3.2 Rows\nSelected expression/numerical columns that will be the rows of the generated correlation matrix (default: all expression columns are selected)."
  },
  {
    "objectID": "columncorrelations.html#columns",
    "href": "columncorrelations.html#columns",
    "title": "Column Correlations",
    "section": "3.3 Columns",
    "text": "3.3 Columns\nSelected expression/numerical columns that will be the columns of the generated correlation matrix (default: all expression columns are selected)."
  },
  {
    "objectID": "silac.html",
    "href": "silac.html",
    "title": "SILAC data",
    "section": "",
    "text": "For the here shown use case Perseus 1.5.1.6 was used."
  },
  {
    "objectID": "silac.html#loading",
    "href": "silac.html#loading",
    "title": "SILAC data",
    "section": "2.1 Loading",
    "text": "2.1 Loading\nLoad the file “proteinGroups.txt” from the “combined/txt” folder of the MaxQuant output. Load → Generic matrix upload is denoted by the green arrow on the top left corner of the Perseus window or load the file using the drag and drop function of Perseus.\n\nMake sure you select the 9 “Ratio H/L Normalized …” columns as main columns. Also make sure you understand what is meant by the column types “Main”, “Categorical”, “Numerical”, “Multi-numerical” and “Text”.\nComment: After each applied operation a new matrix will be generated. Every matrix that you are creating can be saved with Export → Generic matrix export and re-imported later with Load → Generic matrix upload."
  },
  {
    "objectID": "silac.html#filtering",
    "href": "silac.html#filtering",
    "title": "SILAC data",
    "section": "2.2 Filtering",
    "text": "2.2 Filtering\nAfter loading the matrix, we filter out the reverse proteins and the proteins that are only identified by site. This is done by using the function Processing → Filter rows → Filter rows based on categorical column, because both the “Reverse” column and the “Only identified by site” column are categorical.\n\nFirst, we filter out the reverse hits. Reverse hits are indicated by a “+” in the “Reverse” column, so to filter out these hits all rows containing a “+” will be removed from the matrix. Therefore, the column “Reverse” needs to be selected, “+” is the value we are looking for and is selected by default. No further changes need to be applied, because we want to remove the matching rows from the matrix. This results in a matrix, where the value in the “Reverse” column of all rows is empty.\n\nSecond, we filter out the hits that are only identified by site. The column “Only identified by site” has two values, TRUE and FALSE. Thus all rows containing TRUE are filtered out. Therefore, “Only identified by site” has to be selected as column and TRUE is the value of interest, which is selected by clicking on TRUE on the left hand side and using the button in the middle with the arrow to the right on it. Deselection works by using the button with the arrow to the left. No further changes are necessary, because we want to remove the matching rows from the matrix. This results in a matrix, where all rows contain the value FALSE in the column “Only identified by site”.\n\nThe next step is to filter out all rows with less than six valid intensity values of all nine (i.e. being not NaN). Therefore we use Processing → Filter rows → Filter rows based on valid values.\n\nThe required percentage of valid values per row may differ from study to study. In some cases, having valid values only in one of several replicate groups might be a significant finding and these kind of rows should possibly not be discarded. The filter can also be applied to each column group separately. (How to create column groups or categorical annotation rows you will learn in a few steps.) Now we just want to be certain that each row contains at least six of the nine intensity values."
  },
  {
    "objectID": "silac.html#ratio-transformation",
    "href": "silac.html#ratio-transformation",
    "title": "SILAC data",
    "section": "2.3 Ratio transformation",
    "text": "2.3 Ratio transformation\nBecause the actual sample is in the light SILAC channel and the reference is in the heavy channel, we need to invert the ratios using Processing → Basic → Transform.\n\nTo invert the ratios the formula \\(1/x\\) has to be typed in the transformation type field. No further changes are necessary, because we want to apply the transformation to all main columns, which are selected by default.\n\nThen the ratios will be logarithmized by using the function Processing → Basic → Transform again.\n\nNow the transformation function we are going to use is \\(log_2(x)\\), which is the default setting. The transformation should also be applied to all main columns, which are selected by default."
  },
  {
    "objectID": "silac.html#renaming-columns",
    "href": "silac.html#renaming-columns",
    "title": "SILAC data",
    "section": "2.4 Renaming columns",
    "text": "2.4 Renaming columns\nThe main column names “Ratio H/L Normalized …” are a bit bulky. So we rename the main columns with the help of regular expressions and the function Processing → Rearrange → Rename columns [Reg.ex.].\n\nThe regular expression that you may use to remove the repetitive part of the name is “Ratio H/L Normalized (.*)”. The general concept of regular expressions can be found under http://en.wikipedia.org/wiki/Regular_expression. If you already know generally how regular expressions work, you may only need to glance at this Quick Reference or at an even quicker Cheat Sheet.\nThis results in a matrix, where the column names of the ratios are a combination of the cell line and the replica.\n\nIf you want to rename the columns manually without the help of regular expressions you can use Processing → Rearrange → Rename columns.\n\nThen you can type the new names in the predefined text field."
  },
  {
    "objectID": "perseus_tutorials.html",
    "href": "perseus_tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Videos of the lectures and tutorials given at the different MaxQuant Summer Schools in the past years can all be found in our YouTube channel:\nolder Lectures:\n\nPerseus and statistics 01 - Lecture by Juergen Cox\nPerseus and statistics 02 - Lecture by Tami Geiger\nPerseus and statistics 03 - Lecture by Stefka Tyanova"
  },
  {
    "objectID": "andromeda_protdatabases.html",
    "href": "andromeda_protdatabases.html",
    "title": "Andromeda Protdatabases - the (protein) sequence databases table",
    "section": "",
    "text": "For the here shown step by step description Andromeda was used within MaxQuant (version 1.5.3.8).\n\n\nOpen MaxQuant and go to the Andromeda configuration tab. There select the Data type “Sequence databases” (s. Figure 1).\n\n\n\nFigure 1: opening the databases table\n\n\n\n\n\nLet’s assume we have to analyze human proteome measurements and we are interested which human database is used and how it is used (see Description on the right hand side of the Andromeda window in Figure 2).\n\n\n\nFigure 2: example human protease DB\n\n\n\n\n\nNow we are assuming that a reviewer asks us to search our human proteome measurements against the tasmanian devil proteome ( Sarcophilus harrisii ) - for some reason - and we downloaded the corresponding fasta file from Uniprot.\nAfter downloading the file, we are including the database into MaxQuant. First, click the “Add” button (highlithed in Figure 3). Then a new row will be added at the end of the table and a new sequence database form will appear on the right hand side that can be edited.\n\n\n\nFigure 3: Adding a new entry to database\n\n\nThen you just have to fill in the form by defining the fasta file that should be used, the parsing rule that should be applied to retrieve the identifiers, the source of the fasta file, the taxonomy and the organism name. To define the parsing rule regular expressions are used. A regular expression is a sequence of characters that forms a search pattern with a special syntax. A good general introduction can be found, as always, on Wikipedia. If you already know generally how regular expressions work, you may only need to glance at a this Quick Reference or at an even quicker Cheat Sheet. Also you don’t have to know the taxonomy of your organism, just type in the name and use the arrows in the taxonomy line and MaxQuant will complete the form.\nDon’t forget to click the “Modify table” button (Figure 4) when you’re done to transfer the changes you made in the form to the table on the left. And to save the table you have to click the “Save changes” button.\nTo have the added sequence database available in MaxQuant you have to open a new MaxQuant window.\n\n\n\nFigure 4: Saving the entry in the database"
  },
  {
    "objectID": "andromeda_protdatabases.html#open-the-sequence-databases-table",
    "href": "andromeda_protdatabases.html#open-the-sequence-databases-table",
    "title": "Andromeda Protdatabases - the (protein) sequence databases table",
    "section": "",
    "text": "Open MaxQuant and go to the Andromeda configuration tab. There select the Data type “Sequence databases” (s. Figure 1).\n\n\n\nFigure 1: opening the databases table"
  },
  {
    "objectID": "andromeda_protdatabases.html#viewing-an-example",
    "href": "andromeda_protdatabases.html#viewing-an-example",
    "title": "Andromeda Protdatabases - the (protein) sequence databases table",
    "section": "",
    "text": "Let’s assume we have to analyze human proteome measurements and we are interested which human database is used and how it is used (see Description on the right hand side of the Andromeda window in Figure 2).\n\n\n\nFigure 2: example human protease DB"
  },
  {
    "objectID": "andromeda_protdatabases.html#adding-a-new-database",
    "href": "andromeda_protdatabases.html#adding-a-new-database",
    "title": "Andromeda Protdatabases - the (protein) sequence databases table",
    "section": "",
    "text": "Now we are assuming that a reviewer asks us to search our human proteome measurements against the tasmanian devil proteome ( Sarcophilus harrisii ) - for some reason - and we downloaded the corresponding fasta file from Uniprot.\nAfter downloading the file, we are including the database into MaxQuant. First, click the “Add” button (highlithed in Figure 3). Then a new row will be added at the end of the table and a new sequence database form will appear on the right hand side that can be edited.\n\n\n\nFigure 3: Adding a new entry to database\n\n\nThen you just have to fill in the form by defining the fasta file that should be used, the parsing rule that should be applied to retrieve the identifiers, the source of the fasta file, the taxonomy and the organism name. To define the parsing rule regular expressions are used. A regular expression is a sequence of characters that forms a search pattern with a special syntax. A good general introduction can be found, as always, on Wikipedia. If you already know generally how regular expressions work, you may only need to glance at a this Quick Reference or at an even quicker Cheat Sheet. Also you don’t have to know the taxonomy of your organism, just type in the name and use the arrows in the taxonomy line and MaxQuant will complete the form.\nDon’t forget to click the “Modify table” button (Figure 4) when you’re done to transfer the changes you made in the form to the table on the left. And to save the table you have to click the “Save changes” button.\nTo have the added sequence database available in MaxQuant you have to open a new MaxQuant window.\n\n\n\nFigure 4: Saving the entry in the database"
  },
  {
    "objectID": "changecolumntype.html",
    "href": "changecolumntype.html",
    "title": "Change Column type",
    "section": "",
    "text": "Type: -Matrix Processing\nHeading: - Rearrange\nSource code: ChangeColumnType.cs"
  },
  {
    "objectID": "changecolumntype.html#find-what",
    "href": "changecolumntype.html#find-what",
    "title": "Change Column type",
    "section": "3.1 Find what",
    "text": "3.1 Find what\nSpecified text/term that is searched in a selected text column (default: empty)."
  },
  {
    "objectID": "changecolumntype.html#look-in",
    "href": "changecolumntype.html#look-in",
    "title": "Change Column type",
    "section": "3.2 Look in",
    "text": "3.2 Look in\nSelected text column that should be searched for the specified term (default: first text column of the matrix)."
  },
  {
    "objectID": "changecolumntype.html#match-case",
    "href": "changecolumntype.html#match-case",
    "title": "Change Column type",
    "section": "3.3 Match case",
    "text": "3.3 Match case\nThe cells of the text column will be searched for a matching substring (default: checked). The results will be in a new generated categorical column called “Search: original column name”. “\\(+\\)” indicates, whether a match was successful."
  },
  {
    "objectID": "changecolumntype.html#match-whole-word",
    "href": "changecolumntype.html#match-whole-word",
    "title": "Change Column type",
    "section": "3.4 Match whole word",
    "text": "3.4 Match whole word\nThe cells of the text column will be searched to match the whole word of the specified term (default: unchecked). The results will be in a new generated categorical column called “Search: original column name”. “\\(+\\)” indicates, whether a match was successful."
  },
  {
    "objectID": "andromeda_enzymes.html",
    "href": "andromeda_enzymes.html",
    "title": "Andromeda Enzymes - the proteases table",
    "section": "",
    "text": "For the here shown step by step description Andromeda was used within MaxQuant (version 1.5.3.8).\n\n\nOpen MaxQuant and go to the Andromeda configuration tab. There select the Data type “Proteases” (s. Figure 1).\n\n\n\nFigure 1: Adding new Proteases\n\n\n\n\n\nIn most studies samples are digested using Trypsin. In the “Proteases” table you will find two different definitions for Trypsin. The first definition cleaves at the carboxyl side of the amino acids lysine or arginine, except when either is followed by proline (see Description on the right hand side of the Andromeda window). That’s the classical definition. Additional comments in Figure 2 are in black and blue.\n\n\n\nFigure 2: An example of added Trypsin protease to the table\n\n\nHowever the commonly used definition is “Trypsin/P”, which also cleaves at carboxyl side of the amino acids lysine or arginine, also if a proline follows (highlighted in red in Figure 3). Additional comments in Figure 3 are in black and blue.\n\n\n\nFigure 3: TrypsinP cleaves at the carboxyl side of the AA\n\n\n\n\n\nLet’s assume Chymotrypsin is not yet provided in Andromeda and we want to add it. From the literature we know Chymotrypsin cleaves c-terminal after Phenylalanine (F), Tryptophan (W) and Tyrosine (Y).\nFirst click the “Add” button as shown in Figure 4. Then a new row will be added at the end of the table and a new protease form will appear on the right hand side that can be edited.\n\n\n\nFigure 4: Adding Chymotrypsin\n\n\nThen you just have to fill in the form by defining a name, a description and the specificity. Don’t forget to click the “Modify table” button when your done to transfer the changes you made in the form to the table on the left. And to save the table you have to click the “Save changes” button. Additional comments on the screenshot are in black and blue in Figure 5.\nTo have the added modification available in MaxQuant you have to open a new MaxQuant window.\n\n\n\nFigure 5: save added protease\n\n\nComment: Note that you also can do completely unspecific searches in MaxQuant. For this no definition of an enzyme is necessary."
  },
  {
    "objectID": "andromeda_enzymes.html#open-the-proteases-table",
    "href": "andromeda_enzymes.html#open-the-proteases-table",
    "title": "Andromeda Enzymes - the proteases table",
    "section": "",
    "text": "Open MaxQuant and go to the Andromeda configuration tab. There select the Data type “Proteases” (s. Figure 1).\n\n\n\nFigure 1: Adding new Proteases"
  },
  {
    "objectID": "andromeda_enzymes.html#viewing-examples",
    "href": "andromeda_enzymes.html#viewing-examples",
    "title": "Andromeda Enzymes - the proteases table",
    "section": "",
    "text": "In most studies samples are digested using Trypsin. In the “Proteases” table you will find two different definitions for Trypsin. The first definition cleaves at the carboxyl side of the amino acids lysine or arginine, except when either is followed by proline (see Description on the right hand side of the Andromeda window). That’s the classical definition. Additional comments in Figure 2 are in black and blue.\n\n\n\nFigure 2: An example of added Trypsin protease to the table\n\n\nHowever the commonly used definition is “Trypsin/P”, which also cleaves at carboxyl side of the amino acids lysine or arginine, also if a proline follows (highlighted in red in Figure 3). Additional comments in Figure 3 are in black and blue.\n\n\n\nFigure 3: TrypsinP cleaves at the carboxyl side of the AA"
  },
  {
    "objectID": "andromeda_enzymes.html#adding-a-new-protease",
    "href": "andromeda_enzymes.html#adding-a-new-protease",
    "title": "Andromeda Enzymes - the proteases table",
    "section": "",
    "text": "Let’s assume Chymotrypsin is not yet provided in Andromeda and we want to add it. From the literature we know Chymotrypsin cleaves c-terminal after Phenylalanine (F), Tryptophan (W) and Tyrosine (Y).\nFirst click the “Add” button as shown in Figure 4. Then a new row will be added at the end of the table and a new protease form will appear on the right hand side that can be edited.\n\n\n\nFigure 4: Adding Chymotrypsin\n\n\nThen you just have to fill in the form by defining a name, a description and the specificity. Don’t forget to click the “Modify table” button when your done to transfer the changes you made in the form to the table on the left. And to save the table you have to click the “Save changes” button. Additional comments on the screenshot are in black and blue in Figure 5.\nTo have the added modification available in MaxQuant you have to open a new MaxQuant window.\n\n\n\nFigure 5: save added protease\n\n\nComment: Note that you also can do completely unspecific searches in MaxQuant. For this no definition of an enzyme is necessary."
  },
  {
    "objectID": "uniquerows.html",
    "href": "uniquerows.html",
    "title": "Unique rows",
    "section": "",
    "text": "1 eneral =====\n\nType: - Matrix Processing\nHeading: - Rearrange\nSource code: UniqueRows.cs\n\n\n\n2 Brief description\nCombines rows with identical values in the specified columns"
  },
  {
    "objectID": "significancea.html",
    "href": "significancea.html",
    "title": "Significance A",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Outliers\nSource code: SignificanceA.cs"
  },
  {
    "objectID": "significancea.html#columns",
    "href": "significancea.html#columns",
    "title": "Significance A",
    "section": "3.1 Columns",
    "text": "3.1 Columns\nSelected expression columns for which “Significance A” should be calculated (default: no expression columns are selected)."
  },
  {
    "objectID": "significancea.html#side",
    "href": "significancea.html#side",
    "title": "Significance A",
    "section": "3.2 Side",
    "text": "3.2 Side\nTo apply a two-sided test, where the null hypothesis can be rejected regardless of the direction of the effect “both” has to be selected (default). “left” and “right” are the respective one-sided tests."
  },
  {
    "objectID": "significancea.html#use-for-truncation",
    "href": "significancea.html#use-for-truncation",
    "title": "Significance A",
    "section": "3.3 Use for truncation",
    "text": "3.3 Use for truncation\nThe truncation can be based on p-values or the Benjamini-Hochberg correction for multiple hypothesis testing (default: Benjamini-Hochberg FDR). Rows with a test result below a specified value (parameter below) are reported as significant."
  },
  {
    "objectID": "significancea.html#threshold-value",
    "href": "significancea.html#threshold-value",
    "title": "Significance A",
    "section": "3.4 Threshold value",
    "text": "3.4 Threshold value\nBased on a specified threshold a specific row is reported as significant (default: 0.05). Depending on the chosen truncation score this threshold value is applied to the p-value or to the Benjamini-Hochberg FDR."
  },
  {
    "objectID": "combinemaincolumns.html",
    "href": "combinemaincolumns.html",
    "title": "Combine main columns",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: not public.\n\n\n\n2 Brief description\nPairs of columns are combined into single columns. Please make sure that the numbers of columns that are selected in the ‘x’ and ‘y’ box are equal.\nOutput: Expression columns are exchanged by the new combined columns. The original columns are also kept in the output, if ‘Keep original columns’ is checked."
  },
  {
    "objectID": "renamecolumnsregexp.html",
    "href": "renamecolumnsregexp.html",
    "title": "Rename Column by Reg. Exp.",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: RenameColumnsRegexp.cs"
  },
  {
    "objectID": "renamecolumnsregexp.html#regular-expression",
    "href": "renamecolumnsregexp.html#regular-expression",
    "title": "Rename Column by Reg. Exp.",
    "section": "3.1 Regular expression",
    "text": "3.1 Regular expression\nSpecified regular expression that is applied to all column names to rename them (default: no string).\nThe general concept of regular expressions can be found under Regular_expression. If you already know generally how regular expressions work, you may only need to glance at a the Quick Reference or at an even quicker Cheat Sheet.\n\\"
  },
  {
    "objectID": "converttonan.html",
    "href": "converttonan.html",
    "title": "Convert to NaN",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Quality\nSource code: not public."
  },
  {
    "objectID": "converttonan.html#values-should-be",
    "href": "converttonan.html#values-should-be",
    "title": "Convert to NaN",
    "section": "3.1 Values should be",
    "text": "3.1 Values should be\nSpecifies how the expression values of the matrix should be compared to the defined “Threshold” (default: Greater than). The operation how to compare the values can be chosen from a predefined list:\n\nGreater than\nGreater or equal\nLess than\nLess or equal"
  },
  {
    "objectID": "converttonan.html#threshold",
    "href": "converttonan.html#threshold",
    "title": "Convert to NaN",
    "section": "3.2 Threshold",
    "text": "3.2 Threshold\nValue defining which expression values should be converted to NaN (default: 0)."
  },
  {
    "objectID": "duplicatecolumns.html",
    "href": "duplicatecolumns.html",
    "title": "Duplicate Columns",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Rearrange\nSource code: DuplicateColumns.cs\n\n\n\n2 Brief description\nColumns of all types can be duplicated.\nOutput: Same matrix but with duplicated columns added.\n\n\n3 Parameters\nThe selected expression/numerical/multi numerical/categorical/text columns will be duplicated and included in the new generated matrix at the end of each group of column types (default: no columns are selected). The name of the new columns is the old column name with the string “_1” attached to the end.\n\n\n4 Parameter window\n\n\n\nRearrange duplicate columns"
  },
  {
    "objectID": "genericmatrixupload.html",
    "href": "genericmatrixupload.html",
    "title": "Generic Matrix Upload",
    "section": "",
    "text": "Type: - Matrix Upload\nSource code: GenericMatrixUpload.cs"
  },
  {
    "objectID": "genericmatrixupload.html#parameters",
    "href": "genericmatrixupload.html#parameters",
    "title": "Generic Matrix Upload",
    "section": "2.1 Parameters",
    "text": "2.1 Parameters\n\n2.1.1 File\nSpecifies the file path of the tab separated file that should be uploaded (default: empty). It can be specified manually by typing in the path or the file can be browsed by using the “Select” button.\n\n\n2.1.2 Main/Numerical/Categorical/Text/Multi-numerical\nEach of the listed columns in the left panel that should be loaded need to be distributed among the five different column types depending on the analysis that should be applied.\nHint: If necessary, column types can also be changed later using Change column type.\n\n\n2.1.3 Parameter window\n\n\n\nGeneric matrix upload"
  },
  {
    "objectID": "Glossary.html",
    "href": "Glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "===== A =====\n===== B =====\n===== C =====\n===== D =====\n===== E =====\n===== F =====\n\n\n\n\n\nterm\ndefinition\n\n\n\n\nFDR\nFalse Discovery Rate\n\n\nFTICR\nFourier transform ion cyclotron resonance mass spectrometry\n\n\n\n\n\n\n\n===== G =====\n===== H =====\n===== I =====\n\n\n\n\n\nterm\ndefinition\n\n\n\n\niBAQ\nIntensity Based Absolute Quantification\n\n\nICAT\nIsotope-coded affinity tag\n\n\niTRAQ\nIsobaric tag for relative and absolute quantitation\n\n\n\n\n\n\n\n===== J =====\n===== K =====\n===== L =====\n\n\n\n\n\nterm\ndefinition\n\n\n\n\nLFQ\nLabel-free quantification\n\n\n\n\n\n\n\n===== M =====\n\n\n\n\n\nterm\ndefinition\n\n\n\n\nMALDI\nMatrix-assisted laser desorption/ionization\n\n\n\n\n\n\n\n===== N =====\n===== O =====\n===== P =====\n\n\n\n\n\nterm\ndefinition\n\n\n\n\nPAGE\nPolyacrylamide gel electrophoresis\n\n\nPEP\nPosterior Error Probability\n\n\n\n\n\n\n\n===== Q =====\n===== R =====\n===== S =====\n\n\n\n\n\nterm\ndefinition\n\n\n\n\nSDS\nsodium dodecyl sulfate\n\n\nSILAC\nstable isotope labeling by/with amino acids in cell culture\n\n\nSWIFT\nStored waveform inverse Fourier transform\n\n\n\n\n\n\n\n===== T =====\n\n\n\n\n\nterm\ndefinition\n\n\n\n\nTMT\nTandem mass tag\n\n\n\n\n\n\n\n===== U =====\n===== V =====\n===== W =====\n===== X =====\n===== Y =====\n===== Z =====\n\nWe welcome suggestions for additional entries to this glossary, or corrections to existing entries. For technical reasons, we have not made this page editable by users. Please use the following contact possibilities instead.\n\nThe glossary was created using the glossary package1\n\n\n\n\nReferences\n\n1. DeBruine, L. Glossary: Glossaries for markdown and quarto documents. (2023)."
  },
  {
    "objectID": "posthoctests.html",
    "href": "posthoctests.html",
    "title": "Post hoc tests",
    "section": "",
    "text": "1 General =====\n\nType: - Matrix Processing\nHeading: - Tests\nSource code: not public.\n\n\n\n2 Brief description\nPost-hoc analysis is performed, e.g. on the significant findings of an ANOVA test."
  },
  {
    "objectID": "perseus_user_intereface.html",
    "href": "perseus_user_intereface.html",
    "title": "Perseus User Interface",
    "section": "",
    "text": "The title bar along the top has the Perseus logo on the left, followed by an icon that can be used to rename the session, which will be displayed and is by default “Session 1 - Perseus”. The bar below (Figure 1) has on the left a menu (indicated by the blue box with a white arrow on it) with the usual sorts of file functions (Save, Save as, Save as PDF, Open, New, New Window, Annotation Download, Help, Exit), followed by a tab label “Matrix”. On the right are buttons to split/join the Perseus window, minimize/maximize the ribbon with all the activities and the last button redirects to the documentation. The ribbon of the “Matrix” tab contains all the activities (highlighted by a cyan rectangle) that can be applied to loaded matrices.\nThe activities can be divided into five main categories. The icons of the categories Processing, Analysis and Multi-proc. are frequently used activities, which are also listed in one of the drop down menus of each category. To figure out the function behind the icon just hover over it. For simplicity the icons of these three categories are ignored in the listing below:"
  },
  {
    "objectID": "perseus_user_intereface.html#load",
    "href": "perseus_user_intereface.html#load",
    "title": "Perseus User Interface",
    "section": "0.1 Load",
    "text": "0.1 Load\n\nGeneric Matrix Upload\nCreate Gene list\nCreate Random matrix\nRaw upload\nBinary upload\nNGS upload"
  },
  {
    "objectID": "perseus_user_intereface.html#processing",
    "href": "perseus_user_intereface.html#processing",
    "title": "Perseus User Interface",
    "section": "0.2 Processing",
    "text": "0.2 Processing\n\n0.2.1 Basic\n\nTransform\nCombine expression columns\nColumn correlation\nRow correlation\nSummary statistics (columns)\nSummary statistics (rows)\nQuantiles\nDensity estimation\nPerformance curve\nClone\nSignificance A\nSignificance B\nAdd noise\n\n\n\n0.2.2 Rearrange\n\nChange column type\nRename columns\nRename columns (reg. ex.)\nCombine annotations\nDuplicate columns\nReorder/remove columns\nRemove empty columns\nTranspose\nSort by column\nExpand multi-numeric and string columns\nUnique values\nConvert multi-numeric column\nCombine categorical columns\nProcess text column\nSearch text column\n\n\n\n0.2.3 Normalization\n\nZ-score\nRank\nUnit vectors\nScale to interval\nWidth adjustment\nSubtract\nDivide\nModify by column\nSubtract row cluster\nUn-Z-score\n\n\n\n0.2.4 Filter rows\n\nFilter rows based on categorical column\nFilter rows based on numerical/expression column\nFilter rows based on text column\nFilter rows based on valid values\nFilter rows based on random sampling\n\n\n\n0.2.5 Filter columns\n\nFilter columns based on categorical row\nFilter columns based on valid values\n\n\n\n0.2.6 Quality\n\nCreate quality matrix\nFilter quality\nConvert to NaN\n\n\n\n0.2.7 Annot. columns\n\nAdd annotation\nTo base identifiers\nFisher exact test\nAverage categories\nCategory counting\n1D annotation enrichment\n2D annotation enrichment\n\n\n\n0.2.8 Annot. rows\n\nCategorical annotation rows\nNumerical annotation rows\nAverage groups\nJoin terms in categorical row\n\n\n\n0.2.9 Tests\n\nOne-sample tests\nTwo-samples tests\nMultiple-samples tests\nTwo-way ANOVA\n\n\n\n0.2.10 Imputation\n\nReplace missing values from normal distribution\nReplace imputed values by NaN\nReplace missing values by constant\n\n\n\n0.2.11 Modifications\n\nExpand site table\nAdd linear motifs\nAdd modification counts\nAdd known sites\nKinase-substrate relations\nAdd sequence features\nAdd regulatory sites\nShorten motif length\n\n\n\n0.2.12 Clustering\n\nGeneric clustering"
  },
  {
    "objectID": "perseus_user_intereface.html#analysis",
    "href": "perseus_user_intereface.html#analysis",
    "title": "Perseus User Interface",
    "section": "0.3 Analysis",
    "text": "0.3 Analysis\n\n0.3.1 Visualization\n\nScatter plot\nProfile plot\nHistogram\nMulti scatter plot\n3D plot\n\n\n\n0.3.2 Clustering/PCA\n\nHierarchical clustering\nPrinciple component analysis\n\n\n\n0.3.3 Misc.\n\nVolcano plot\nSelect rows manually\nSequence logos\nNumeric venn diagram"
  },
  {
    "objectID": "perseus_user_intereface.html#multi-proc.",
    "href": "perseus_user_intereface.html#multi-proc.",
    "title": "Perseus User Interface",
    "section": "0.4 Multi-proc.",
    "text": "0.4 Multi-proc.\n\n0.4.1 Basic\n\nMatching rows by name\nMatching columns by name"
  },
  {
    "objectID": "perseus_user_intereface.html#export",
    "href": "perseus_user_intereface.html#export",
    "title": "Perseus User Interface",
    "section": "0.5 Export",
    "text": "0.5 Export\n\nGeneric matrix export"
  },
  {
    "objectID": "createnumericalannotrow.html",
    "href": "createnumericalannotrow.html",
    "title": "Numerical annotation rows",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. rows\nSource code: CreateNumericalAnnotRow.cs\n\n\n\nAdd or edit numerical annotation rows. This could for instance define the times of samples for time series data.\nOutput: Same matrix with numerical annotation row added or modified."
  },
  {
    "objectID": "createnumericalannotrow.html#brief-description",
    "href": "createnumericalannotrow.html#brief-description",
    "title": "Numerical annotation rows",
    "section": "",
    "text": "Add or edit numerical annotation rows. This could for instance define the times of samples for time series data.\nOutput: Same matrix with numerical annotation row added or modified."
  },
  {
    "objectID": "createnumericalannotrow.html#action",
    "href": "createnumericalannotrow.html#action",
    "title": "Numerical annotation rows",
    "section": "2.1 Action",
    "text": "2.1 Action\nDefines the action that should be applied to a numerical annotation row (default: Create). The action can be selected from a predefined list:\n\nCreate\nEdit\nRename\nDelete\n\nEach of the above listed options has different parameters, which are explained below in more detail and grouped according to the action.\n\n2.1.1 Create\n\n2.1.1.1 Row name\nThis parameter is just relevant, if “Action” is set to “Create”. It defines the name of the new generated numerical annotation row (default: Quantity1).\n\n\n2.1.1.2 Here: Column 1 … Column 12\nThis parameter is just relevant, if “Action” is set to “Create”. For each of the expression columns in the matrix the numerical value of that column in the numerical annotation row can be specified (default: each expression column has an own numerical group indicated by a number).\n\n\n\n2.1.2 Edit\n\n2.1.2.1 Numerical row\nThis parameter is just relevant, if “Action” is set to “Edit”. It specifies the selected numerical row that should be edited (default: first numerical column in the matrix).\n\n\n2.1.2.2 Here: Column 1…Column 12\nThis parameter is just relevant, if “Action” is set to “Edit”. For each of the expression columns in the matrix the value in the numerical row can be edited by typing into the defined text field after the column name (default: numerical value of each expression column in that row).\n\n\n\n2.1.3 Rename\n\n2.1.3.1 Numerical row\nThis parameter is just relevant, if “Action” is set to “Rename”. It specifies the selected numerical row that should be renamed (default: first numerical row in the matrix).\n\n\n2.1.3.2 New name\nThis parameter is just relevant, if “Action” is set to “Rename”. It defines the new name of the numerical row (default: empty).\n\n\n2.1.3.3 New description\nThis parameter is just relevant, if “Action” is set to “Rename”. It defines the new description of the numerical row (default: empty).\n\n\n\n2.1.4 Delete\n\n2.1.4.1 Numerical row\nThis parameter is just relevant, if “Action” is set to “Delete”. It specifies the selected categorical row that should be deleted (default: first category row in the matrix)."
  },
  {
    "objectID": "addsequencefeatures.html",
    "href": "addsequencefeatures.html",
    "title": "Add sequence features",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Modifications\nSource code: AddSequenceFeatures.cs"
  },
  {
    "objectID": "addsequencefeatures.html#proteins",
    "href": "addsequencefeatures.html#proteins",
    "title": "Add sequence features",
    "section": "3.1 Proteins",
    "text": "3.1 Proteins\nSelected text column containing the Uniprot IDs (default: first text column of the matrix)."
  },
  {
    "objectID": "addsequencefeatures.html#positions-within-proteins",
    "href": "addsequencefeatures.html#positions-within-proteins",
    "title": "Add sequence features",
    "section": "3.2 Positions within proteins",
    "text": "3.2 Positions within proteins\nSelected text column containing the positions within the proteins to add site-specific features (default: first text column of the matrix). The column is generated by MaxQuant and is called “Positions”."
  },
  {
    "objectID": "addsequencefeatures.html#add-status-column",
    "href": "addsequencefeatures.html#add-status-column",
    "title": "Add sequence features",
    "section": "3.3 Add status column",
    "text": "3.3 Add status column\nIf checked additional information of Uniprot about the protein sites is added (default: unchecked)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cox Docs",
    "section": "",
    "text": "Welcome to coxdocs.org, the home of the documentation for\n\nMaxQuant\nPerseus\nAndromeda"
  },
  {
    "objectID": "selectrowsmanually.html",
    "href": "selectrowsmanually.html",
    "title": "Select rows manually",
    "section": "",
    "text": "1 General\n\nType: - Matrix Analysis\nHeading: - Misc. (Analysis)\nSource code: SelectRowsManually.cs\n\n\n\n2 Brief description\nRows can be selected interactively and a new matrix can be produced which contains only the selected rows or only the unselected rows.\n\n\n\n3 Parameters\n“Select rows manually” has no parameters."
  },
  {
    "objectID": "ClusterNorm.html",
    "href": "ClusterNorm.html",
    "title": "Cluster normalization",
    "section": "",
    "text": "1 General =====\n\nType: - Matrix Processing\nHeading: - Normalization\nSource code: not public.\n\n\n\n2 Brief description\nPerforms mean subtraction on a set of row clusters."
  },
  {
    "objectID": "createcategoricalannotrow.html",
    "href": "createcategoricalannotrow.html",
    "title": "Categorical annotation rows",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. rows\nSource code: CreateCategoricalAnnotRow.cs"
  },
  {
    "objectID": "createcategoricalannotrow.html#action",
    "href": "createcategoricalannotrow.html#action",
    "title": "Categorical annotation rows",
    "section": "3.1 Action",
    "text": "3.1 Action\nDefines the action that should be applied to a categorical annotation row (default: Create). The action can be selected from a predefined list:\n\nCreate\nCreate from experiment name\nEdit\nRename\nDelete\nWrite template file\nRead from file\n\nEach of the above listed options has different parameters, which are explained below in more detail and grouped according to the action.\n\n3.1.1 Create\n\n3.1.1.1 Row name\nThis parameter is just relevant, if “Action” is set to “Create”. It defines the name of the new generated categorical annotation row (default: Group1).\n\n\n3.1.1.2 Here: Column 1 … Column 12\nThis parameter is just relevant, if “Action” is set to “Create”. For each of the expression columns in the matrix the category value of that column in the categorical annotation row can be specified (default: each expression column has its own category indicated by the name of the expression column).\n\n\n\n3.1.2 Create from experiment name\n\n3.1.2.1 Pattern\nThis parameter is just relevant, if “Action” is set to “Create from experiment name”. It specifies the pattern that will be applied to the column names to group the columns and generate a new categorical annotation row (default: …_01,02,03). The Pattern can be selected from a predefined list:\n\n…_01,02,03\n(LFQ) intensity …_01,02,03\n(Normalized) ratio H/L …_01,02,03\nmatch regular expression\nreplace regular expression\n\n\n\n3.1.2.2 Regex\nThis parameter is just relevant, if “Action” is set to “Create from experiment name” and the parameter “Pattern” is set to “match regular expression” or “replace regular expression” (default: empty text field). Here a regular expression can be typed in, which should be applied to the column names to group the columns. The general rules for regular expressions apply here.\n\n\n3.1.2.3 Replace with\nThis parameter is just relevant, if “Action” is set to “Create from experiment name” and the parameter “Pattern” is set to “replace regular expression” (default: empty text field). Columns matching the defined regular expression in the field “Regex” get the value specified in the “Replace with” field and are therefore grouped.\n\n\n\n3.1.3 Edit\n\n3.1.3.1 Category row\nThis parameter is just relevant, if “Action” is set to “Edit”. It specifies the selected categorical row that should be edited (default: first categorical column in the matrix).\n\n\n3.1.3.2 Here: Column 1…Column 12\nThis parameter is just relevant, if “Action” is set to “Edit”. For each of the expression columns in the matrix the value in the categorical row can be edited by typing into the defined text field after the column name (default: category values of each expression column in that row).\n\n\n\n3.1.4 Rename\n\n3.1.4.1 Category row\nThis parameter is just relevant, if “Action” is set to “Rename”. It specifies the selected categorical row that should be renamed (default: first category row in the matrix).\n\n\n3.1.4.2 New name\nThis parameter is just relevant, if “Action” is set to “Rename”. It defines the new name of the categorical row (default: empty).\n\n\n3.1.4.3 New description\nThis parameter is just relevant, if “Action” is set to “Rename”. It defines the new description of the categorical row (default: empty).\n\n\n\n3.1.5 Delete\n\n3.1.5.1 Category row\nThis parameter is just relevant, if “Action” is set to “Delete”. It specifies the selected categorical row that should be deleted (default: first category row in the matrix).\n\n\n\n3.1.6 Write template file\n\n3.1.6.1 Output file\nThis parameter is just relevant, if “Action” is set to “Write template file”. It specifies the file name and path, where a grouping template of a categorical annotation rows is saved in a tab separated text file (default: Groups.txt). The first column of the output file is named “Name” and contains the names of the columns. The second column has the column names as values and can be edited manually. After editing the file can be read using “Read from file” (see below).\n\n\n\n3.1.7 Read from file\n\n3.1.7.1 Input file\nThis parameter is just relevant, if “Action” is set to “Read from file”. It defines the file name and path of a tab separated file containing information about a new grouping of the columns of a matrix (default: empty). The first column is called “Name” and contains the names of the columns of the matrix. The second column has the name of the new grouping and contains the values of each column of the matrix."
  },
  {
    "objectID": "divide.html",
    "href": "divide.html",
    "title": "Divide",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: Divide.cs"
  },
  {
    "objectID": "divide.html#matrix-access",
    "href": "divide.html#matrix-access",
    "title": "Divide",
    "section": "3.1 Matrix access",
    "text": "3.1 Matrix access\nSpecifies whether the division is performed on rows or columns (default: Rows)."
  },
  {
    "objectID": "divide.html#divide-by-what",
    "href": "divide.html#divide-by-what",
    "title": "Divide",
    "section": "3.2 Divide by what",
    "text": "3.2 Divide by what\nDefines by what value all entries in expression columns should be divided (default: Median). The divisor can be selected from a predefined list:\n\nSum\nMean\nMedian\nMost frequent value\nTukey’s biweight"
  },
  {
    "objectID": "expandsitetable.html",
    "href": "expandsitetable.html",
    "title": "Expand site table",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Modifications\nSource code: ExpandSiteTable.cs\n\n\n\n2 Brief description\nThe ___1, ___2 and ___3 versions of MaxQuant output table columns are rearranged in the matrix to become a single column each.\n\n\n\n3 Parameters\n“Expand site table” has no parameters."
  },
  {
    "objectID": "viewer_instructions.html",
    "href": "viewer_instructions.html",
    "title": "The Viewer",
    "section": "",
    "text": "Modern software platforms enable the analysis of shotgun proteomics data in an automated fashion resulting in high quality identification and quantification results. Additional understanding of the underlying data can be gained with the help of advanced visualization tools that allow for easy navigation through large LC-MS/MS datasets potentially consisting of terabytes of raw data. The updated MaxQuant version has a map navigation component that steers the users through mass and retention time-dependent mass spectrometric signals. It can be used to monitor a peptide feature used in label-free quantification over many LC-MS runs and visualize it with advanced 3D graphic models. An expert annotation system aids the interpretation of the MS/MS spectra used for the identification of these peptide features.\nTo run MaxQuant with its Viewer module, you will need .NET framework 4.5. If you use Windows you should have Windows 10 or newer.\n\n1 Documentation outline\n\nDownload and installation\nUser interface\nGetting started\nTutorial\nTrouble shooting\nGoogle groups\nmaxquant Bug reporting\nGlossary\n\nYou can find raw files and processed data to test the Viewer (v. 1.5.2.8) here.\nImportant note: If you load the mqpar.xml file, make sure all file paths are correct! If needed, update them using the “Change folder” option.\nFor additional training, consider attending our annual MaxQuant Summer School.\nAlso watching some MaxQuant videos on our video channel provides more insight.\nFor more details and better understanding of the viewer, you might be interested in reading our publication1.\n\n\n\n\n\nReferences\n\n1. Tyanova, S. et al. Visualization of LC-MS/MS proteomics data in MaxQuant. PROTEOMICS 15, 1453–1456 (2015)."
  },
  {
    "objectID": "principalcomponentanalysis.html",
    "href": "principalcomponentanalysis.html",
    "title": "Principle component analysis",
    "section": "",
    "text": "Type: - Matrix Analysis\nHeading: - Clustering/PCA\nSource code: not public."
  },
  {
    "objectID": "principalcomponentanalysis.html#category-enrichment-in-components",
    "href": "principalcomponentanalysis.html#category-enrichment-in-components",
    "title": "Principle component analysis",
    "section": "3.1 Category enrichment in components",
    "text": "3.1 Category enrichment in components\nSpecifies, whether each term in all categorical columns will be tested for enrichment at high or low values in the loadings for the first few components (default: unchecked).\n\n3.1.1 Number of components\nThis parameter is just relevant, if the parameter “Category enrichment in components” is checked. It specifies the number of principal components that will be used for the transformation (default: 5).\n\n\n3.1.2 Cutoff method\nThis parameter is just relevant, if the parameter “Category enrichment in components” is checked. It defines the method that is used to calculate the cutoff of potential principal components (default: Benjamini-Hochberg FDR) . The method can be selected from a predefined list:\n\nBenjamini-Hochberg FDR\np-value\n\n\n\n3.1.3 Threshold\nThis parameter is just relevant, if the parameter “Category enrichment in components” is checked. It defines the threshold value of the previously specified used cutoff method (default: 0.05).\n\n\n3.1.4 Relative enrichment\nThis parameter is just relevant, if the parameter “Category enrichment in components” is checked. It defines, which categorical column will be used to group rows in the enrichment test (default: &lt;None&gt;). All rows having the same identifier will be counted as one entity in the enrichment test. The main application is for posttranslational modification sites. Then one should select here protein or gene identifiers. This will make sure that multiple sites from the same protein (or gene) are counted only once for the enrichment analysis."
  },
  {
    "objectID": "annotateproteins.html",
    "href": "annotateproteins.html",
    "title": "Proteomic ruler: Annotate proteins",
    "section": "",
    "text": "If you arrived here directly from Perseus, it is a good idea to read the Proteomic ruler overview first.\nFor those that want to apply the proteomic ruler concept and are in a hurry: If you imported the columns Sequence length and Molecular weight of your proteinGroups.txt, you can skip this and directly estimate copy numbers."
  },
  {
    "objectID": "annotateproteins.html#input",
    "href": "annotateproteins.html#input",
    "title": "Proteomic ruler: Annotate proteins",
    "section": "2.1 Input",
    "text": "2.1 Input\n\n2.1.1 Protein IDs\nSelect the column containing your semicolon-separated protein group IDs (UniProt format). It is recommended to use the ‘Majority protein IDs’ when coming from MaxQuant\n\n\n2.1.2 Fasta file\nSpecify the uniprot fasta file you used to process your dataset. The plugin will parse this file and extract information from the header and the amino acid sequences."
  },
  {
    "objectID": "annotateproteins.html#output",
    "href": "annotateproteins.html#output",
    "title": "Proteomic ruler: Annotate proteins",
    "section": "2.2 Output",
    "text": "2.2 Output\nAs one often has more than one uniprot ID per protein group, you can specify whether you want to extract annotations and calculate sequence properties for the leading ID alone or for all IDs in the protein group. In case of text annotations, all annotations will be semicolon-separated. In case of numeric properties, the plugin will average over the list of sequence by reporting the median.\n\n2.2.1 Fasta header annotations\n\nEntry name: e.g. KAL1L_HUMAN\nGene name, e.g. KANSL1L\nProtein name (verbose), e.g. KAT8 regulatory NSL complex subunit 1-like protein\nProtein name (consensus), e.g. Isoform 2 of KAT8 regulatory NSL complex subunit 1-like protein. The consensus protein names will be stripped of all Isoform xy of prefixes and (Fragment) suffixes.\nSpecies, e.g. Homo sapiens\n\n\n\n2.2.2 Numeric annotations\n\nSequence length\nMonoisotopic molecular mass\nAverage molecular mass\n\n\n\n2.2.3 Calculate theoretical peptides\nThe plugin will perform an in-silico digestion of the protein sequences with the specified protease and report the number of theoretically expected peptides without miscleavages in the selected size range. ### Count sequence features\nThe plugin will count the number of occurrences of a given regular expression in the protein sequences. It is recommended to normalize this count by the sequence length if you want to average across all IDs."
  },
  {
    "objectID": "fisherexacttestproces.html",
    "href": "fisherexacttestproces.html",
    "title": "Fisher exact test",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. columns\nSource code: not public."
  },
  {
    "objectID": "fisherexacttestproces.html#input-type",
    "href": "fisherexacttestproces.html#input-type",
    "title": "Fisher exact test",
    "section": "3.1 Input type",
    "text": "3.1 Input type\nSpecification whether the sub-population that is tested for contingency against all other categorical columns is taken directly from a categorical column (“Categorical column” is default) or, if it is defined by a threshold on a numerical/expression column (“Numerical column”).\n\n3.1.1 Column\nThis parameter is just relevant, if “Input type” is set to “Categorical column”. The selected categorical column is checked against all other categorical columns for association between the occurrence of terms (default: first categorical column in the matrix).\n\n\n3.1.2 Columns\nThis parameter is just relevant, if “Input type” is set to “Numerical column”. The selected expression/numerical columns are used as a threshold to define the set of interest. The set is then checked against all categorical columns for association between the occurrence of terms (default: all expression and numerical columns are selected).\n\n\n3.1.3 Threshold\nThis parameter is just relevant, if “Input type” is set to “Numerical column”. It defines the threshold, which rows are kept/discarded to define the set of interest (default: 2).\n\n\n3.1.4 Selection is\nThis parameter is just relevant, if “Input type” is set to “Numerical column”. It defines, whether the values in the selected “Columns” should be “Larger than threshold” or “Less than threshold” (default: Larger than threshold)."
  },
  {
    "objectID": "fisherexacttestproces.html#sec-truncation",
    "href": "fisherexacttestproces.html#sec-truncation",
    "title": "Fisher exact test",
    "section": "3.2 Use for truncation",
    "text": "3.2 Use for truncation\nThe truncation can be based on p-values or the Benjamini-Hochberg correction for multiple hypothesis testing (default: Benjamini-Hochberg, FDR). Rows with a test result below a specified value (Section 3.3) are reported as significant."
  },
  {
    "objectID": "fisherexacttestproces.html#sec-threshold-value",
    "href": "fisherexacttestproces.html#sec-threshold-value",
    "title": "Fisher exact test",
    "section": "3.3 Threshold value",
    "text": "3.3 Threshold value\nBased on a specified threshold (default: 0.02) a specific row is reported as significant. Depending on the chosen truncation score (Section 3.2) this threshold value is applied to the p-value or to the Benjamini-Hochberg FDR."
  },
  {
    "objectID": "fisherexacttestproces.html#relative-enrichment",
    "href": "fisherexacttestproces.html#relative-enrichment",
    "title": "Fisher exact test",
    "section": "3.4 Relative enrichment",
    "text": "3.4 Relative enrichment\nSelected text column, where all rows having the same identifier will be counted as one entity in the Fisher exact test (default: ). The main application is for post-translational modification sites. Then one should select here protein or gene identifiers. This will make sure that multiple sites from the same protein (or gene) are counted only once for the enrichment analysis."
  },
  {
    "objectID": "matchingcolumnsbyname.html",
    "href": "matchingcolumnsbyname.html",
    "title": "Matching columns by name",
    "section": "",
    "text": "Type: - Matrix MultiProcessing\nHeading: - Basic (MultiProcessing)\nSource code: MatchingColumnsByName.cs"
  },
  {
    "objectID": "matchingcolumnsbyname.html#base-matrix",
    "href": "matchingcolumnsbyname.html#base-matrix",
    "title": "Matching columns by name",
    "section": "3.1 Base matrix",
    "text": "3.1 Base matrix\nSpecifies the matrix, whose columns are used as basis for the matching of the column names. If no matrix is selected the default value is empty, otherwise the default is the currently selected matrix. The selection can be changed by first clicking on “Base matrix” line of the pop-up window and then clicking on the matrix of choice in the middle pane."
  },
  {
    "objectID": "matchingcolumnsbyname.html#other-matrix",
    "href": "matchingcolumnsbyname.html#other-matrix",
    "title": "Matching columns by name",
    "section": "3.2 Other matrix",
    "text": "3.2 Other matrix\nSpecifies the matrix, whose columns are matched to the ones of the “Base matrix”. If no matrix is selected the default value is empty, otherwise the default is the currently selected matrix. The selection can be changed by first clicking on “Other matrix” line of the pop-up window and then clicking on the matrix of choice in the middle pane."
  },
  {
    "objectID": "MQ_FirstSteps.html",
    "href": "MQ_FirstSteps.html",
    "title": "First steps with MaxQuant",
    "section": "",
    "text": "The descriptions and screenshots in this tutorial refer to the MaxQuant GUI around version 1.4.3.14 from August 2014.\nIn case you are a first time user, you might be worried by the many options and parameters that one can set in the user interface. If this is the case, we have good news for you. In almost all user cases the standard values of most parameters are fine and you only need to adjust a small number of factors. Typically there is only little information that you need to provide. Every parameter in the interface has context help which you obtain by hovering with the mouse pointer over the text string for this parameter. This documentation also has that information, and more.\nYou will have to tell MaxQuant where to find your raw data files and your fasta files, and which labels and digestion enzymes you are using. 90% of the time that will be enough, and you can leave the rest of the bells and whistles on their default values. Of course, we are assuming you have successfully installed and started MaxQuant, and that you can find the Start button once you have finished entering the processing parameters (hint: look in the lower left corner of the GUI). Below are more details on how to enter those four most important parameters:"
  },
  {
    "objectID": "MQ_FirstSteps.html#entering-raw-data",
    "href": "MQ_FirstSteps.html#entering-raw-data",
    "title": "First steps with MaxQuant",
    "section": "1 Entering Raw data",
    "text": "1 Entering Raw data\nAlong the top of the GUI there are six tabs. The first of these is Raw files. Select that tab, then click “Load” to open a browser where you can select the file containing your raw data. You can select any number of files. You can also select all the raw data files in a single folder by using the “Load folder” button (s. Figure 1). In the screen shot, two files have been loaded. (If you still have questions, there is a page that goes into details about the Raw Files tab)\n\n\n\nFigure 1: raw Files"
  },
  {
    "objectID": "MQ_FirstSteps.html#the-labels",
    "href": "MQ_FirstSteps.html#the-labels",
    "title": "First steps with MaxQuant",
    "section": "2 The Labels",
    "text": "2 The Labels\nThe second tab is Group-specific parameters. You need to go there to specify your labels. The “Type” will usually be “Standard” (obviously), and the “Multiplicity” will be \\(1\\) for label-free quantification, \\(2\\) if you have light and heavy labels, and \\(3\\) if you have light, medium, and heavy. Any number of labels can be checked in the lists. In this example, the light sample is unlabeled and the heavy sample has been labeled with Arg10 and Lys8. (You can’t see them both without scrolling. Figure 2)\n\n\n\nFigure 2: Group-specific parameters"
  },
  {
    "objectID": "MQ_FirstSteps.html#enzymes",
    "href": "MQ_FirstSteps.html#enzymes",
    "title": "First steps with MaxQuant",
    "section": "3 Enzymes",
    "text": "3 Enzymes\nAlso under the Group-specific parameters tab, you should enter the Enzyme(s) (one or more) with which you have digested your proteins. They can be moved from the list at the left and back with the arrow buttons, and the order changed with the “t” (to top), “u” (up one place), “d” (down one place), and “b” (to bottom) buttons. In this example (s. Figure 3), we used Trypsin/P.\n\n\n\nFigure 3: Enzymes"
  },
  {
    "objectID": "MQ_FirstSteps.html#load-fasta-file",
    "href": "MQ_FirstSteps.html#load-fasta-file",
    "title": "First steps with MaxQuant",
    "section": "4 Load FASTA file",
    "text": "4 Load FASTA file\nFinally you will need to go to the third tab (s. Figure 4), “Global parameters” to specify where to find the Fasta files (one or more) you want to use. The Add file button will open a file browser.\n\n\n\nFigure 4: fasta files"
  },
  {
    "objectID": "MQ_FirstSteps.html#start-the-analysis",
    "href": "MQ_FirstSteps.html#start-the-analysis",
    "title": "First steps with MaxQuant",
    "section": "5 Start the Analysis",
    "text": "5 Start the Analysis\nAfter you press the Start button, you can monitor the progress of the analysis under the fourth tab, Performance. A popup window saying ‘Done’ will appear when MaxQuant is finished. All result files will appear in the folder …\\combined\\txt as tab-delimited text files. A pdf document with a description of all columns in all tables will be written to …\\combined\\txt\\tables.pdf.\nThe fifth tab is the Viewer, used to examine the results of the analysis, which will be the topic of the viewer tutorial. All columns have interactive descriptions in the Viewer program. Just move the mouse over the beginning of the column title and click on the question mark that will appear.\nThe sixth and final tab is Andromeda configuration. Andromeda is the peptide search engine, which needs to know what modifications, proteases, and sequence databases to take into consideration. Almost all that you will ever need are pre-configured, but you can use the buttons under this tab to add more, if your experiment requires it.\nThat’s how easy it can be!\n\nFor question or problems with the running MqxQuant we would love to hear from you under Contact."
  },
  {
    "objectID": "uniquevalues.html",
    "href": "uniquevalues.html",
    "title": "Unique Values",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: UniqueValues.cs"
  },
  {
    "objectID": "uniquevalues.html#text-columns",
    "href": "uniquevalues.html#text-columns",
    "title": "Unique Values",
    "section": "3.1 Text columns",
    "text": "3.1 Text columns\nSelected text columns, whose values should be made unique (default: no columns are selected)."
  },
  {
    "objectID": "perseus_download_guide.html",
    "href": "perseus_download_guide.html",
    "title": "Perseus Download and Installation",
    "section": "",
    "text": "The latest Perseus release is: 2.0.11 [date: 29.09.2023]"
  },
  {
    "objectID": "perseus_download_guide.html#download",
    "href": "perseus_download_guide.html#download",
    "title": "Perseus Download and Installation",
    "section": "1 Download",
    "text": "1 Download\nDownloading and using the software is free of charge.\nSimply download from our MQ home page and unpack the compressed file Perseus.zip."
  },
  {
    "objectID": "perseus_download_guide.html#requierments",
    "href": "perseus_download_guide.html#requierments",
    "title": "Perseus Download and Installation",
    "section": "2 requierments",
    "text": "2 requierments\nSupported operation system versions (64 bit is required) are Windows 10 or 11 or Windows Server 2016, 2019, 2022.\nInstall .NET Framework 4.7.2 or higher - To find out whether you already have it, follow the instructions on How to Determine Which .NET Framework Versions Are Installed. If you need to, you can download the software and get installation instructions at the Microsoft Download Center."
  },
  {
    "objectID": "perseus_download_guide.html#running",
    "href": "perseus_download_guide.html#running",
    "title": "Perseus Download and Installation",
    "section": "3 Running",
    "text": "3 Running\nTo run Perseus the GUI just double click on Perseus.exe in the Perseus folder."
  },
  {
    "objectID": "perseus_download_guide.html#hardware-requirements",
    "href": "perseus_download_guide.html#hardware-requirements",
    "title": "Perseus Download and Installation",
    "section": "4 Hardware requirements",
    "text": "4 Hardware requirements\n\nIntel Pentium III/800 MHz or higher (or compatible) although one should probably not go below a dual core processor.\n2 GB RAM minimum.\n2 GB RAM per thread that is executed in parallel is required.\nThere is no upper limit on the number of cores. Whatever you can fit into a shared memory machine will work as long as the disk performance scales up with it."
  },
  {
    "objectID": "viewer_tutorials.html",
    "href": "viewer_tutorials.html",
    "title": "Viewer - Tutorials",
    "section": "",
    "text": "Tutorials explaining how to use the viewer can be found in our YouTube channel.\nA few examples are listed below:\nMaxQuant Viewer tutorials\nFrom our 2023 MaxQunat Summer School in Boston\n\nMaxQuant basic I + II\nFrom our 2022 MaxQunat Summer School in Barcelona\n\nMaxQuant Basics I and II\nFrom our online MaxQunat SummerSchool in 2021.\n\n\nMaxQuant Viewer\nFrom our 2019 MaxQunat Summer School in Madison"
  },
  {
    "objectID": "filtercategoricalrow.html",
    "href": "filtercategoricalrow.html",
    "title": "Filter columns based on categorical row",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Filter columns\nSource code: FilterCategoricalRow.cs"
  },
  {
    "objectID": "filtercategoricalrow.html#row",
    "href": "filtercategoricalrow.html#row",
    "title": "Filter columns based on categorical row",
    "section": "3.1 Row",
    "text": "3.1 Row\nSelected categorical row the filtering is based on (default: first categorical row in the matrix).\n\n3.1.1 Values\nSelected values out of all values of the specified categorical row that should be present to keep/discard the column (default: no values are selected)."
  },
  {
    "objectID": "filtercategoricalrow.html#mode",
    "href": "filtercategoricalrow.html#mode",
    "title": "Filter columns based on categorical row",
    "section": "3.2 Mode",
    "text": "3.2 Mode\nThe columns with the selected values will be kept/discarded depending on the selected “Mode” (default: “Remove matching columns”). If “Remove matching columns” is selected, rows having the values will be removed while all other columns will be kept. If “Keep matching columns” is selected, the opposite will happen."
  },
  {
    "objectID": "filtercategoricalrow.html#filter-mode",
    "href": "filtercategoricalrow.html#filter-mode",
    "title": "Filter columns based on categorical row",
    "section": "3.3 Filter mode",
    "text": "3.3 Filter mode\nThe “Filter mode” defines, whether the input matrix will be reduced (“Reduce matrix” = default) or a new categorical row called “Filter” will be generated containing the categories “Keep” and “Discard” (“Filter mode” = “Add categorical row”)."
  },
  {
    "objectID": "replacemissingfromgaussian.html",
    "href": "replacemissingfromgaussian.html",
    "title": "Replace missing values from normal distribution",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Imputation\nSource code: ReplaceMissingFromGaussian.cs"
  },
  {
    "objectID": "replacemissingfromgaussian.html#width",
    "href": "replacemissingfromgaussian.html#width",
    "title": "Replace missing values from normal distribution",
    "section": "3.1 Width",
    "text": "3.1 Width\nDefines the width of the Gaussian distribution relative to the standard deviation of measured values (default: 0.3). A value of 0.5 would mean that the width of the distribution used for drawing random numbers is half of the standard deviation of the data."
  },
  {
    "objectID": "replacemissingfromgaussian.html#down-shift",
    "href": "replacemissingfromgaussian.html#down-shift",
    "title": "Replace missing values from normal distribution",
    "section": "3.2 Down shift",
    "text": "3.2 Down shift\nSpecifies the amount by which the distribution used for the random numbers is shifted downwards (default: 1.8). This is in units of the standard deviation of the valid data."
  },
  {
    "objectID": "replacemissingfromgaussian.html#mode",
    "href": "replacemissingfromgaussian.html#mode",
    "title": "Replace missing values from normal distribution",
    "section": "3.3 Mode",
    "text": "3.3 Mode\nSpecifies whether the replacement of missing values should be applied to each expression column separately (default) or on the whole matrix at once (“Total matrix”)."
  },
  {
    "objectID": "replacemissingfromgaussian.html#columns",
    "href": "replacemissingfromgaussian.html#columns",
    "title": "Replace missing values from normal distribution",
    "section": "3.4 Columns",
    "text": "3.4 Columns\nSelected expression columns, where missing values should be replaced (default: all expression columns are selected)."
  },
  {
    "objectID": "twosampletestprocessing.html",
    "href": "twosampletestprocessing.html",
    "title": "Two-sample tests",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Tests\nSource code: not public."
  },
  {
    "objectID": "twosampletestprocessing.html#grouping",
    "href": "twosampletestprocessing.html#grouping",
    "title": "Two-sample tests",
    "section": "3.1 Grouping",
    "text": "3.1 Grouping\nThe grouping(s) of columns to be used in the test. Each test takes two groups as input. Multiple tests can be performed simultaneously by specifying more than one pair of groups.\n\n3.1.1 First group (right)\nAll ‘right’ groups of the two sample tests are defined here. The number of groups selected here equals the number of different tests performed.\n\n\n3.1.2 Second groups mode\nSpecify here how the ‘left’ groups of the two sample tests are specified. Possible ways are to specify for each individual ‘right’ group the corresponding ‘left’ group, to use one single control group, or to always use the complement of each individual ‘right’ group as the ‘left’ group."
  },
  {
    "objectID": "twosampletestprocessing.html#test",
    "href": "twosampletestprocessing.html#test",
    "title": "Two-sample tests",
    "section": "3.2 Test",
    "text": "3.2 Test\nDefines what kind of test should be applied (default: T-test). The test can be selected from a predefined list:\n\nT-test\nWelch test\n\n\n3.2.1 S0\nThis parameter defines the artificial within groups variance (default: 0). It controls the relative importance of the resulted p-value and difference between means. At \\(s0=0\\) only the p-value matters, while at nonzero s0 also the difference of means plays a role. See (Tusher, Tibshirani, and Chu 2001) for details.\n\n\n3.2.2 Side\nTo apply a two-sided test, where the null hypothesis can be rejected regardless of the direction of the effect “both” has to be selected (default). “left” and “right” are the respective one-sided tests."
  },
  {
    "objectID": "twosampletestprocessing.html#valid-value-filter",
    "href": "twosampletestprocessing.html#valid-value-filter",
    "title": "Two-sample tests",
    "section": "3.3 Valid value filter",
    "text": "3.3 Valid value filter\nSpecify here how rows are filtered regarding the number and percentage of valid values. This criterion will be applied to each test individually, not just once to the whole matrix. The absolute number and relative percentage filters are both applied together.\n\n3.3.1 Min. number of valid values\nHere the required number of valid values is specified. How this threshold is applied (in total, per group, etc.) is specified in the next field.\n\n\n3.3.2 Min. number mode\nSpecify here how the threshold is applied.\n\n\n3.3.3 Min. percentage of valid values\nHere the required percentage of valid values is specified. How this threshold is applied (in total, per group, etc.) is specified in the next field. Values can range from 0 to 100.\n\n\n3.3.4 Min. percentage mode\nSpecify here how the above threshold is applied."
  },
  {
    "objectID": "twosampletestprocessing.html#use-for-truncation",
    "href": "twosampletestprocessing.html#use-for-truncation",
    "title": "Two-sample tests",
    "section": "3.4 Use for truncation",
    "text": "3.4 Use for truncation\nDefines on what value the truncation is based on (default: Permutation-based FDR). Choose here whether the truncation should be based on the p-values, on permutation-based FDR values or, if the Benjamini-Hochberg correction for multiple hypothesis testing should be applied.\n\n3.4.1 Threshold p-value\nThis parameter is just relevant, if the parameter “Use for truncation” is set to “P-value”. Rows with a test result below this value are reported as significant (default: 0.05).\n\n\n3.4.2 FDR\nThis parameter is just relevant, if the parameter “Use for truncation” is set to “Benjamini-Hochberg FDR” or “Permutation-based FDR”. Rows with a test result below this value are reported as significant (default: 0.05).\n\n\n3.4.3 Number of randomizations\nSpecifies the number of randomizations that should be applied (default: 250).\n\n\n3.4.4 Preserve grouping in randomizations\nDefines, whether the grouping specified in a categorical row should be preserved in the randomizations (default: &lt;None&gt;). It can be selected from a list including all available groupings of the matrix."
  },
  {
    "objectID": "twosampletestprocessing.html#calculate-combined-score",
    "href": "twosampletestprocessing.html#calculate-combined-score",
    "title": "Two-sample tests",
    "section": "3.5 Calculate combined score",
    "text": "3.5 Calculate combined score\nIn case multiple two sample tests are performed, the combined score helps to define a global set of significant items over all the tests combined. A global q-value can be calculated based on permutations of the whole matrix.\n\n3.5.1 Mode\nHere the user can define the combined score which is either the p-value from the best test or the product over all tests.\n\n\n3.5.2 Combined q-value\nIn case this is checked, a combined q-value based on the combined score and permutations of the whole matrix is calculated."
  },
  {
    "objectID": "twosampletestprocessing.html#log10",
    "href": "twosampletestprocessing.html#log10",
    "title": "Two-sample tests",
    "section": "3.6 -Log10",
    "text": "3.6 -Log10\nIf checked, \\(-Log_{10}(test\\ value)\\) is reported in the output matrix (default). Otherwise the test-value is reported."
  },
  {
    "objectID": "twosampletestprocessing.html#suffix",
    "href": "twosampletestprocessing.html#suffix",
    "title": "Two-sample tests",
    "section": "3.7 Suffix",
    "text": "3.7 Suffix\nThe entered suffix will be attached to newly generated columns (default: empty). That way columns from multiple runs of the test can be distinguished more easily."
  },
  {
    "objectID": "createqualitymatrix.html",
    "href": "createqualitymatrix.html",
    "title": "Create quality matrix",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Quality\nSource code: CreateQualityMatrix.cs\n\n\n\n2 Brief description\nCreate a matrix of quality values from a set of numerical columns. There has to be one numerical column per expression column.\n\n\n\n3 Parameters\nTo each of the expression columns in the matrix one numerical column has to be assigned (default: to all expression columns the first numerical column in the matrix is assigned).\n\n\n4 Parameter window"
  },
  {
    "objectID": "averagegroups.html",
    "href": "averagegroups.html",
    "title": "Average Groups",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. rows\nSource code: AverageGroups.cs"
  },
  {
    "objectID": "averagegroups.html#grouping",
    "href": "averagegroups.html#grouping",
    "title": "Average Groups",
    "section": "3.1 Grouping",
    "text": "3.1 Grouping\nDefines, which grouping specified in a categorical row should be used to average values (default: first categorical row in the matrix)."
  },
  {
    "objectID": "averagegroups.html#average-type",
    "href": "averagegroups.html#average-type",
    "title": "Average Groups",
    "section": "3.2 Average type",
    "text": "3.2 Average type\nSpecifies which operation should be applied for the averaging (default: Median). The operation can be selected from a predefined list:\n\nMedian\nMean\nSum\nGeometric mean"
  },
  {
    "objectID": "averagegroups.html#min.-valid-values-per-group",
    "href": "averagegroups.html#min.-valid-values-per-group",
    "title": "Average Groups",
    "section": "3.3 Min. valid values per group",
    "text": "3.3 Min. valid values per group\nDefines the minimal values a group must contain to calculate the average (default: 1)."
  },
  {
    "objectID": "averagegroups.html#keep-original-data",
    "href": "averagegroups.html#keep-original-data",
    "title": "Average Groups",
    "section": "3.4 Keep original data",
    "text": "3.4 Keep original data\nIf checked the original data will be kept In the output matrix (default: unchecked)."
  },
  {
    "objectID": "averagegroups.html#add-variation",
    "href": "averagegroups.html#add-variation",
    "title": "Average Groups",
    "section": "3.5 Add variation",
    "text": "3.5 Add variation\nSpecifies, whether a measure of group-wise variation should be calculated and displayed in a numerical column (default: &lt;None&gt;). The measure can be selected from a predefined list:\n\n&lt;None&gt;\nStandard deviation\nError of mean"
  },
  {
    "objectID": "managenumericalannotrow.html",
    "href": "managenumericalannotrow.html",
    "title": "Numerical annotation rows",
    "section": "",
    "text": "1 General =====\n\nType: - Matrix Processing\nHeading: - Annot. rows\nSource code: ManageNumericalAnnotRow.cs\n\n===== Brief description =====\nAdd or edit numerical annotation rows. This could for instance define the times of samples for time series data.\nOutput: Same matrix with numerical annotation row added or modified."
  },
  {
    "objectID": "densityestimationprocessing.html",
    "href": "densityestimationprocessing.html",
    "title": "Density Estimation Processing",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: DensityEstimationProcessing.cs"
  },
  {
    "objectID": "densityestimationprocessing.html#x",
    "href": "densityestimationprocessing.html#x",
    "title": "Density Estimation Processing",
    "section": "3.1 x",
    "text": "3.1 x\nSelected expression columns for the first dimension of the generated density map(s) (default: first expression column in the matrix). Multiple columns can be chosen for each dimension leading to the creation of multiple density maps, but the number of columns have to be the same in both dimensions."
  },
  {
    "objectID": "densityestimationprocessing.html#y",
    "href": "densityestimationprocessing.html#y",
    "title": "Density Estimation Processing",
    "section": "3.2 y",
    "text": "3.2 y\nSelected expression columns for the second dimension of the generated density map(s) (default: second expression column in the matrix). Multiple columns can be chosen for each dimension leading to the creation of multiple density maps, but the number of columns have to be the same in both dimensions."
  },
  {
    "objectID": "densityestimationprocessing.html#number-of-points",
    "href": "densityestimationprocessing.html#number-of-points",
    "title": "Density Estimation Processing",
    "section": "3.3 Number of points",
    "text": "3.3 Number of points\nThe specified number of points defines the resolution of the density map (default: 300). It reflects the number of pixel per dimension."
  },
  {
    "objectID": "densityestimationprocessing.html#distribution-type",
    "href": "densityestimationprocessing.html#distribution-type",
    "title": "Density Estimation Processing",
    "section": "3.4 Distribution type",
    "text": "3.4 Distribution type\nEach data point is smoothed out by a suitable Gaussian kernel, which is defined in the “Distribution type” (default: \\(P(x,y)\\). The function can be selected out of a predefined list:\n\n\\(P(x,y)\\)\n\\(P(y|x)\\)\n\\(P(x|y)\\)\n\\(P(x,y)/(P(x)*P(y))\\)"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "contact",
    "section": "",
    "text": "MaxQuant, Perseus, and related software packages are developed by the Computational Systems Biochemistry under Prof. Jürgen Cox.\nIf you need to contact the developers, you have a few options to proceed:\n\nIf you think something is misspelled or unclear in the documentation, consider changing it yourself (creating a pull request is quite easy).\nIf you have found a bug in the software or want to make a feature request, please create an issue on our github repository.\n\nIf you have a general usage question, ask it on the appropriate Google group.\n\nWe are also always on the lookout for promising canditates. In case you are interested in becoming a Master’s/PhD student or a postdoc, we encourage you to apply at all time via e-mail to Jürgen Cox.\nOr connect with us on GitHub or Twitter (links in top right)."
  },
  {
    "objectID": "backtobaseidentifiers.html",
    "href": "backtobaseidentifiers.html",
    "title": "To base identifiers",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. columns\nSource code: BackToBaseIdentifiers.cs"
  },
  {
    "objectID": "backtobaseidentifiers.html#source",
    "href": "backtobaseidentifiers.html#source",
    "title": "To base identifiers",
    "section": "3.1 Source",
    "text": "3.1 Source\nSpecified path to the file containing the annotations that should be mapped back to the base identifiers (default: first file in Perseus-version /conf/annotations/). The file should be the same that was used to add the annotations, also it can be selected from all files in Perseus-version /conf/annotations/."
  },
  {
    "objectID": "backtobaseidentifiers.html#identifiers",
    "href": "backtobaseidentifiers.html#identifiers",
    "title": "To base identifiers",
    "section": "3.2 Identifiers",
    "text": "3.2 Identifiers\nSelected text column containing the identifiers that should be matched back to UniProt identifiers (default: first text column in the matrix).\nComment: Only text columns can be matched back not categorical ones, because there is no unique match."
  },
  {
    "objectID": "backtobaseidentifiers.html#identifier-type",
    "href": "backtobaseidentifiers.html#identifier-type",
    "title": "To base identifiers",
    "section": "3.3 Identifier type",
    "text": "3.3 Identifier type\nSelected type of the identifiers that will be mapped back (default: Gene name). The identifier type can be selected from a predefined list:\n\nENSG\nENSP\nENST\nFlybase\nGene name\nMGI\nPDB\nUniProt names\nWormbase\nEC\neggNOG\n\n==== Parameter window"
  },
  {
    "objectID": "perseus_plugins.html",
    "href": "perseus_plugins.html",
    "title": "Plug Ins for Perseus",
    "section": "",
    "text": "The “out-of-the-box” version of Perseus already provides much functionality, but it can easily be extended further with plugins. Standard activities are implemented in the same way as plugins.\n\n1 Plugin Store\nTo browse a list of available plugins, visit our Perseus Plugin Store.\n\n\n2 Developer’s Guide\nThe developers guide is available on plugintutorial on github."
  },
  {
    "objectID": "periodogram.html",
    "href": "periodogram.html",
    "title": "Periodogram",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Time series\nSource code: not public."
  },
  {
    "objectID": "periodogram.html#time-column",
    "href": "periodogram.html#time-column",
    "title": "Periodogram",
    "section": "3.1 Time column",
    "text": "3.1 Time column\nSelected numerical row, which defines the time points (default: first numerical row). Proteins are listed in rows, whereas columns represent a specific point in time."
  },
  {
    "objectID": "periodogram.html#separate-time-series",
    "href": "periodogram.html#separate-time-series",
    "title": "Periodogram",
    "section": "3.2 Separate time series",
    "text": "3.2 Separate time series\nSpecifies, if a periodogram should be calculated separately for each group of the defined grouping (default: &lt;None&gt;)."
  },
  {
    "objectID": "periodogram.html#in-vivo-quantitative-proteomics-reveals-a-key-contribution-of-post-transcriptional-mechanisms-to-the-circadian-regulation-of-liver-metabolism",
    "href": "periodogram.html#in-vivo-quantitative-proteomics-reveals-a-key-contribution-of-post-transcriptional-mechanisms-to-the-circadian-regulation-of-liver-metabolism",
    "title": "Periodogram",
    "section": "5.1 In-Vivo Quantitative Proteomics Reveals a Key Contribution of Post-Transcriptional Mechanisms to the Circadian Regulation of Liver Metabolism",
    "text": "5.1 In-Vivo Quantitative Proteomics Reveals a Key Contribution of Post-Transcriptional Mechanisms to the Circadian Regulation of Liver Metabolism\nThe algorithms were first applied in 2014 by Robles et. al.1.\nAbstract\nCircadian clocks are endogenous oscillators that drive the rhythmic expression of a broad array of genes, orchestrating metabolism and physiology. Recent evidence indicates that post-transcriptional and post-translational mechanisms play essential roles in modulating temporal gene expression for proper circadian function, particularly for the molecular mechanism of the clock. Due to technical limitations in large-scale, quantitative protein measurements, it remains unresolved to what extent the circadian clock regulates metabolism by driving rhythms of protein abundance. Therefore, we aimed to identify global circadian oscillations of the proteome in the mouse liver by applying in vivo SILAC mouse technology in combination with state of the art mass spectrometry. Among the 3000 proteins accurately quantified across two consecutive cycles, 6% showed circadian oscillations with a defined phase of expression. Interestingly, daily rhythms of one fifth of the liver proteins were not accompanied by changes at the transcript level. The oscillations of almost half of the cycling proteome were delayed by more than six hours with respect to the corresponding, rhythmic mRNA. Strikingly we observed that the length of the time lag between mRNA and protein cycles varies across the day. Our analysis revealed a high temporal coordination in the abundance of proteins involved in the same metabolic process, such as xenobiotic detoxification. Apart from liver specific metabolic pathways, we identified many other essential cellular processes in which protein levels are under circadian control, for instance vesicle trafficking and protein folding. Our large-scale proteomic analysis reveals thus that circadian post-transcriptional and post-translational mechanisms play a key role in the temporal orchestration of liver metabolism and physiology."
  },
  {
    "objectID": "estimateaccuracy.html",
    "href": "estimateaccuracy.html",
    "title": "Proteomic ruler: Estimate absolute protein quantification accuracy",
    "section": "",
    "text": "If you arrived here directly, it is a good idea to read the Proteomic ruler overview first."
  },
  {
    "objectID": "estimateaccuracy.html#total-number-of-peptides-select",
    "href": "estimateaccuracy.html#total-number-of-peptides-select",
    "title": "Proteomic ruler: Estimate absolute protein quantification accuracy",
    "section": "2.1 Total number of peptides Select",
    "text": "2.1 Total number of peptides Select\nThe Peptides column of the MaxQuant output table."
  },
  {
    "objectID": "estimateaccuracy.html#unique-razor-peptides",
    "href": "estimateaccuracy.html#unique-razor-peptides",
    "title": "Proteomic ruler: Estimate absolute protein quantification accuracy",
    "section": "2.2 Unique + razor peptides",
    "text": "2.2 Unique + razor peptides\nSelect the Unique + razor peptides column of the MaxQuant output table."
  },
  {
    "objectID": "estimateaccuracy.html#sequence-length",
    "href": "estimateaccuracy.html#sequence-length",
    "title": "Proteomic ruler: Estimate absolute protein quantification accuracy",
    "section": "2.3 Sequence length",
    "text": "2.3 Sequence length\nSelect either the Sequence length column of the MaxQuant output table or the corresponding annotation column generated by Annotate proteins."
  },
  {
    "objectID": "estimateaccuracy.html#number-of-theoretical-peptides",
    "href": "estimateaccuracy.html#number-of-theoretical-peptides",
    "title": "Proteomic ruler: Estimate absolute protein quantification accuracy",
    "section": "2.4 Number of theoretical peptides",
    "text": "2.4 Number of theoretical peptides\nSelect the corresponding annotation column generated by Annotate proteins."
  },
  {
    "objectID": "estimateaccuracy.html#confidence-class-thresholds",
    "href": "estimateaccuracy.html#confidence-class-thresholds",
    "title": "Proteomic ruler: Estimate absolute protein quantification accuracy",
    "section": "2.5 Confidence class thresholds",
    "text": "2.5 Confidence class thresholds\nFor the high and medium confidence class, specify the following thresholds. Every protein that does not fulfill the criteria for high or medium accuracy will be classified as low accuracy.\n\nMin. peptides: The minimum number of detected peptides.\nMin. razor fraction: The minimum ratio of razor+unique/total peptides.\nMin. theoretical peptides per 100 amino acids."
  },
  {
    "objectID": "scatterplotanalysis.html",
    "href": "scatterplotanalysis.html",
    "title": "Scatter plot analysis",
    "section": "",
    "text": "Type: - Matrix Analysis\nHeading: - Visualization\nSource code: not public."
  },
  {
    "objectID": "scatterplotanalysis.html#matrix-access",
    "href": "scatterplotanalysis.html#matrix-access",
    "title": "Scatter plot analysis",
    "section": "3.1 Matrix access",
    "text": "3.1 Matrix access\nSpecifies whether columns (default) or rows are plotted against each other."
  },
  {
    "objectID": "rowcorrelation.html",
    "href": "rowcorrelation.html",
    "title": "Row correlation",
    "section": "",
    "text": "1 General\n\nType: - Matrix MultiProcessing\n\n\n\n2 Brief description\nCalculate correlation coefficients between the row in two matrices with the same columns. Useful for e.g. calculating kME values in co-expression analysis workflow."
  },
  {
    "objectID": "assertmatrixequal.html",
    "href": "assertmatrixequal.html",
    "title": "Assert matrix equals",
    "section": "",
    "text": "1 General\n\nType: - Matrix MultiProcessing\nHeading: - CI\nSource code: not public.\n\n\n\n2 Brief description\nThrow exception if the two provided matrices are not equal. Used for regression testing."
  },
  {
    "objectID": "threewayanova.html",
    "href": "threewayanova.html",
    "title": "Three ways ANOVA",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Tests\nSource code: not public."
  },
  {
    "objectID": "threewayanova.html#first-grouping",
    "href": "threewayanova.html#first-grouping",
    "title": "Three ways ANOVA",
    "section": "3.1 First Grouping",
    "text": "3.1 First Grouping\nSelected categorical row that defines the first grouping of columns that should be used in the test (default: first categorical row in the matrix)."
  },
  {
    "objectID": "threewayanova.html#second-grouping",
    "href": "threewayanova.html#second-grouping",
    "title": "Three ways ANOVA",
    "section": "3.2 Second Grouping",
    "text": "3.2 Second Grouping\nSelected categorical row that defines the second grouping of columns that should be used in the test (default: first categorical row in the matrix)."
  },
  {
    "objectID": "threewayanova.html#third-grouping",
    "href": "threewayanova.html#third-grouping",
    "title": "Three ways ANOVA",
    "section": "3.3 Third Grouping",
    "text": "3.3 Third Grouping\nSelected categorical row that defines the third grouping of columns that should be used in the test (default: first categorical row in the matrix)."
  },
  {
    "objectID": "threewayanova.html#log10",
    "href": "threewayanova.html#log10",
    "title": "Three ways ANOVA",
    "section": "3.4 -Log10",
    "text": "3.4 -Log10\nIf checked, -Log10(test value) is reported in the output matrix (default). Otherwise the test-value is reported."
  },
  {
    "objectID": "threewayanova.html#suffix",
    "href": "threewayanova.html#suffix",
    "title": "Three ways ANOVA",
    "section": "3.5 Suffix",
    "text": "3.5 Suffix\nThe entered suffix will be attached to newly generated columns (default: empty). That way columns from multiple runs of the test can be distinguished more easily."
  },
  {
    "objectID": "output_tables.html",
    "href": "output_tables.html",
    "title": "Output Tables",
    "section": "",
    "text": "MaxQuant produces multiple Output tables:"
  },
  {
    "objectID": "output_tables.html#summary-table",
    "href": "output_tables.html#summary-table",
    "title": "Output Tables",
    "section": "1 Summary Table",
    "text": "1 Summary Table\nThe summary file contains summary information for all the raw files processed with a single MaxQuant run. The summary information consists of some MaxQuant parameters, information of the raw file contents, and statistics on the peak detection. Based on this file a quick overview can be gathered on the quality of the data in the raw file.\nThe last row in this file contains the summary information for each column on each of the processed files.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nRaw file\nThe raw file processed.\n\n\nExperiment\nExperiment name assigned to this LC-MS run in the experimental design.\n\n\nFraction\nFraction assigned to this LC-MS run in the experimental design.\n\n\nEnzyme\nThe protease used to digest the protein sample.\n\n\nEnzyme mode\nThe protease used to digest the protein sample.\n\n\nEnzyme first search\nThe protease used for the first search.\n\n\nEnzyme mode first search\nThe protease used for the first search.\n\n\nUse enzyme first search\nMarked with ‘+’ when a different protease setup was used for the first search.\n\n\nVariable modifications\nThe variable modification(s) used during the identification of peptides.\n\n\nMulti modifications\nThe multi modification(s) used during the identification of peptides.\n\n\nVariable modifications first search\nThe variable modification(s) used during the first search.\n\n\nUse variable modifications first search\nMarked with ‘+’ when different variable modifications were used for the first search.\n\n\nRequantify\nThe number of labels used.\n\n\nMultiplicity\nThe number of labels used.\n\n\nMax. missed cleavages\nThe maximum allowed number of missed cleavages.\n\n\nMax. labeled AAs\nThe maximum allowed of labeled amino acids in a peptide amino acid sequence.\n\n\n“Labels” + i\nThe labels used in the labeling experiment. Allowed values for X: 0=light; 1=medium; 2=heavy label partner.\n\n\nLC-MS run type\nThe type of LC-MS run. Usually it will be ‘Standard’ which refers to a conventional shotgun proteomics run with data-dependent MS/MS.\n\n\nTime-dependent recalibration\nWhen marked with ‘+’, time-dependent recalibration was applied to improve the data quality.\n\n\nMS\nThe number of MS spectra recorded in this raw file.\n\n\nMS/MS\nThe number of MS/MS spectra recorded in this raw file.\n\n\nMS3\nThe number of MS3 spectra recorded in this raw file.\n\n\nMS/MS Submitted\nThe number of tandem MS spectra submitted for analysis.\n\n\nMS/MS Submitted (SIL)\nThe number of tandem MS spectra submitted for analysis, where the precursor ion was detected as part of a labeling cluster.\n\n\nMS/MS Submitted (ISO)\nThe number of tandem MS spectra submitted for analysis, where the precursor ion was detected as an isotopic pattern.\n\n\nMS/MS Submitted (PEAK)\nThe number of tandem MS spectra submitted for analysis, where the precursor ion was detected as a single peak.\n\n\nMS/MS Identified\nThe total number of identified tandem MS spectra.\n\n\nMS/MS Identified (SIL)\nThe total number of identified tandem MS spectra, where the precursor ion was detected as part of a labeling cluster.\n\n\nMS/MS Identified (ISO)\nThe total number of identified tandem MS spectra, where the precursor ion was detected as an isotopic pattern.\n\n\nMS/MS Identified (PEAK)\nThe total number of identified tandem MS spectra, where the precursor ion was detected as a single peak.\n\n\nMS/MS Identified [%]\nThe percentage of identified tandem MS spectra.\n\n\nMS/MS Identified (SIL) [%]\nThe percentage of identified tandem MS spectra, where the precursor ion was detected as part of a labeling cluster.\n\n\nMS/MS Identified (ISO) [%]\nThe percentage of identified tandem MS spectra, where the precursor ion was detected as an isotopic pattern.\n\n\nMS/MS Identified (PEAK) [%]\nThe percentage of identified tandem MS spectra, where the precursor ion was detected as a single peak.\n\n\nPeptide Sequences Identified\nThe total number of unique peptide amino acid sequences identified from the recorded tandem mass spectra.\n\n\nPeaks\nThe total number of peaks detected in the full scans.\n\n\nPeaks Sequenced\nThe total number of peaks sequenced by tandem MS.\n\n\nPeaks Sequenced [%]\nThe percentage of peaks sequenced by tandem MS.\n\n\nPeaks Repeatedly Sequenced\nThe total number of peaks repeatedly sequenced (i.e. 1 or more times) by tandem MS.\n\n\nPeaks Repeatedly Sequenced [%]\nThe percentage of peaks repeatedly sequenced (i.e. 1 or more times) by tandem MS.\n\n\nIsotope Patterns\nThe total number of detected isotope patterns.\n\n\nIsotope Patterns Sequenced\nThe total number of isotope patterns sequenced by tandem MS.\n\n\nIsotope Patterns Sequenced (z&gt;1)\nThe total number of isotope patterns sequenced by tandem MS with a charge state of 2 or more.\n\n\nIsotope Patterns Sequenced [%]\nThe percentage of isotope patterns sequenced by tandem MS.\n\n\nIsotope Patterns Sequenced (z&gt;1) [%]\nThe percentage of isotope patterns sequenced by tandem MS with a charge state of 2 or more.\n\n\nIsotope Patterns Repeatedly Sequenced\nThe total number of isotope patterns repeatedly sequenced (i.e. 1 or more times) by tandem MS.\n\n\nIsotope Patterns Repeatedly Sequenced [%]\nThe percentage of isotope patterns repeatedly sequenced (i.e. 1 or more times) by tandem MS.\n\n\nMultiplets\nThe total number of detected labeling pairs.\n\n\n“Multiplets z=” + z\n“The total number of detected labeling pairs with a charge state of” + z + “.”\n\n\nMultiplets Sequenced\nThe total number of labeling pairs sequenced by tandem MS.\n\n\nMultiplets Sequenced [%]\nThe percentage of labeling pairs sequenced by tandem MS.\n\n\nMultiplets Repeatedly Sequenced\nThe total number of labeling pairs repeatedly sequenced (i.e. 1 or more times) by tandem MS.\n\n\nMultiplets Repeatedly Sequenced [%]\nThe percentage of labeling pairs repeatedly sequenced (i.e. 1 or more times) by tandem MS.\n\n\nMultiplets Identified\nThe total number of labeling pairs identified.\n\n\nMultiplets Identified [%]\nThe percentage of labeling pairs identified.\n\n\nMultiplets\nThe total number of detected labeling triplets.\n\n\n“Multiplets z=” + z\n“The total number of detected labeling triplets with a charge state of” + z + “.”\n\n\nMultiplets Sequenced\nThe total number of labeling triplets sequenced by tandem MS.\n\n\nMultiplets Sequenced [%]\nThe percentage of labeling triplets sequenced by tandem MS.\n\n\nMultiplets Repeatedly Sequenced\nThe total number of labeling triplets repeatedly sequenced (i.e. 1 or more times) by tandem MS.\n\n\nMultiplets Repeatedly Sequenced [%]\nThe percentage of labeling triplets repeatedly sequenced (i.e. 1 or more times) by tandem MS.\n\n\nMultiplets Identified\nThe total number of labeling triplets identified.\n\n\nMultiplets Identified [%]\nThe percentage of labeling triplets identified.\n\n\nRecalibrated\nWhen marked with ‘+’, the masses taken from the raw file were recalibrated.\n\n\nAv. Absolute Mass Deviation [ppm]\nThe average absolute mass deviation found comparing to the identification mass in parts per million.\n\n\nMass Standard Deviation [ppm]\nThe standard deviation of the mass deviation found comparing to the identification mass in parts per million.\n\n\nAv. Absolute Mass Deviation [mDa]\nThe average absolute mass deviation found comparing to the identification mass in milli-Dalton.\n\n\nMass Standard Deviation [mDa]\nThe standard deviation of the mass deviation found comparing to the identification mass in milli-Dalton.\n\n\nLabel free norm param\nThe normalization factor used to scale the intensity values in a label-free experiment.\n\n\nMBRFDR\nFalse discovery rate for matching between runs."
  },
  {
    "objectID": "output_tables.html#evidence-table",
    "href": "output_tables.html#evidence-table",
    "title": "Output Tables",
    "section": "2 Evidence Table",
    "text": "2 Evidence Table\nThe evidence file combines all the information about the identified peptides and normally is the only file required for processing the results. Additional information about the peptides, modifications, proteins, etc. can be found in the other files by unique identifier linkage.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nSequence\nThe identified AA sequence of the peptide.\n\n\nLength\nThe length of the sequence stored in the column ‘Sequence’.\n\n\naa + ” Count”\n“The number of instances of” + aa + ” contained within the sequence. The value for this can reliably be determined in the case of labeling partners based on the distance between the partners. These counts are used to solidify the peptide identification process.”\n\n\nModifications\nPost-translational modifications contained within the identified peptide sequence.\n\n\nModified sequence\nSequence representation including the post-translational modifications (abbreviation of the modification in brackets before the modified AA). The sequence is always surrounded by underscore characters (’_’).\n\n\ntitle + ” Probabilities”\n“Sequence representation of the peptide including PTM positioning probabilities ([0..1], where 1 is best match) for ’” + title + “’.”\n\n\ntitle + ” Score Diffs”\nSequence representation for each of the possible PTM positions in each possible configuration, the difference is calculated between the identification score with the PTM added to that position and the best scoring identification where no PTM is added to that position. When this value is negative, it is unlikely that the particular modification is located at this position.\n\n\ntitle\n“The number of occurrences of the modification ’” + title + “’.”\n\n\n“Missed cleavages (” + enzyme + “)”\nNumber of missed enzymatic cleavages.\n\n\nMissed cleavages\nNumber of missed enzymatic cleavages.\n\n\nProteins\nThe identifiers of the proteins this particular peptide is associated with.\n\n\nLeading proteins\nThe identifiers of the proteins in the proteinGroups file, with this protein as best match, this particular peptide is associated with. When multiple matches are found here, the best scoring protein can be found in the ‘Leading Razor Protein’ column.\n\n\nLeading razor protein\nThe identifier of the best scoring protein, from the proteinGroups file this, this peptide is associated to.\n\n\nGene names\nNames of genes this peptide is associated with.\n\n\nProtein names\nNames of proteins this peptide is associated with.\n\n\nType\nThe type of the feature. ‘MSMS’ - for an MS/MS spectrum without an MS1 isotope pattern assigned. ‘ISO-MSMS’ - MS1 isotope cluster identified by MS/MS. ‘MULTI-MSMS’ - MS1 labeling cluster identified by MS/MS. ‘MULTI-SECPEP’ - MS1 labeling cluster identified by MS/MS as second peptide. ‘MULTI-MATCH’ - MS1 labeling cluster identified by matching between runs. In case of label-free data there is no difference between ‘MULTI’ and ‘ISO’.\n\n\nLabeling State\nLabeling state of the precursor isotope pattern used to identify the peptide.\n\n\nRaw file\nThe name of the RAW-file the mass spectral data was derived from.\n\n\nFraction\nThe fraction in which this peptide was detected.\n\n\ncolName\n\n\n\nMS/MS m/z\nThe m/z used for fragmentation (not necessarily the mono-isotopic m/z).\n\n\nCharge\nThe charge-state of the precursor ion.\n\n\nm/z\nThe recalibrated mass-over-charge value of the precursor ion.\n\n\nMass\nThe predicted monoisotopic mass of the identified peptide sequence.\n\n\nResolution\nThe resolution of precursor ion measured in Full Width at Half Maximum (FWHM).\n\n\nUncalibrated - Calibrated m/z [ppm]\nThe difference between the uncalibrated and recalibrated mass-over-charge value of the precursor ion measured in parts-per-million. This gives an indication of the mass drift in the original data, which was automatically corrected by MaxQuant.\n\n\nUncalibrated - Calibrated m/z [Da]\nThe difference between the uncalibrated and recalibrated mass-over-charge value of the precursor ion measured in parts-per-million. This gives an indication of the mass drift in the original data, which was automatically corrected by MaxQuant.\n\n\nMass Error [ppm]\nMass error of the recalibrated mass-over-charge value of the precursor ion in comparison to the predicted monoisotopic mass of the identified peptide sequence in parts per million.\n\n\nMass Error [Da]\nMass error of the recalibrated mass-over-charge value of the precursor ion in comparison to the predicted monoisotopic mass of the identified peptide sequence in milli-Dalton.\n\n\nUncalibrated Mass Error [ppm]\nMass error of the uncalibrated mass-over-charge value of the precursor ion in comparison to the predicted monoisotopic mass of the identified peptide sequence. Note: This column can contain missing values (denoted as NaN).\n\n\nUncalibrated Mass Error [Da]\nMass error of the uncalibrated mass-over-charge value of the precursor ion in comparison to the predicted monoisotopic mass of the identified peptide sequence. Note: This column can contain missing values (denoted as NaN).\n\n\nMax intensity m/z 0\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 0\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 1\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 0\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 1\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 2\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nRetention time\nThe uncalibrated retention time in minutes in the elution profile of the precursor ion.\n\n\nRetention length\nThe total retention time of the peak (last timepoint - first timepoint).\n\n\nCalibrated retention time\nThe recalibrated retention time in minutes in the elution profile of the precursor ion.\n\n\nCalibrated retention time start\nThe recalibrated retention start in minutes in the elution profile of the precursor ion.\n\n\nCalibrated retention time finish\nThe recalibrated retention finish in minutes in the elution profile of the precursor ion.\n\n\nRetention time calibration\nThe difference in minutes between the uncalibrated and recalibrated retention time. This gives an indication of the retention time drift in the original data, which was automatically corrected by MaxQuant. Note: This column can contain missing values (NaN).\n\n\nMatch time difference\nWhen the option ‘match between runs’ is used in MaxQuant, this value indicates the time difference between the feature from the raw file it was taken from and the feature from the raw file it was matched to.\n\n\nMatch m/z difference\nWhen the option ‘match between runs’ is used in MaxQuant, this value indicates the m/z difference between the feature from the raw file it was taken from and the feature from the raw file it was matched to.\n\n\nMatch q-value\nThis is the q-value for features that have been identified by ‘matching between runs’.\n\n\nMatch score\nThe andromeda score of the MS/MS identification that is the source of this identification by ‘matching between runs’.\n\n\nNumber of data points\nThe number of data points (peak centroids) collected for this peptide feature.\n\n\nNumber of scans\nThe number of MS scans that the 3d peaks of this peptide feature are overlapping with.\n\n\nNumber of isotopic peaks\nThe number of isotopic peaks contained in this peptide feature.\n\n\nPIF\nShort for Parent Ion Fraction; indicates the fraction the target peak makes up of the total intensity in the inclusion window.\n\n\nFraction of total spectrum\nThe percentage the ion intensity makes up of the total intensity of the whole spectrum.\n\n\nBase peak fraction\nThe percentage the parent ion intensity in comparison to the highest peak in the MS spectrum.\n\n\nPEP\nPosterior Error Probability of the identification. This value essentially operates as a p-value, where smaller is more significant.\n\n\nMS/MS Count\nThe number of sequencing events for this sequence, which matches the number of identifiers stored in the column ‘MS/MS IDs’. This number is independent of the times the AA sequence has been identified through (other) modifications (e.g. heavy label, oxidation, etc.), about which information can be found in the columns ‘Labeling State’ and ‘Modification’.\n\n\nMS/MS Scan Number\nThe RAW-file derived scan number of the MS/MS with the highest peptide identification score (the highest score is stored in the column ‘Score’).\n\n\nScore\nAndromeda score for the best associated MS/MS spectrum.\n\n\nDelta score\nScore difference to the second best identified peptide.\n\n\nCombinatorics\nNumber of possible distributions of the modifications over the peptide sequence.\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L shift\n\n\n\nRatio M/L\nThe ratio between two medium and light label partners.\n\n\nRatio M/L normalized\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio M/L shift\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L shift\n\n\n\nRatio H/M\nThe ratio between two heavy and medium label partners.\n\n\nRatio H/M normalized\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/M shift\n\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity M\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nReverse\nWhen marked with ‘+’, this particular peptide was found to be part of a protein derived from the reversed part of the decoy database. These should be removed for further data analysis.\n\n\nPotential contaminant\nWhen marked with ‘+’, this particular peptide was found to be part of a commonly occurring contaminant. These should be removed for further data analysis.\n\n\nid\nA unique (consecutive) identifier for each row in the evidence table, which is used to cross-link the information in this file with the information stored in the other files.\n\n\nProtein group IDs\nThe identifier of the protein-group this redundant peptide sequence is associated with, which can be used to look up the extended protein information in the file ‘proteinGroups.txt’. As a single peptide can be linked to multiple proteins (e.g. in the case of razor-proteins), multiple id’s can be stored here separated by a semicolon. As a protein can be identified by multiple peptides, the same id can be found in different rows.\n\n\nPeptide ID\nThe identifier of the non-redundant peptide sequence.\n\n\nMod. peptide ID\nIdentifier of the associated modification summary stored in the file ‘modificationSpecificPeptides.txt’.\n\n\nMS/MS IDs\nIdentifier(s) of the associated MS/MS summary(s) stored in the file ‘msms.txt’.\n\n\nBest MS/MS\nIdentifier(s) of the best MS/MS associated spectrum stored in the file ‘msms.txt’.\n\n\nAIF MS/MS IDs\nIdentifier(s) of the associated All Ion Fragmentation MS/MS summary(s) stored in the file ‘aifMsms.txt’.\n\n\nt + ” site IDs”\n“Identifier(s) of the modification summary stored in the file ’” + t + “Sites.txt’.”\n\n\n“Reporter intensity” + i\n\n\n\n“Reporter intensity not corrected” + i\n\n\n\n“Reporter intensity count” + i\n\n\n\nReporter PIF\n\n\n\nReporter fraction"
  },
  {
    "objectID": "output_tables.html#peptide-table",
    "href": "output_tables.html#peptide-table",
    "title": "Output Tables",
    "section": "3 Peptide Table",
    "text": "3 Peptide Table\nThe peptides table contains information on the identified peptides in the processed raw-files.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nSequence\nThe amino acid sequence of the identified peptide.\n\n\nN-term cleavage window\n“Sequence window from -” + cleaveWindowHalf + ” to ” + cleaveWindowHalf + ” around the N-terminal cleavage site of this peptide.”\n\n\nC-term cleavage window\n“Sequence window from -” + cleaveWindowHalf + ” to ” + cleaveWindowHalf + ” around the C-terminal cleavage site of this peptide.”\n\n\nAmino acid before\nThe amino acid in the protein sequence before the peptide.\n\n\nFirst amino acid\nThe amino acid in the first position of the peptide sequence.\n\n\nSecond amino acid\nThe amino acid in the first position of the peptide sequence.\n\n\nSecond last amino acid\nThe amino acid in the last position of the peptide sequence.\n\n\nLast amino acid\nThe amino acid in the last position of the peptide sequence.\n\n\nAmino acid after\nThe amino acid in the protein sequence after the peptide.\n\n\naa + ” Count”\n“The number of instances of the ’” + aa + “’ amino acid contained within the sequence.”\n\n\nLength\nThe length of the sequence stored in the column “Sequence”.\n\n\nMutated\n\n\n\nMutation names\n\n\n\n“Missed cleavages (” + enzyme + “)”\nNumber of missed enzymatic cleavages.\n\n\nMissed cleavages\nNumber of missed enzymatic cleavages.\n\n\nMass\nMonoisotopic mass of the peptide.\n\n\nProteins\nIdentifiers of proteins this peptide is associated with.\n\n\nLeading razor protein\nIdentifiers of the best scoring protein this peptide is associated with.\n\n\nStart position\nPosition of the first amino acid of this peptide in the protein sequence. (one-based)\n\n\nEnd position\nPosition of the last amino acid of this peptide in the protein sequence. (one-based)\n\n\nGene names\nNames of genes this peptide is associated with.\n\n\nProtein names\nNames of proteins this peptide is associated with.\n\n\nUnique (Groups)\nWhen marked with ‘+’, this particular peptide is unique to a single protein group in the proteinGroups file.\n\n\nUnique (Proteins)\nWhen marked with ‘+’, this particular peptide is unique to a single protein sequence in the fasta file(s).\n\n\nCharges\nAll charge states that have been observed.\n\n\nPEP\nPosterior Error Probability of the identification. This value essentially operates as a p-value, where smaller is more significant.\n\n\nScore\nHighest Andromeda score for the associated MS/MS spectra.\n\n\n“Identification type” + exp\nIndicates whether this experiment was identified by MS/MS or only by matching between runs.\n\n\nFraction Average\n\n\n\nFraction Std. Dev.\n\n\n\n“Fraction” + t\n\n\n\ncolName + ” ” + t\nNumber of evidence entries for this ‘Experiment’.\n\n\nReverse\nWhen marked with ‘+’, this particular peptide was found to be part of a protein derived from the reversed part of the decoy database. These should be removed for further data analysis.\n\n\nPotential contaminant\nWhen marked with ‘+’, this particular peptide was found to be part of a commonly occurring contaminant. These should be removed for further data analysis.\n\n\nid\nA unique (consecutive) identifier for each row in the peptides table, which is used to cross-link the information in this table with the information stored in the other tables.\n\n\nProtein group IDs\nThe identifiers of the protein groups this peptide was linked to, referenced against the proteinGroups table.\n\n\nMod. peptide IDs\nIdentifier(s) for peptide sequence(s), associated with the peptide, referenced against the corresponding modified peptides table.\n\n\nEvidence IDs\nIdentifier(s) for analyzed peptide evidence associated with the protein group referenced against the evidences table.\n\n\nMS/MS IDs\nThe identifiers of the MS/MS scans identifying this peptide, referenced against the msms table.\n\n\nBest MS/MS\nThe identifier of the best (in terms of quality) MS/MS scan identifying this peptide, referenced against the msms table.\n\n\nt + ” site IDs”\nIdentifier(s) for site(s) associated with the protein group, which show(s) evidence of the modification, referenced against the appropriate modification site file.\n\n\nMS/MS Count\n\n\n\n“Reporter intensity” + i\n\n\n\n“Reporter intensity not corrected” + i\n\n\n\n“Reporter intensity count” + i\n\n\n\n“Reporter intensity” + i + ” ” + exp\n\n\n\n“Reporter intensity not corrected” + i + ” ” + exp\n\n\n\n“Reporter intensity count” + i + ” ” + exp\n\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity L” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\n“Intensity M” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\n“Intensity H” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity M\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity L” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\n“Intensity H” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Ratio M/L” + exp\nThe ratio between two medium and light label partners.\n\n\n“Ratio M/L normalized” + exp\nThe ratio between two medium and light label partners.\n\n\n“Ratio M/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio M/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio M/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio M/L type” + exp\n\n\n\n“Ratio H/L” + exp\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L normalized” + exp\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/L type” + exp\n\n\n\n“Ratio H/M” + exp\nThe ratio between two heavy and medium label partners.\n\n\n“Ratio H/M normalized” + exp\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/M variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/M count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/M iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/M type” + exp\n\n\n\n“Ratio H/L” + exp\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L normalized” + exp\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/L type” + exp\n\n\n\nRatio M/L\nThe ratio between two medium and light label partners.\n\n\nRatio M/L normalized\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio M/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio M/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio M/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio M/L type\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/L type\n\n\n\nRatio H/M\nThe ratio between two heavy and medium label partners.\n\n\nRatio H/M normalized\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/M variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/M count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/M iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/M type\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/L type"
  },
  {
    "objectID": "output_tables.html#modification-specific-peptide-table",
    "href": "output_tables.html#modification-specific-peptide-table",
    "title": "Output Tables",
    "section": "4 Modification-Specific Peptide Table",
    "text": "4 Modification-Specific Peptide Table\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nSequence\nThe identified AA sequence of the peptide.\n\n\naa + ” Count”\n“The number of instances of the ’” + aa + “’ AA contained within the sequence. The value for this can reliably be determined in the case of SILAC partners based on the distance between the partners. These counts are used to solidify the peptide identification process.”\n\n\nModifications\nPost-translational modifications contained within the sequence. When no modifications exist, this is set to ‘unmodified’.\n\n\nMass\nCharge corrected mass of the precursor ion.\n\n\nMass Fractional Part\nThe values after the decimal point (ie value - floor(value)).\n\n\nProtein Groups\nIDs of the protein groups to whoch this peptide belongs.\n\n\nProteins\nThe identifiers of the proteins this particular peptide is associated with.\n\n\nGene Names\nNames of genes this peptide is associated with.\n\n\nProtein Names\nNames of proteins this peptide is associated with.\n\n\nUnique (Groups)\nWhen marked with ‘+’, this particular peptide is unique to a single protein group in the proteinGroups file.\n\n\nUnique (Proteins)\nWhen marked with ‘+’, this particular peptide is unique to a single protein sequence in the fasta file(s).\n\n\nmodName\n“Number of” + modName + ” on this peptide.”\n\n\n“Missed cleavages (” + enzyme + “)”\nNumber of missed enzymatic cleavages.\n\n\nMissed cleavages\nNumber of missed enzymatic cleavages.\n\n\n“Identification type” + exp\nIndicates whether this experiment was identified by MS/MS or only by matching between runs.\n\n\nFraction Average\n\n\n\nFraction Std. Dev.\n\n\n\n“Fraction” + t\n\n\n\ncolName + ” ” + t\nNumber of evidence entries for this ‘Experiment’.\n\n\nRetention time\nRetention time in minutes averaged over the evidence entries belonging to this modification-specific peptide.\n\n\nCalibrated retention time\nCalibrated retention time averaged over the evidence entries belonging to this modification-specific peptide. Obviously this only makes sense if retention time recalibration has been performed which is the case when matching between run is selected.\n\n\nCharges\nAll charge states that have been observed.\n\n\nPEP\nPosterior Error Probability of the identification. This value essentially operates as a p-value, where smaller is more significant.\n\n\nMS/MS scan number\nThe RAW-file derived scan number of the MS/MS with the highest peptide identification score (the highest score is stored in the column ‘Score’).\n\n\nRaw file\nThe name of the RAW-file the mass spectral data was derived from.\n\n\nScore\nAndromeda score for the best identified among the associated MS/MS spectra.\n\n\nDelta score\nScore difference to the second best identified peptide.\n\n\nReverse\nWhen marked with ‘+’, this particular peptide was found to be part of a protein derived from the reversed part of the decoy database. These should be removed for further data analysis.\n\n\nPotential contaminant\nWhen marked with ‘+’, this particular peptide was found to be part of a commonly occurring contaminant. These should be removed for further data analysis.\n\n\nid\nA unique (consecutive) identifier for each row in the peptides table, which is used to cross-link the information in this table with the information stored in the other tables.\n\n\nProtein group IDs\nThe identifiers of the protein groups this peptide was linked to, referenced against the proteinGroups table.\n\n\nPeptide ID\nIdentifier of the associated peptide sequence summary, which can be found in the file ‘peptides.txt’.\n\n\nEvidence IDs\nIdentifier(s) for analyzed peptide evidence associated with the protein group referenced against the evidences table.\n\n\nMS/MS IDs\nThe identifiers of the MS/MS scans identifying this peptide, referenced against the msms table.\n\n\nBest MS/MS\nThe identifier of the best (in terms of quality) MS/MS scan identifying this peptide, referenced against the msms table.\n\n\nt + ” site IDs”\nIdentifier(s) for site(s) associated with this peptide, which show(s) evidence of the modification, referenced against the appropriate modification site file.\n\n\nMS/MS Count\n\n\n\n“Reporter intensity” + i\n\n\n\n“Reporter intensity not corrected” + i\n\n\n\n“Reporter intensity count” + i\n\n\n\n“Reporter intensity” + i + ” ” + exp\n\n\n\n“Reporter intensity not corrected” + i + ” ” + exp\n\n\n\n“Reporter intensity count” + i + ” ” + exp\n\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity L” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\n“Intensity M” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\n“Intensity H” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity L” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\n“Intensity H” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity M\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Ratio M/L” + exp\nThe ratio between two medium and light label partners.\n\n\n“Ratio M/L normalized” + exp\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio M/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio M/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio M/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio M/L type” + exp\n\n\n\n“Ratio H/L” + exp\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L normalized” + exp\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/L type” + exp\n\n\n\n“Ratio H/M” + exp\nThe ratio between two heavy and medium label partners.\n\n\n“Ratio H/M normalized” + exp\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/M variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/M count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/M iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/M type” + exp\n\n\n\n“Ratio H/L” + exp\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L normalized” + exp\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/L type” + exp\n\n\n\nRatio M/L\nThe ratio between two medium and light label partners.\n\n\nRatio M/L normalized\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio M/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio M/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio M/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio M/L type\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/L type\n\n\n\nRatio H/M\nThe ratio between two heavy and medium label partners.\n\n\nRatio H/M normalized\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/M variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/M count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/M iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/M type\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/L type"
  },
  {
    "objectID": "output_tables.html#site-table",
    "href": "output_tables.html#site-table",
    "title": "Output Tables",
    "section": "5 Site Table",
    "text": "5 Site Table\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nProteins\nIdentifiers of proteins this site is associated with.\n\n\nPositions within proteins\nFor each protein identifier in the ‘Proteins’ column you find here the psoition of the site in the respective protein sequence. The index of the first amino acid in the sequence is 1.\n\n\nLeading proteins\n\n\n\nProtein\nIdentifier of the protein this peptide is associated with.\n\n\nProtein names\nNames of proteins this peptide is associated with.\n\n\nGene names\nNames of genes this peptide is associated with.\n\n\nFasta headers\nDescriptions of proteins this peptide is associated with.\n\n\nLocalization prob\n\n\n\nScore diff\n\n\n\nPEP\nThe posterior error probability (PEP) of the best identified modified peptide containing this site.\n\n\nScore\nThe Andromeda score of the best identified modified peptide containing this site.\n\n\nDelta score\nThe Andromeda delta score of the best identified modified peptide containing this site.\n\n\nScore for localization\nThe Andromeda score of the MS/MS spectrum used for calculating the localization score for this site.\n\n\n“Localization prob” + exp\n\n\n\n“Score diff” + exp\n\n\n\n“PEP” + exp\n\n\n\n“Score” + exp\n\n\n\nDiagnostic peak\n\n\n\n“Number of” + mod\n“Different numbers of” + mod + ” on peptides that this site is involved in.”\n\n\nAmino acid\n\n\n\nSequence window\n\n\n\nModification window\n\n\n\nPeptide window coverage\n\n\n\nmod + ” Probabilities”\n\n\n\nmod + ” Score diffs”\n\n\n\nPosition in peptide\n\n\n\nCharge\nCharge state of the precursor ion.\n\n\nMass error [ppm]\nMass error of the recalibrated mass-over-charge value of the precursor ion in comparison to the predicted monoisotopic mass of the identified peptide sequence.\n\n\n“Identification type” + exp\nIndicates whether this experiment was identified by MS/MS or only by matching between runs.\n\n\nReverse\nWhen marked with ‘+’, this particular peptide was found to be part of a protein derived from the reversed part of the protein sequence database. These should be removed for further data analysis.\n\n\nPotential contaminant\nWhen marked with ‘+’, this particular peptide was found to be part of a commonly occurring contaminant. These should be removed for further data analysis.\n\n\nid\nA unique (consecutive) identifier for each row in the site table, which is used to cross-link the information in this file with the information stored in the other files.\n\n\nProtein group IDs\nThe identifier of the protein-group this peptide sequence is associated with, which can be used to look up the extended protein information in the file ‘proteinGroups.txt’. As a single peptide can be linked to multiple proteins (e.g. in the case of razor-proteins), multiple id’s can be stored here separated by a semicolon. As a protein can be identified by multiple peptides, the same id can be found in different rows.\n\n\nPositions\nThe positions of the modifications in the protein amino acid sequence.\n\n\nPosition\nThe position of the modification in the protein amino acid sequence.\n\n\nPeptide IDs\nIdentifier(s) of the associated peptide sequence(s) summary, which can be found in the file ‘peptides.txt’.\n\n\nMod. peptide IDs\nIdentifier(s) of the associated peptide sequence(s) summary, which can be found in the file ‘modificationSpecificPeptides.txt’.\n\n\nEvidence IDs\nIdentifier(s) for analyzed peptide evidence associated with the protein group referenced against the evidences table.\n\n\nMS/MS IDs\nThe identifiers of the MS/MS scans identifying this peptide, referenced against the msms table.\n\n\nBest localization evidence ID\n\n\n\nBest localization MS/MS ID\n\n\n\nBest localization raw file\n\n\n\nBest localization scan number\n\n\n\nBest score evidence ID\n\n\n\nBest score MS/MS ID\n\n\n\nBest score raw file\n\n\n\nBest score scan number\n\n\n\nBest PEP evidence ID\n\n\n\nBest PEP MS/MS ID\n\n\n\nBest PEP raw file\n\n\n\nBest PEP scan number\n\n\n\n“Reporter intensity” + i\n\n\n\n“Reporter intensity not corrected” + i\n\n\n\n“Reporter intensity count” + i\n\n\n\n“Reporter intensity” + i + ” ” + exp\n\n\n\n“Reporter intensity not corrected” + i + ” ” + exp\n\n\n\n“Reporter intensity count” + i + ” ” + exp\n\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity L” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\n“Intensity M” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\n“Intensity H” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\n“Ratio mod/base L” + exp\n\n\n\n“Ratio mod/base M” + exp\n\n\n\n“Ratio mod/base H” + exp\n\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity L” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\n“Intensity H” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\n“Ratio mod/base L” + exp\n\n\n\n“Ratio mod/base H” + exp\n\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Ratio mod/base” + exp\n\n\n\n“Intensity” + exp + “___” + j\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Occupancy” + exp\n\n\n\n“Occupancy ratio” + exp\n\n\n\n“Occupancy error scale” + exp\n\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity M\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nRatio mod/base L\n\n\n\nRatio mod/base M\n\n\n\nRatio mod/base H\n\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nRatio mod/base L\n\n\n\nRatio mod/base H\n\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity___” + j\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nRatio mod/base\n\n\n\n“Ratio M/L” + exp\nThe ratio between two medium and light label partners.\n\n\n“Ratio M/L” + exp + “___” + i\nThe ratio between two medium and light label partners.\n\n\n“Ratio M/L normalized” + exp\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio M/L normalized” + exp + “___” + i\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio M/L unmod. pep.” + exp\n\n\n\n“Ratio M/L localized” + exp\n\n\n\n“Ratio M/L nmods” + exp\n\n\n\n“Ratio M/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio M/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio M/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio M/L type” + exp\n\n\n\n“Ratio H/L” + exp\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L” + exp + “___” + i\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L normalized” + exp\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L normalized” + exp + “___” + i\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L unmod. pep.” + exp\n\n\n\n“Ratio H/L localized” + exp\n\n\n\n“Ratio H/L nmods” + exp\n\n\n\n“Ratio H/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/L type” + exp\n\n\n\n“Ratio H/M” + exp\nThe ratio between two heavy and medium label partners.\n\n\n“Ratio H/M” + exp + “___” + i\nThe ratio between two heavy and medium label partners.\n\n\n“Ratio H/M normalized” + exp\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/M normalized” + exp + “___” + i\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/M unmod. pep.” + exp\n\n\n\n“Ratio H/M localized” + exp\n\n\n\n“Ratio H/M nmods” + exp\n\n\n\n“Ratio H/M variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/M count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/M iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/M type” + exp\n\n\n\n“Occupancy L” + exp\n\n\n\n“Occupancy M” + exp\n\n\n\n“Occupancy H” + exp\n\n\n\n“Ratio H/L” + exp\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L” + exp + “___” + i\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L normalized” + exp\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L normalized” + exp + “___” + i\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L unmod. pep.” + exp\n\n\n\n“Ratio H/L localized” + exp\n\n\n\n“Ratio H/L nmods” + exp\n\n\n\n“Ratio H/L variability [%]” + exp\n\n\n\n“Ratio H/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/L type” + exp\n\n\n\n“Occupancy L” + exp\n\n\n\n“Occupancy H” + exp\n\n\n\nRatio M/L\nThe ratio between two medium and light label partners.\n\n\n“Ratio M/L___” + i\nThe ratio between two medium and light label partners.\n\n\nRatio M/L normalized\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio M/L normalized___” + i\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio M/L unmod. pep.\n\n\n\nRatio M/L localized\n\n\n\nRatio M/L nmods\n\n\n\nRatio M/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio M/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio M/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio M/L type\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L___” + i\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L normalized___” + i\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L unmod. pep.\n\n\n\nRatio H/L localized\n\n\n\nRatio H/L nmods\n\n\n\nRatio H/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/L type\n\n\n\nRatio H/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/M\nThe ratio between two heavy and medium label partners.\n\n\n“Ratio H/M___” + i\nThe ratio between two heavy and medium label partners.\n\n\nRatio H/M normalized\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/M normalized___” + i\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/M unmod. pep.\n\n\n\nRatio H/M localized\n\n\n\nRatio H/M nmods\n\n\n\nRatio H/M variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/M count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/M iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/M type\n\n\n\nOccupancy L\n\n\n\nOccupancy M\n\n\n\nOccupancy H\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L___” + i\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L normalized___” + i\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L unmod. pep.\n\n\n\nRatio H/L localized\n\n\n\nRatio H/L nmods\n\n\n\nRatio H/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/L type\n\n\n\nOccupancy L\n\n\n\nOccupancy H"
  },
  {
    "objectID": "output_tables.html#protein-groups",
    "href": "output_tables.html#protein-groups",
    "title": "Output Tables",
    "section": "6 Protein Groups",
    "text": "6 Protein Groups\nThe Protein Groups table contains information on the identified proteins in the processed raw-files. Each single row contains the group of proteins that could be reconstructed from a set of peptides.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nProtein IDs\nIdentifier(s) of protein(s) contained in the protein group. They are sorted by number of identified peptides in descending order.\n\n\nMajority protein IDs\nThese are the IDs of those proteins that have at least half of the peptides that the leading protein has.\n\n\nPeptide counts (all)\nNumber of peptides associated with each protein in protein group, occuring in the order as the protein IDs occur in the ‘Protein IDs’ column. Here distinct peptide sequences are counted. Modified forms or different charges are counted as one peptide.\n\n\nPeptide counts (razor+unique)\nNumber of peptides associated with each protein in protein group, occuring in the order as the protein IDs occur in the ‘Protein IDs’ column. Here distinct peptide sequences are counted. Modified forms or different charges are counted as one peptide.\n\n\nPeptide counts (unique)\nNumber of peptides associated with each protein in protein group, occuring in the order as the protein IDs occur in the ‘Protein IDs’ column. Here distinct peptide sequences are counted. Modified forms or different charges are counted as one peptide.\n\n\nProtein names\nName(s) of protein(s) contained within the group.\n\n\nGene names\nName(s) of the gene(s) associated to the protein(s) contained within the group.\n\n\nMutated peptide count\n\n\n\nMutation names\n\n\n\n“Mutated peptide count” + exp\n\n\n\n“Mutation names” + exp\n\n\n\nFasta headers\nFasta headers(s) of protein(s) contained within the group.\n\n\nNumber of proteins\nNumber of proteins contained within the group. This corresponds to the number of entries in the colum ‘Protein IDs’.\n\n\nPeptides\nThe total number of peptide sequences associated with the protein group (i.e. for all the proteins in the group).\n\n\nRazor + unique peptides\nThe total number of razor + unique peptides associated with the protein group (i.e. these peptides are shared with another protein group).\n\n\nUnique peptides\nThe total number of unique peptides associated with the protein group (i.e. these peptides are not shared with another protein group).\n\n\n“Peptides” + exp\n“Number of peptides (distinct peptide sequences) in experiment” + exp\n\n\n“Razor + unique peptides” + exp\n“Number of razor + unique peptides (distinct peptide sequences) in experiment” + exp\n\n\n“Unique peptides” + exp\n“Number of unique peptides (distinct peptide sequences) in experiment” + exp\n\n\nSequence coverage [%]\nPercentage of the sequence that is covered by the identified peptides of the best protein sequence contained in the group.\n\n\nUnique + razor sequence coverage [%]\nPercentage of the sequence that is covered by the identified unique and razor peptides of the best protein sequence contained in the group.\n\n\nUnique sequence coverage [%]\nPercentage of the sequence that is covered by the identified unique peptides of the best protein sequence contained in the group.\n\n\nMol. weight [kDa]\nMolecular weight of the leading protein sequence contained in the protein group.\n\n\nSequence length\nThe length of the leading protein sequence contained in the group.\n\n\nSequence lengths\nThe length of all sequences of the proteins contained in the group.\n\n\nFraction average\n\n\n\n“Fraction” + t\n\n\n\nQ-value\nThis is the ratio of reverse to forward protein groups.\n\n\nScore\nProtein score which is derived from peptide posterior error probabilities.\n\n\n“Identification type” + exp\nIndicates whether this experiment was identified by MS/MS or only by matching between runs.\n\n\n“Sequence coverage” + exp + ” [%]”\nPercentage of the sequence that is covered by the identified peptides in this sample of the longest protein sequence contained within the group.\n\n\nOnly identified by site\nWhen marked with ‘+’, this particular protein group was identified only by a modification site.\n\n\nReverse\nWhen marked with ‘+’, this particular protein group contains no protein, made up of at least 50% of the peptides of the leading protein, with a peptide derived from the reversed part of the decoy database. These should be removed for further data analysis. The 50% rule is in place to prevent spurious protein hits to erroneously flag the protein group as reverse.\n\n\nPotential contaminant\nWhen marked with ‘+’, this particular protein group was found to be a commonly occurring contaminant. These should be removed for further data analysis.\n\n\nid\nA unique (consecutive) identifier for each row in the proteinGroups table, which is used to cross-link the information in this file with the information stored in the other files.\n\n\nPeptide IDs\nIdentifier(s) of the associated peptide sequence(s) summary, which can be found in the file ‘peptides.txt’.\n\n\nPeptide is razor\nIndicates for each peptide ID if it is a razor or group unique peptide (true) or a non unique non razor peptide (false).\n\n\nMod. peptide IDs\n\n\n\nEvidence IDs\n\n\n\nMS/MS IDs\n\n\n\nBest MS/MS\nThe identifier of the best (in terms of quality) MS/MS scans identifying the peptides of this protein, referenced against the msms table.\n\n\nt + ” site IDs”\nIdentifier(s) for site(s) associated with the protein group, which show(s) evidence of the modification, referenced against the appropriate modification site file.\n\n\nt + ” site positions”\nPositions of the sites in the leading protein of this group.\n\n\n“LFQ intensity L” + exp\n\n\n\n“LFQ intensity M” + exp\n\n\n\n“LFQ intensity H” + exp\n\n\n\n“LFQ intensity L” + exp\n\n\n\n“LFQ intensity H” + exp\n\n\n\n“LFQ intensity” + exp\n\n\n\n“iBAQ” + exp\n\n\n\n“iBAQ L” + exp\n\n\n\n“iBAQ M” + exp\n\n\n\n“iBAQ H” + exp\n\n\n\niBAQ\n\n\n\niBAQ L\n\n\n\niBAQ M\n\n\n\niBAQ H\n\n\n\n“iBAQ” + exp\n\n\n\n“iBAQ L” + exp\n\n\n\n“iBAQ H” + exp\n\n\n\niBAQ\n\n\n\niBAQ L\n\n\n\niBAQ H\n\n\n\n“iBAQ” + exp\n\n\n\niBAQ\n\n\n\nAVALON\n\n\n\nMS/MS Count\n\n\n\n“MS/MS Count” + exp\n\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity L” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\n“Intensity M” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\n“Intensity H” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity M\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Intensity L” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\n“Intensity H” + exp\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\n“Intensity” + exp\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\n“Ratio M/L” + exp\nThe ratio between two medium and light label partners.\n\n\n“Ratio M/L normalized” + exp\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio M/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio M/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio M/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio M/L type” + exp\n\n\n\n“Ratio H/L” + exp\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L normalized” + exp\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/L type” + exp\n\n\n\n“Ratio H/M” + exp\nThe ratio between two heavy and medium label partners.\n\n\n“Ratio H/M normalized” + exp\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/M variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/M count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/M iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/M type” + exp\n\n\n\nRatio M/L\nThe ratio between two medium and light label partners.\n\n\nRatio M/L normalized\nNormalized ratio between two heavy and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio M/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio M/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio M/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio M/L type\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/L type\n\n\n\nRatio H/M\nThe ratio between two heavy and medium label partners.\n\n\nRatio H/M normalized\nNormalized ratio between two heavy and medium label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/M variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/M count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/M iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/M type\n\n\n\n“Ratio H/L” + exp\nThe ratio between two heavy and light label partners.\n\n\n“Ratio H/L normalized” + exp\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\n“Ratio H/L variability [%]” + exp\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\n“Ratio H/L count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\n“Ratio H/L iso-count” + exp\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\n“Ratio H/L type” + exp\n\n\n\nRatio H/L\nThe ratio between two heavy and light label partners.\n\n\nRatio H/L normalized\nNormalized ratio between two medium and light label partners. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L variability [%]\nCoefficient of variability over all redundant quantifiable peptides. It is calculated as the standard deviation of the naturally logarithmized ratios times 100.\n\n\nRatio H/L count\nNumber of redundant peptides (MS1 features) used for quantitation.\n\n\nRatio H/L iso-count\nNumber of redundant peptides (MS1 features) used for quantitation that are quantified with the re-quantify method.\n\n\nRatio H/L type\n\n\n\n“Reporter intensity” + i\n\n\n\n“Reporter intensity not corrected” + i\n\n\n\n“Reporter intensity count” + i\n\n\n\n“Reporter intensity” + i + ” ” + exp\n\n\n\n“Reporter intensity not corrected” + i + ” ” + exp\n\n\n\n“Reporter intensity count” + i + ” ” + exp"
  },
  {
    "objectID": "output_tables.html#all-peptides",
    "href": "output_tables.html#all-peptides",
    "title": "Output Tables",
    "section": "7 All Peptides",
    "text": "7 All Peptides\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nRaw file\nName of the raw file the spectral data was extracted from.\n\n\nType\nThe type of detection for the peptide. MULTI - A labeling multiplet was detected. ISO - An isotope pattern was detected.\n\n\nCharge\nThe charge state of the peptide.\n\n\nm/z\nThe mass divided by the charge of the charged peptide.\n\n\nMass\nThe mass of the neutral peptide ((m/z-proton) * charge).\n\n\nUncalibrated m/z\nm/z before recalibrations have been applied.\n\n\nResolution\nThe resolution of the peak detected for the peptide measured in Full Width at Half Maximum (FWHM).\n\n\nNumber of data points\nThe number of data points (peak centroids) collected for this peptide feature.\n\n\nNumber of scans\nThe number of MS scans that the 3d peaks of this peptide feature are overlapping with.\n\n\nNumber of isotopic peaks\nThe number of isotopic peaks contained in this peptide feature.\n\n\nPIF\nShort for Parent Ion Fraction; indicates the fraction the target peak makes up of the total intensity in the inclusion window.\n\n\nMass fractional part\nThe values after the radix point (ie value - floor(value)).\n\n\nMass deficit\nEmpirically derived deviation measure to the next nearest integer scaled to center around 0. Can be used to visually detect contaminants in a plot setting Mass against this value. ma+b - round(ma+b) m: the peptide mass a: 0.999555 b: -0.10\n\n\nMass precision [ppm]\nThe precision of the mass detection of the peptide in parts-per-million.\n\n\nMax intensity m/z 0\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 0\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 1\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 0\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 1\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nMax intensity m/z 2\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nRetention time\nThe retention time of the peak detected for the peptide measured in minutes.\n\n\nRetention length\nThe total retention time width of the peak (last timepoint - first timepoint) in seconds.\n\n\nRetention length (FWHM)\nThe full width at half maximum value retention time width of the peak in seconds.\n\n\nMin scan number\nThe first scan-number at which the peak was encountered.\n\n\nMax scan number\nThe last scan-number at which the peak was encountered.\n\n\nIdentified\nWhen marked with ‘+’ this particular MS/MS scan was identified as a peptide; when marked with ‘-’ no identification was made.\n\n\nMS/MS IDs\nUnique identifier linking this identification to the MS/MS scans.\n\n\nSequence\nThe identified AA sequence of the peptide.\n\n\nLength\n“The length of the sequence stored in the column Sequence”.”\n\n\nModifications\nPost-translational modifications contained within the sequence. When no modifications exist, this is set to ‘unmodified’. Note: This column only set when this MS/MS spectrum has been identified.\n\n\nModified sequence\nSequence representation of the peptide including location(s) of modified AAs. Note: This column only set when this MS/MS spectrum has been identified.\n\n\nProteins\nIdentifiers of proteins this peptide is associated with. Note: This column only set when this MS/MS spectrum has been identified.\n\n\nScore\nThe score of the identification (higher is better). Note: This column only set when this MS/MS spectrum has been identified.\n\n\nt.Abbreviation + ” Count”\n“The number of instances of” + t.Abbreviation + ” contained within the sequence. The value for this can reliably be determined in the case of label partners, based on the distance between the partners. These counts are used to solidify the peptide identification process.”\n\n\nRatio H/L\nThe ratio between two heavy and light multiplet members.\n\n\nRatio H/L normalized\nNormalized ratio between two heavy and light multiplet members. The median of the total ratio population was shifted to 1.\n\n\nRatio M/L\nThe ratio between two medium and light multiplet members.\n\n\nRatio M/L normalized\nNormalized ratio between two medium and light multiplet members. The median of the total ratio population was shifted to 1.\n\n\nRatio H/L\nThe ratio between two heavy and light multiplet members.\n\n\nRatio H/L normalized\nNormalized ratio between two heavy and light multiplet members. The median of the total ratio population was shifted to 1.\n\n\nRatio H/M\nThe ratio between two heavy and medium multiplet members.\n\n\nRatio H/M normalized\nNormalized ratio between two heavy and medium multiplet members. The median of the total ratio population was shifted to 1.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensity\nSummed up eXtracted Ion Current (XIC) of all isotopic clusters associated with the identified AA sequence. In case of a labeled experiment this is the total intensity of all the isotopic patterns in the label cluster.\n\n\nIntensity L\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the light label partner.\n\n\nIntensity M\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the medium label partner.\n\n\nIntensity H\nSummed up eXtracted Ion Current (XIC) of the isotopic cluster linked to the heavy label partner.\n\n\nIntensities\nThe intensity values of the isotopes.\n\n\nIntensities L\nThe intensity values of the light label partner isotopes.\n\n\nIntensities H\nThe intensity values of the heavy label partner isotopes.\n\n\nIntensities L\nThe intensity values of the light label partner isotopes.\n\n\nIntensities M\nThe intensity values of the medium label partner isotopes.\n\n\nIntensities H\nThe intensity values of the heavy label partner isotopes.\n\n\nMS/MS Count\nThe number of MS/MS spectra recorded for the peptide.\n\n\nMSMS Scan Numbers\nThe scan numbers where the MS/MS spectra were recorded.\n\n\nMSMS label States\nThe label partner detected for the peptide. The value 0 is always the light partner. In the case of double label labeling 1 is the heavy partner. In the case of triple label labeling 1 is the medium and 2 the heavy partner.\n\n\nMSMS Isotope Indices\nIndices of the isotopic peaks that the MS/MS spectra reside on. A value of 0 corresponds to the monoisotopic peak.\n\n\nDP Mass Difference\n\n\n\nDP Time Difference\n\n\n\nDP Score\n\n\n\nDP PEP\n\n\n\nDP Positional Probability\n\n\n\nDP Base Sequence\n\n\n\nDP Probabilities\n\n\n\nDP AA\n\n\n\nDP Base Scan Number\n\n\n\nDP Mod Scan Number\n\n\n\nDP Decoy\n\n\n\nDP Proteins\n\n\n\nDP Cluster Index\n\n\n\nDP Cluster Mass\n\n\n\nDP Cluster Mass SD\n\n\n\nDP Cluster Size Total\n\n\n\nDP Cluster Size Forward\n\n\n\nDP Cluster Size Reverse\n\n\n\nDP Modification\n\n\n\nDP Peptide Length Difference\n\n\n\nDP Gene Names\n\n\n\nDP Protein Names\n\n\n\nDP Ratio mod/base\n\n\n\nDP Ratio mod/base\n\n\n\nDP Base Ratio H/L\n\n\n\nDP Base Ratio H/L Normalized\n\n\n\nDP Occupancy L\n\n\n\nDP Occupancy H\n\n\n\nDP Ratio mod/base\n\n\n\nDP Base Ratio M/L\n\n\n\nDP Base Ratio M/L Normalized\n\n\n\nDP Base Ratio H/L\n\n\n\nDP Base Ratio H/L Normalized\n\n\n\nDP Base Ratio H/M\n\n\n\nDP Base Ratio H/M Normalized\n\n\n\nDP Occupancy L\n\n\n\nDP Occupancy M\n\n\n\nDP Occupancy H\n\n\n\nMD modification\n\n\n\nMD mass error\n\n\n\nMD time difference\n\n\n\nMD sequence\n\n\n\nMD base scan number\n\n\n\nMD proteins\n\n\n\nMD gene names\n\n\n\nMD protein names\n\n\n\nCentroid mass differences [Da]\n\n\n\nCentroid mass differences [ppm]\n\n\n\nIsotope mass differences [Da]\n\n\n\nIsotope mass differences [ppm]"
  },
  {
    "objectID": "output_tables.html#msscans-table",
    "href": "output_tables.html#msscans-table",
    "title": "Output Tables",
    "section": "8 msScans Table",
    "text": "8 msScans Table\nThe msScans table contains information about the full scans, which can be used to verify data quality and generated useful statistics about the interaction between the samples and LC.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nRaw file\nThe name of the RAW-file the mass spectral data originates from.\n\n\nScan number\nThe scan number (defined in the raw-file) at which the full scan was made.\n\n\nScan index\nThe consecutive index of this full scan.\n\n\nRetention time\nThe retention time at which the full scan was made.\n\n\nCycle time\nThe total time (full scan including the tandem MS scans) this full scan has taken up.\n\n\nIon injection time\nThe total injection time that was required to capture the specified amount of ions. This value is limited by a maximum, which can be used to determine whether the time has maxed out (indicative of a bad acquisition).\n\n\nBase peak intensity\nThe intensity of the most intense ion in the spectrum.\n\n\nTotal ion current\nThe total intensity acquired in the full scan.\n\n\nMS/MS count\nThe number of tandem MS scans that were made based on this full scan (e.g. a top 10 method selects the top 10 most intense ions in the scan and fragments those).\n\n\nMass calibration\nThe applied mass correction in Th to the full scan.\n\n\nFraction\nThe fraction measured with this full scan.\n\n\ncolName\n\n\n\nPeak length\nThe average time between the start and the end of the peaks detected in the full scan.\n\n\nIsotope pattern length\nThe average time between the start and the end of the isotope patterns detected in the full scan.\n\n\nMultiplet length\nThe average time between the start and the end of the isotope patterns of the labeling multiplets detected in the full scan.\n\n\nPeaks / s\nThe average number of peaks detected per second of chromatography.\n\n\nSingle peaks / s\nThe average number of single peaks detected per second of chromatography.\n\n\nIsotope patterns / s\nThe average number of isotope patterns detected per second of chromatography.\n\n\nSingle isotope patterns / s\nThe average number of single isotope patterns detected per of second chromatography.\n\n\nMultiplets / s\nThe average number of labeling multiplets detected per of second chromatography.\n\n\nIdentified multiplets / s\nThe percentage of labeling multiplets actually identified.\n\n\nMultiplet identification rate [%]\nThe percentage of the detected labeling multiplets that were identified.\n\n\nAIF peaks / s\nThe average number of AIF peaks detected per second of chromatography.\n\n\nAIF single peaks / s\nThe average number of AIF single peaks detected per second of chromatography.\n\n\nAIF isotope patterns / s\nThe average number of AIF isotope patterns detected per second of chromatography.\n\n\nAIF single isotope patterns / s\nThe average number of AIF single isotope patterns detected per of second chromatography.\n\n\nAIF multiplets / s\nThe average number of AIF labeling multiplets detected per of second chromatography.\n\n\nMS/MS / s\nThe average number of MS/MS events per second of chromatography.\n\n\nIdentified MS/MS / s\nThe average number of identified MS/MS events per second of chromatography.\n\n\nMS/MS identification rate [%]\nThe percentage of tandem MS scans that were identified.\n\n\nIntens Comp Factor\nTaken from the Thermo RAW file.\n\n\nCTCD Comp\nTaken from the Thermo RAW file.\n\n\nRawOvFtT\nFor Thermo Fisher only. TIC estimation done with the orbitrap cell.\n\n\nAGC Fill\nTaken from the Thermo RAW file."
  },
  {
    "objectID": "output_tables.html#mzrange-table",
    "href": "output_tables.html#mzrange-table",
    "title": "Output Tables",
    "section": "9 MzRange Table",
    "text": "9 MzRange Table\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nRaw file\nThe name of the RAW-file the mass spectral data was derived from.\n\n\nm/z\nThe mass-over-charge value.\n\n\nPeaks / Da\nThe average number of peaks detected per Dalton.\n\n\nSingle peaks / Da\nThe average number of single peaks detected per Dalton.\n\n\nIsotope patterns / Da\nThe average number of isotope patterns detected per Dalton.\n\n\nSingle isotope patterns / Da\nThe average number of single isotope patterns detected per Dalton.\n\n\nSILAC pairs / Da\nThe average number of SILAC pairs detected per Dalton.\n\n\nIdentified SILAC pairs / Da\nThe percentage of SILAC pairs actually identified.\n\n\nSILAC identification rate [%]\nThe percentage of the detected SILAC pairs that were identified.\n\n\nAIF Peaks / Da\nThe average number of AIF peaks detected per Dalton.\n\n\nAIF Single peaks / Da\nThe average number of AIF single peaks detected per Dalton.\n\n\nAIF Isotope patterns / Da\nThe average number of AIF isotope patterns detected per Dalton.\n\n\nAIF Single isotope patterns / Da\nThe average number of AIF single isotope patterns detected per Dalton.\n\n\nAIF SILAC pairs / Da\nThe average number of AIF SILAC pairs detected per Dalton.\n\n\nMS/MS / Da\nThe average number of MS/MS events per Dalton.\n\n\nIdentified MS/MS / Da\nThe average number of identified MS/MS events per Dalton.\n\n\nIdentification rate [%]\nThe percentage of tandem MS scans that were identified."
  },
  {
    "objectID": "output_tables.html#msms-scans-table",
    "href": "output_tables.html#msms-scans-table",
    "title": "Output Tables",
    "section": "10 ms/ms Scans Table",
    "text": "10 ms/ms Scans Table\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nRaw file\nName of the RAW file the spectral MS/MS data was extracted from.\n\n\nScan number\nRAW file derived scan number for the MS/MS spectrum.\n\n\nRetention time\nTime point along the elution profile at which the MS/MS data was recorded.\n\n\nIon injection time\nThe ion inject time for the MS/MS scan. This can be used to determine if this time equals to the maximum ion inject time, general indicative of a lower quality spectrum.\n\n\nTotal ion current\nThe total ion current of the MS/MS scan. For Thermo data this value is calculated by summing all the intensity values found in the mass spectral data, which is different from the Xcalibur reported TIC (Xcalibur TIC is about 25% of the value reported here).\n\n\nCollision energy\nThe collision energy used for the fragmentation that resulted in this MS/MS scan.\n\n\nSummations\nFor time of flight instruments only.\n\n\nBase peak intensity\nThe intensity of the most intense ion in the spectrum.\n\n\nElapsed time\nThe time the MS/MS scan took to complete.\n\n\nIdentified\nWhen marked with ‘+’ this particular MS/MS scan was identified as a peptide; when marked with ‘-’ no identification was made.\n\n\nMS/MS IDs\nUnique identifier linking this identification to the MS/MS scans.\n\n\nSequence\nThe identified AA sequence of the peptide.\n\n\nLength\n“The length of the sequence stored in the column Sequence”.”\n\n\nFiltered peaks\nNumber of peaks after the ‘top X per 100 Da’ filtering.\n\n\nm/z\nRecalibrated m/z of the precursor ion.\n\n\nMass\nCharge corrected mass of the precursor ion.\n\n\nCharge\nCharge state of the precursor ion.\n\n\nType\nThe type of precursor ion as identified by MaxQuant. ISO - isotopic cluster. PEAK - single peak. MULTI - labeling cluster.\n\n\nFragmentation\nThe type of fragmentation used to create the MS/MS spectrum. CID - Collision Induced Dissociation. HCD - High energy Collision induced Dissociation. ETD - Electron Transfer Dissociation.\n\n\nMass analyzer\nThe mass analyzer used to record the MS/MS spectrum. ITMS - Ion trap. FTMS - Fourier transform ICR or orbitrap cell. TOF - Time of flight.\n\n\nParent intensity fraction\nThe percentage the parent ion intensity makes up of the total intensity in the selection window.\n\n\nFraction of total spectrum\nThe percentage the parent ion intensity makes up of the total intensity of the whole MS spectrum.\n\n\nBase peak fraction\nThe percentage the parent ion intensity in comparison to the highest peak in he MS spectrum.\n\n\nPrecursor full scan number\nThe full scan number where the precursor ion was selected for fragmentation.\n\n\nPrecursor intensity\nThe intensity of the precursor ion at the scannumber it was selected.\n\n\nPrecursor apex fraction\nThe fraction the intensity of the precursor ion makes up of the peak (apex) intensity.\n\n\nPrecursor apex offset\nHow many full scans the precursor ion is offset from the peak (apex) position.\n\n\nPrecursor apex offset time\nHow much time the precursor ion is offset from the peak (apex) position.\n\n\nScan event number\nThis number indicates which MS/MS scan this one is in the consecutive order of the MS/MS scans that are acquired after an MS scan.\n\n\nModifications\nPost-translational modifications contained within the sequence. When no modifications exist, this is set to ‘unmodified’. Note: This column only set when this MS/MS spectrum has been identified.\n\n\nModified sequence\nSequence representation of the peptide including location(s) of modified AAs. Note: This column only set when this MS/MS spectrum has been identified.\n\n\nProteins\nIdentifiers of proteins this peptide is associated with. Note: This column only set when this MS/MS spectrum has been identified.\n\n\nScore\nThe score of the identification (higher is better). Note: This column only set when this MS/MS spectrum has been identified.\n\n\nFraction\nThe identifier of the fraction the sample was taken from.\n\n\ncolName\n\n\n\nDN Sequence\n\n\n\nDN Score\n\n\n\nDN Normalized Score\n\n\n\nDN nterm mass\n\n\n\nDN cterm mass\n\n\n\nDN missing mass\n\n\n\nDN Sequence2\n\n\n\nDN Score2\n\n\n\nDN Normalized Score2\n\n\n\nDN nterm mass2\n\n\n\nDN cterm mass2\n\n\n\nDN missing mass2\n\n\n\nDN Score diff\n\n\n\nDP Mass Difference\nThis dependent peptide’s mass difference to the associated identified peptide.\n\n\nDP Time Difference\nThis dependent peptide’s time difference to the associated identified peptide.\n\n\nDP Score\nThe andromeda identification score.\n\n\nDP PEP\nPosterior Error Probability of the identification. This value essentially operates as a p-value, where smaller is more significant.\n\n\nDP Positional Probability\n\n\n\nDP Base Sequence\n\n\n\nDP Probabilities\n\n\n\nDP AA\n\n\n\nDP Base Scan Number\n\n\n\nDP Mod Scan Number\n\n\n\nDP Decoy\n\n\n\nDP Proteins\n\n\n\nDP Cluster Index\n\n\n\nDP Cluster Mass\n\n\n\nDP Cluster Mass SD\n\n\n\nDP Cluster Size Total\n\n\n\nDP Cluster Size Forward\n\n\n\nDP Cluster Size Reverse\n\n\n\nDP Modification\nPost-translational modifications contained within the sequence. When no modifications exist, this is set to ‘unmodified’.\n\n\nDP Peptide Length Difference\n\n\n\nDP Gene Names\nNames of proteins the identified peptide is associated with.\n\n\nDP Protein Names\nDescriptions of the proteins the identified peptide is associated with.\n\n\nIntens Comp Factor\nTaken from the Thermo RAW file.\n\n\nCTCD Comp\nTaken from the Thermo RAW file.\n\n\nRawOvFtT\nFor Thermo Fisher only. TIC estimation done with the orbitrap cell.\n\n\nAGC Fill\nTaken from the Thermo RAW file.\n\n\nScan index\nConsecutive index of the MS/MS spectrum.\n\n\nMS scan index\nConsecutive index of the MS spectrum prior to this MS/MS spectrum.\n\n\nMS scan number\nScan number of the MS spectrum prior to this MS/MS spectrum."
  },
  {
    "objectID": "output_tables.html#msms-table",
    "href": "output_tables.html#msms-table",
    "title": "Output Tables",
    "section": "11 ms/ms Table",
    "text": "11 ms/ms Table\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nRaw file\nThe name of the RAW file the mass spectral data was read from.\n\n\nScan number\nThe RAW-file derived scan number of the MS/MS spectrum.\n\n\nScan index\nThe consecutive index of the MS/MS spectrum.\n\n\nSequence\nThe identified AA sequence of the peptide.\n\n\nLength\nThe length of the sequence stored in the column “Sequence”.\n\n\n“Missed cleavages (” + enzyme + “)”\nNumber of missed enzymatic cleavages.\n\n\nMissed cleavages\nNumber of missed enzymatic cleavages.\n\n\nModifications\nPost-translational modifications contained within the identified peptide sequence.\n\n\nModified sequence\nSequence representation including the post-translational modifications (abbreviation of the modification in brackets before the modified AA). The sequence is always surrounded by underscore characters (’_’).\n\n\ntitle + ” Probabilities”\n“Sequence representation of the peptide including PTM positioning probabilities ([0..1], where 1 is best match) for ’” + title + “’.”\n\n\ntitle + ” Score Diffs”\n\n\n\nTables.ModificationList[t].Name\n\n\n\nProteins\nThe identifiers of the proteins the identified peptide is associated with.\n\n\nGene Names\nNames of genes the identified peptide is associated with.\n\n\nProtein Names\nDescriptions of the proteins the identified peptide is associated with.\n\n\nCharge\nThe charge state of the precursor ion.\n\n\nFragmentation\nThe type of fragmentation used to create the MS/MS spectrum. CID - Collision Induced Dissociation. HCD - High energy Collision induced Dissociation. ETD - Electron Transfer Dissociation.\n\n\nMass analyzer\nThe mass analyzer used to record the MS/MS spectrum. ITMS - Ion trap. FTMS - Fourier transform ICR or orbitrap cell. TOF - Time of flight.\n\n\nType\nThe type of precursor ion as identified by MaxQuant. ISO - isotopic cluster. PEAK - single peak. MULTI - labeling cluster.\n\n\nScan event number\n\n\n\nIsotope index\n\n\n\nm/z\nThe mass-over-charge of the precursor ion.\n\n\nMass\nThe charge corrected mass of the precursor ion.\n\n\nMass Error [ppm]\nMass error of the recalibrated mass-over-charge value of the precursor ion in comparison to the predicted monoisotopic mass of the identified peptide sequence.\n\n\nSimple Mass Error [ppm]\n\n\n\nRetention time\nThe uncalibrated retention time in minutes where the MS/MS spectrum has been acquired.\n\n\nPEP\nPosterior Error Probability of the identification. This value essentially operates as a p-value, where smaller is more significant.\n\n\nScore\nAndromeda score for the best associated MS/MS spectrum.\n\n\nDelta score\nScore difference to the second best identified peptide with a different amino acid sequence.\n\n\nScore diff\nScore difference to the second best positioning of modifications identified peptide with the same amino acid sequence.\n\n\nLocalization prob\n\n\n\nCombinatorics\nNumber of possible distributions of the modifications over the peptide sequence.\n\n\nLabeling State\nLabeling state of the precursor isotope pattern used to identify the peptide.\n\n\nPIF\nShort for Parent Ion Fraction; indicates the fraction the target peak makes up of the total intensity in the inclusion window.\n\n\nFraction of total spectrum\nThe percentage the parent ion intensity makes up of the total intensity of the whole spectrum.\n\n\nBase peak fraction\nThe percentage the parent ion intensity in comparison to the highest peak in he MS spectrum.\n\n\nPrecursor Full ScanNumber\nThe full scannumber where the precursor ion was selected for fragmentation.\n\n\nPrecursor Intensity\nThe intensity of the precursor ion at the scannumber it was selected.\n\n\nPrecursor Apex Fraction\nThe fraction the intensity of the precursor ion makes up of the peak (apex) intensity.\n\n\nPrecursor Apex Offset\nHow many full scans the precursor ion is offset from the peak (apex) position.\n\n\nPrecursor Apex Offset Time\nHow much time the precursor ion is offset from the peak (apex) position.\n\n\n“Diagnostic peak” + modName + ” ” + diagAa[i]\n\n\n\nMatches\nThe species of the peaks in the fragmentation spectrum after TopN filtering.\n\n\nIntensities\nThe intensities of the peaks in the fragmentation spectrum after TopN filtering.\n\n\nMass Deviations [Da]\nThe mass deviation of each peak in the fragmentation spectrum in absolute mass units.\n\n\nMass Deviations [ppm]\nThe mass deviation of each peak in the fragmentation spectrum in parts per million.\n\n\nMasses\nThe masses-over-charge of the peaks in the fragmentation spectrum.\n\n\nNumber of Matches\nThe number of peaks matching to the predicted fragmentation spectrum.\n\n\nIntensity coverage\nThe fraction of intensity in the MS/MS spectrum that is annotated.\n\n\nPeak coverage\nThe fraction of peaks in the MS/MS spectrum that are annotated.\n\n\nNeutral loss level\nHow many neutral losses were applied to each fragment in the Andromeda scoring.\n\n\nETD identification type\nFor ETD spectra several different combinations of ion series are scored. Here the highest scoring combination is indicated\n\n\nReverse\nWhen marked with ‘+’, this particular peptide was found to be part of a protein derived from the reversed part of the decoy database. These should be removed for further data analysis.\n\n\nAll scores\n\n\n\nAll sequences\n\n\n\nAll modified sequences\n\n\n\nid\nA unique (consecutive) identifier for each row in the msms table, which is used to cross-link the information in this file with the information stored in the other files.\n\n\nProtein group IDs\nThe identifier of the protein-group this redundant peptide sequence is associated with, which can be used to look up the extended protein information in the file ‘proteinGroups.txt’. As a single peptide can be linked to multiple proteins (e.g. in the case of razor-proteins), multiple id’s can be stored here separated by a semicolon. As a protein can be identified by multiple peptides, the same id can be found in different rows.\n\n\nPeptide ID\nThe identifier of the non-redundant peptide sequence.\n\n\nMod. peptide ID\nIdentifier of the associated modification summary stored in the file ‘modificationSpecificPeptides.txt’.\n\n\nEvidence ID\nIdentifier of the associated evidence stored in the file ‘evidence.txt’.\n\n\nt + ” site IDs”\n“Identifier of the oxidation summary stored in the file ’” + t + “Sites.txt’.”\n\n\n“Reporter intensity” + i\n\n\n\n“Reporter intensity not corrected” + i\n\n\n\nReporter PIF\n\n\n\nReporter fraction"
  },
  {
    "objectID": "output_tables.html#aif-msms-table",
    "href": "output_tables.html#aif-msms-table",
    "title": "Output Tables",
    "section": "12 AIF ms/ms Table",
    "text": "12 AIF ms/ms Table\n====== AIF MS/MS ======\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nid\nA unique (consecutive) identifier for each row in the AIF MS/MS table, which is used to cross-link the information in this file with the information stored in the other files.\n\n\nProtein group IDs\nThe identifier of the protein group this redundant peptide sequence is associated with, which can be used to look up the extended protein information in the file ‘proteinGroups.txt’. As a single peptide can be linked to multiple proteins (e.g. in the case of razor-proteins), multiple id’s can be stored here separated by a semicolon. As a protein can be identified by multiple peptides, the same id can be found in different rows.\n\n\nPeptide ID\nThe identifier of the non-redundant peptide sequence.\n\n\nMod. peptide ID\nIdentifier of the associated modification summary stored in the file ‘modificationSpecificPeptides.txt’.\n\n\nEvidence ID\nIdentifier for analyzed peptide evidence associated with the protein group referenced against the evidences table.\n\n\nt + ” site IDs”\n\n\n\nRaw file\nName of the RAW file the spectral data was extracted from, which led to the identification of this peptide.\n\n\nSequence\nThe identified AA sequence of the peptide.\n\n\nLength\nThe length of the sequence stored in the column “Sequence”.\n\n\n“Missed Cleavages (” + enzyme + “)”\nNumber of missed enzymatic cleavages.\n\n\nMissed Cleavages\nNumber of missed enzymatic cleavages.\n\n\nModifications\nPost-translational modifications contained within the sequence. When no modifications exist, this is set to ‘unmodified’. Note: This column only set when this MS/MS spectrum has been identified.\n\n\nModified Sequence\nSequence representation of the peptide including location(s) of modified AAs. Note: This column only set when this MS/MS spectrum has been identified.\n\n\nTables.ModificationList[t].Name + ” Probabilities”\n\n\n\nTables.ModificationList[t].Name + ” Score Diffs”\n\n\n\nTables.ModificationList[t].Name\n\n\n\nProteins\nThe IPI identifiers of the proteins the identified peptide is associated with.\n\n\nCharge\nThe charge of the precursor ion.\n\n\nm/z\nThe mass-over-charge of the precursor ion.\n\n\nMass\nThe charge corrected mass of the precursor ion.\n\n\nRetention time\nThe uncalibrated retention time in minutes in the elution profile of the precursor ion.\n\n\nPrecursor intensity\nThe intensity of the precursor ion.\n\n\nPEP\nPosterior Error Probability of the identification. This value essentially operates as a p-value, where smaller is more significant.\n\n\nScore\nAndromeda identification score for the MS/MS spectrum.\n\n\nDelta score\nScore difference to the second best identified peptide.\n\n\nCombinatorics\nNumber of possible distributions of the modifications over the peptide sequence.\n\n\nMatches\n\n\n\nIntensities\nThe intensities of the peaks in the fragmentation spectrum after top-N filtering.\n\n\nMass Deviations\nThe search engine allowed mass deviations of the peaks in the fragmentation spectrum.\n\n\nMasses\nThe masses-over-charge of the peaks in the fragmentation spectrum.\n\n\nCharges\n\n\n\nCorrelations\n\n\n\nNumber of Matches\n\n\n\nReverse\nWhen marked with ‘+’, this particular peptide was found to be part of a protein derived from the reversed part of the decoy database. These should be removed for further data analysis."
  },
  {
    "objectID": "scaletointerval.html",
    "href": "scaletointerval.html",
    "title": "Scale to interval",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: ScaleToInterval.cs"
  },
  {
    "objectID": "scaletointerval.html#matrix-access",
    "href": "scaletointerval.html#matrix-access",
    "title": "Scale to interval",
    "section": "3.1 Matrix access",
    "text": "3.1 Matrix access\nDefines if rows or columns should be scaled to an interval (default: Rows). The lowest value of each column/row results in the value determined at the “Minimum” parameter and the largest value of the row/column is the value determined at the “Maximum” parameter."
  },
  {
    "objectID": "scaletointerval.html#minimum",
    "href": "scaletointerval.html#minimum",
    "title": "Scale to interval",
    "section": "3.2 Minimum",
    "text": "3.2 Minimum\nDefines the lower bound of the interval all the values within a row/column will be scaled to (default: 0)."
  },
  {
    "objectID": "scaletointerval.html#maximum",
    "href": "scaletointerval.html#maximum",
    "title": "Scale to interval",
    "section": "3.3 Maximum",
    "text": "3.3 Maximum\nDefines the upper bound of the interval all the values within a row/column will be scaled to (default: 1)."
  },
  {
    "objectID": "subtractrowcluster.html",
    "href": "subtractrowcluster.html",
    "title": "Subtract row cluster",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: SubtractRowCluster.cs"
  },
  {
    "objectID": "subtractrowcluster.html#indicator-column",
    "href": "subtractrowcluster.html#indicator-column",
    "title": "Subtract row cluster",
    "section": "3.1 Indicator column",
    "text": "3.1 Indicator column\nSelected categorical column that should be used for filtering the rows that should be used to calculate the subtrahend for the normalization (default: first categorical column in the matrix)."
  },
  {
    "objectID": "subtractrowcluster.html#value",
    "href": "subtractrowcluster.html#value",
    "title": "Subtract row cluster",
    "section": "3.2 Value",
    "text": "3.2 Value\nSpecifies the value/text term that will be searched in the previously defined “Indicator column” (default: +). The average of rows containing the value/text term is subtracted from all other rows."
  },
  {
    "objectID": "summarystatisticsrows.html",
    "href": "summarystatisticsrows.html",
    "title": "Summary statistics (rows)",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: SummaryStatisticsRows.cs"
  },
  {
    "objectID": "summarystatisticsrows.html#expression-column-selection",
    "href": "summarystatisticsrows.html#expression-column-selection",
    "title": "Summary statistics (rows)",
    "section": "3.1 Expression column selection",
    "text": "3.1 Expression column selection\nSelected expression columns that are used to summarize each row (default: use all expression columns). The way of choosing the expression columns can be specified by selecting one of the three options:\n\nUse all expression columns\nSelect columns\nWithin groups\n\n\n3.1.1 Columns\nThis parameter is just relevant, if “Expression column selection” is set to “Select columns”. Manually selected expression columns, whose values in each row should be used for the calculation of the defined operations (default: all expression columns are selected).\n\n\n3.1.2 Group\nThis parameter is just relevant, if “Expression column selection” is set to “Within groups”. Selected categorical row that defines which values in a row are grouped together to calculate the specified summary values (default: first categorical row in the matrix)."
  },
  {
    "objectID": "summarystatisticsrows.html#calculate",
    "href": "summarystatisticsrows.html#calculate",
    "title": "Summary statistics (rows)",
    "section": "3.2 Calculate",
    "text": "3.2 Calculate\nList of quantities that are calculated for the selected columns (default: all of the below listed quantities are selected). The available quantities are:\n\nSum\nMean\nTurkey biweight\nStandard deviation\nCoefficient of variation\nMedian absolute deviation\nMinimum\nMaximum\nRange\nValid values\nInter-quartile range\n1st quartile\n3rd quartile\nSkewness\nKurtosis"
  },
  {
    "objectID": "combineannotations.html",
    "href": "combineannotations.html",
    "title": "Combine annotations",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: CombineAnnotations.cs\n\n===== Brief description =====\nSearch multiple categorical or string columns for the occurrence of a set of terms.\nOutput: A new categorical column is generated indicating the presence of any of these terms."
  },
  {
    "objectID": "combineannotations.html#categories",
    "href": "combineannotations.html#categories",
    "title": "Combine annotations",
    "section": "2.1 Categories",
    "text": "2.1 Categories\nSelect categorical and/or text columns that should be searched for the specified “Search terms” (default: no columns selected)."
  },
  {
    "objectID": "combineannotations.html#search-terms",
    "href": "combineannotations.html#search-terms",
    "title": "Combine annotations",
    "section": "2.2 Search terms",
    "text": "2.2 Search terms\nSpecified terms will be searched in the previously defined categorical and/or text columns (default: empty). Multiple terms are possible and must be separated by new lines, so each line in the “Search terms” field will be searched in each column. In the newly generated categorical column rows containing one of the search terms in one of the selected columns are indicated by a “\\(+\\)”."
  },
  {
    "objectID": "combineannotations.html#name-of-new-column",
    "href": "combineannotations.html#name-of-new-column",
    "title": "Combine annotations",
    "section": "2.3 Name of new column",
    "text": "2.3 Name of new column\nThe name of the new generated categorical column containing a “\\(+\\)” if one one “Search terms” matches one of the selected columns."
  },
  {
    "objectID": "combineannotations.html#inverse",
    "href": "combineannotations.html#inverse",
    "title": "Combine annotations",
    "section": "2.4 Inverse",
    "text": "2.4 Inverse\nIf checked, rows not matching the “Search terms” are indicated by a “\\(+\\)” (default: unchecked)."
  },
  {
    "objectID": "unstructuredtxtupload.html",
    "href": "unstructuredtxtupload.html",
    "title": "Unstructured Text Upload",
    "section": "",
    "text": "Type: - Matrix Upload\nSource code: UnstructuredTxtUpload.cs"
  },
  {
    "objectID": "unstructuredtxtupload.html#file",
    "href": "unstructuredtxtupload.html#file",
    "title": "Unstructured Text Upload",
    "section": "3.1 File",
    "text": "3.1 File\nSpecifies the file path of the text file that should be uploaded (default: empty). It can be specified manually by typing in the path or the file can be browsed by using the “Select” button."
  },
  {
    "objectID": "unstructuredtxtupload.html#split-into-columns",
    "href": "unstructuredtxtupload.html#split-into-columns",
    "title": "Unstructured Text Upload",
    "section": "3.2 Split into columns",
    "text": "3.2 Split into columns\nIf checked the lines of the specified text file are split into several columns (default: FALSE)."
  },
  {
    "objectID": "unstructuredtxtupload.html#separator",
    "href": "unstructuredtxtupload.html#separator",
    "title": "Unstructured Text Upload",
    "section": "3.3 Separator",
    "text": "3.3 Separator\nThis parameter is just relevant, if the parameter “Split into columns” is TRUE.\nIt specifies how the values within a row of the text file are separated (default: Tab). The separation type can be selected from a predefined list:\n\nTab\nComma"
  },
  {
    "objectID": "unstructuredtxtupload.html#parameter-window",
    "href": "unstructuredtxtupload.html#parameter-window",
    "title": "Unstructured Text Upload",
    "section": "3.4 Parameter window",
    "text": "3.4 Parameter window\n\n\n\nRaw upload"
  },
  {
    "objectID": "unitvector.html",
    "href": "unitvector.html",
    "title": "Unit Vector",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: UnitVector.cs"
  },
  {
    "objectID": "unitvector.html#matrix-access",
    "href": "unitvector.html#matrix-access",
    "title": "Unit Vector",
    "section": "3.1 Matrix access",
    "text": "3.1 Matrix access\nSpecifies whether rows or columns should be regarded as high-dimensional vectors and be transformed into unit vectors (default: Rows)."
  },
  {
    "objectID": "WGCNA.html",
    "href": "WGCNA.html",
    "title": "WGCNA",
    "section": "",
    "text": "This package is provided through the R-language integration into Perseus and therefore requires R as well as the package itself to be installed in order to be used. Visit the PluginInterop and PerseusR for additional technical information on the integration of R and Perseus."
  },
  {
    "objectID": "WGCNA.html#weighted-gene-co-expression-network-analysis",
    "href": "WGCNA.html#weighted-gene-co-expression-network-analysis",
    "title": "WGCNA",
    "section": "",
    "text": "This package is provided through the R-language integration into Perseus and therefore requires R as well as the package itself to be installed in order to be used. Visit the PluginInterop and PerseusR for additional technical information on the integration of R and Perseus."
  },
  {
    "objectID": "WGCNA.html#installation",
    "href": "WGCNA.html#installation",
    "title": "WGCNA",
    "section": "2 Installation",
    "text": "2 Installation\n\nDownload the R installer install it.\nRemember the R installation directory. If you do not know where you have installed R, check the default location C:\\Program Files\\R\\R-3.5.0\\bin, or search for Rscript.exe.\nAdd R to your Path environment variable. Open the start menu and search for “environment variables”, and select “Edit system environment variables”.\nFind the Path variable in the editor and select Edit.\nNow we can add R to the Path by clicking New and entering the location of the .../bin folder of the R installation, e.g C:\\Program Files\\R\\R-3.5.0\\bin.\nIn certain Windows version you might have to edit the Path directly. The Path consists of folder locations separated by ‘;’. Therefore, just append the installation location of R and the ‘;’ separator. No spaces or quotes are required. If the Path variable is not defined on your system, you can define it yourself.\nCheck that R is installed correctly by opening cmd.exe from the start menu and entering R An R session should start.\nInside the running R session, install the WGCNA and PerseusR libraries by entering/pasting the following code line-by-line. When prompted for local install, type ‘yes’ twice and select a close-by server for the package download.\n\n\n# | eval: false\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"WGCNA\", \"devtools\"))\nlibrary(devtools)\ninstall_github('jdrudolph/PerseusR')\n\n\nYou should now be able to load the WGCNA and PerseusR libraries by entering the following code. Upon loading, WGCNA will print some output, while PerseusR will not print any.\n\n\n# | eval: false\nlibrary(WGCNA)\nlibrary(PerseusR)\n\nYou are now ready to perform co-expression analysis from within Perseus."
  },
  {
    "objectID": "WGCNA.html#wgcna-functions",
    "href": "WGCNA.html#wgcna-functions",
    "title": "WGCNA",
    "section": "3 WGCNA functions",
    "text": "3 WGCNA functions\n\nSoft-threshold\nCo-expression clustering"
  },
  {
    "objectID": "tabseparatedexport.html",
    "href": "tabseparatedexport.html",
    "title": "Generic matrix export",
    "section": "",
    "text": "Type: - Matrix Export\nSource code: TabSeparatedExport.cs"
  },
  {
    "objectID": "tabseparatedexport.html#file",
    "href": "tabseparatedexport.html#file",
    "title": "Generic matrix export",
    "section": "3.1 File",
    "text": "3.1 File\nSpecifies the file path, where the currently selected matrix should be saved (default: empty). It can be specified manually by typing in the path or the location can be browsed by using the “Select” button."
  },
  {
    "objectID": "tabseparatedexport.html#parameter-window",
    "href": "tabseparatedexport.html#parameter-window",
    "title": "Generic matrix export",
    "section": "3.2 Parameter window",
    "text": "3.2 Parameter window"
  },
  {
    "objectID": "categorycounting.html",
    "href": "categorycounting.html",
    "title": "Category counting",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. columns\nSource code: CategoryCounting.cs"
  },
  {
    "objectID": "categorycounting.html#categories",
    "href": "categorycounting.html#categories",
    "title": "Category counting",
    "section": "3.1 Categories",
    "text": "3.1 Categories\nSelected categorical columns for which the absolute and relative occurrence of each term gets counted and displayed in two newly generated numerical columns called “Count” and “Percentage of total” (default: all categorical columns are selected)."
  },
  {
    "objectID": "categorycounting.html#min.-count",
    "href": "categorycounting.html#min.-count",
    "title": "Category counting",
    "section": "3.2 Min. count",
    "text": "3.2 Min. count\nMinimal occurrence of a certain term to be displayed in the output table (default: 1)."
  },
  {
    "objectID": "categorycounting.html#selection",
    "href": "categorycounting.html#selection",
    "title": "Category counting",
    "section": "3.3 Selection",
    "text": "3.3 Selection\nSelected categorical column in which the “Value” will be searched and counted for each term in the “Categories” field specified categorical columns (default: ). Two new numerical columns called “Selection count” and “Selection percentage” are generated in the output table containing the absolute and relative amounts."
  },
  {
    "objectID": "categorycounting.html#value",
    "href": "categorycounting.html#value",
    "title": "Category counting",
    "section": "3.4 Value",
    "text": "3.4 Value\nDefines the value that is searched in the categorical column specified in “Selection” (default: +)."
  },
  {
    "objectID": "fillcategoricalcolumn.html",
    "href": "fillcategoricalcolumn.html",
    "title": "Fill categorical columns",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: not public."
  },
  {
    "objectID": "fillcategoricalcolumn.html#columns",
    "href": "fillcategoricalcolumn.html#columns",
    "title": "Fill categorical columns",
    "section": "3.1 Columns",
    "text": "3.1 Columns\nSelected categorical columns, whose empty cells should be filled with the defined “Value” (default: all categorical columns are selected)."
  },
  {
    "objectID": "fillcategoricalcolumn.html#value",
    "href": "fillcategoricalcolumn.html#value",
    "title": "Fill categorical columns",
    "section": "3.2 Value",
    "text": "3.2 Value\nThe empty cells of the selected categorical columns will be filled with the defined value, which has to be typed in the predefined text field (default: empty)."
  },
  {
    "objectID": "andromeda_user-interface.html",
    "href": "andromeda_user-interface.html",
    "title": "Andromeda User Interface",
    "section": "",
    "text": "There are two way to run Andromeda, within the MaxQuant or as a stand-alone tool."
  },
  {
    "objectID": "andromeda_user-interface.html#andromeda-with-gui",
    "href": "andromeda_user-interface.html#andromeda-with-gui",
    "title": "Andromeda User Interface",
    "section": "2.1 Andromeda with GUI",
    "text": "2.1 Andromeda with GUI\nThe user interface of the Andromeda stand-alone version has the MaxQuant logo on the top left. The functionality buttons can be found on the right (highlighted green rectangle in Figure 2).\nLoading “Source Files” is possible with the “Browse” button. The corresponding “Parameter Files” can be loaded by clicking on the source files and using the “Select .apar” button. It can also be defined, whether a protein summary should be reported, whether the results should have a minimal score and how many threads should be used for the calculation.\n\n\n\nFigure 2: Andromeda User Interface standalone"
  },
  {
    "objectID": "andromeda_user-interface.html#andromeda-as-command-line-tool",
    "href": "andromeda_user-interface.html#andromeda-as-command-line-tool",
    "title": "Andromeda User Interface",
    "section": "2.2 Andromeda as command line tool",
    "text": "2.2 Andromeda as command line tool\nTo figure out how to specify the parameters in the command line tool, follow the steps below:\n\nOpen a cmd\nGo to the unziped Andromeda folder\nRun the executable AndromedaCmd.exe without parameters (highlighted by a cyan rectangle in Figure 3) to figure out the Andromeda settings (highlighted by a red rectangle in Figure 3).\n\n\n\n\nFigure 3: Andromeda User Interface standalone"
  },
  {
    "objectID": "andromeda_modifications_table.html",
    "href": "andromeda_modifications_table.html",
    "title": "Andromeda modifications table",
    "section": "",
    "text": "For the here shown step by step description Andromeda was used within MaxQuant (version 1.5.3.8).\n\n\nOpen MaxQuant and go to the Andromeda configuration tab. The “Modifications” page is already selected Figure 1.\n\n\n\nFigure 1: Andromeda modification page\n\n\n\n\nFirst we will have a look at a modification to understand how modifications are defined in MaxQuant.\nChoose the “Phospho (STY)” modification in the table on the left hand side. You can either browse through the table or use the \"Find...\" function of Andromeda by right clicking on the table and choosing it from the appearing menu. Then a pop-up window will show up and you can specify what you are searching and whether the term should be searched (parameter “Look in”) in the whole table or in a specific column. in Figure 2 we search in the “Name” column.\n\n\n\nFigure 2: Andromeda search modification window\n\n\nThe row will be selected and the settings of the modification appear on the right hand side of the Andromeda window (s. Figure 3. Only one modification is defined together for S, T and Y. This is important for the proper calculation of localization scores for phosphorylation sites. You will notice that a neutral loss of a phosphate is only defined for serine and threonine by clicking on the amino acids in the window on the right.\nA diagnostic peak of composition \\(C_8H_{10}O_4P\\) is only defined for tyrosine, which is the characteristic immonium ion for tyrosine phosphorylation. This will only be relevant for fragmentation spectra in which the lower mass range is available.\n\n\n\nFigure 3: Andromeda modification information\n\n\n\n\n\nNow, we will add a new Modification. You can visit unimod.org and select a modification that has not been defined yet in MaxQuant and add this modification to the modification list in MaxQuant. Here we are adding Dehydroalanine from Tyrosine. All the information that we need can be extracted from the unimod homepage (Figure 4) like the compositional change as well as the site specificity.\n\n\n\nFigure 4: Information extracted from unimod can be used to add modifications to the Andromeda search\n\n\nFirst click the “Add” button (left side of the ribbon on Figure 5). Then a new row will be added at the end of the table and a new modification form will appear on the right hand side that can be edited.\n\n\n\nFigure 5: Add modification to the andromeda search\n\n\nThen you just have to transfer the information from the unimod table to the form. Note that when your done you have to press the “Modify table” button to transfer the changes you made on the right side to the table on the left. And to save the table to the modifications.xml file you press the button “Save changes” (highlighted in Figure 6.\nTo have the added modification available in MaxQuant you have to open a new MaxQuant window.\n\n\n\nFigure 6: Save modification\n\n\n\n\n\nNote that the isotopic and isobaric labels are also defined in this table. Labels have written “Label” in the “Type” column and are the labels that can be used in MaxQuant for MS1 level quantification. Isobaric labels can also be identified by their term “isobaric Label” in the “Type” column.\nNow we define a new hypothetical SILAC label, His8 (\\(His^{13}C_5^{15}N_3\\)). (s. Figure 7) Therefore, we add a new entry to the modifications table.\n\n\n\nFigure 7: Add a new label\n\n\nThen the form on the right can be edited. It is important to keep in mind that we are not just adding heavy amino acids, but also have to remove the same amount of “normal” amino acids. Don’t forget to press the “Modify table” button when your done to transfer the changes you made on the right side to the table on the left. And to save the table to the modifications.xml file you have to click the “Save changes” button.\nTo have the added modification available in MaxQuant you have to open a new MaxQuant window.\n\n\n\nFigure 8: save the changed label"
  },
  {
    "objectID": "andromeda_modifications_table.html#open-the-modifications-table",
    "href": "andromeda_modifications_table.html#open-the-modifications-table",
    "title": "Andromeda modifications table",
    "section": "",
    "text": "Open MaxQuant and go to the Andromeda configuration tab. The “Modifications” page is already selected Figure 1.\n\n\n\nFigure 1: Andromeda modification page\n\n\n\n\nFirst we will have a look at a modification to understand how modifications are defined in MaxQuant.\nChoose the “Phospho (STY)” modification in the table on the left hand side. You can either browse through the table or use the \"Find...\" function of Andromeda by right clicking on the table and choosing it from the appearing menu. Then a pop-up window will show up and you can specify what you are searching and whether the term should be searched (parameter “Look in”) in the whole table or in a specific column. in Figure 2 we search in the “Name” column.\n\n\n\nFigure 2: Andromeda search modification window\n\n\nThe row will be selected and the settings of the modification appear on the right hand side of the Andromeda window (s. Figure 3. Only one modification is defined together for S, T and Y. This is important for the proper calculation of localization scores for phosphorylation sites. You will notice that a neutral loss of a phosphate is only defined for serine and threonine by clicking on the amino acids in the window on the right.\nA diagnostic peak of composition \\(C_8H_{10}O_4P\\) is only defined for tyrosine, which is the characteristic immonium ion for tyrosine phosphorylation. This will only be relevant for fragmentation spectra in which the lower mass range is available.\n\n\n\nFigure 3: Andromeda modification information\n\n\n\n\n\nNow, we will add a new Modification. You can visit unimod.org and select a modification that has not been defined yet in MaxQuant and add this modification to the modification list in MaxQuant. Here we are adding Dehydroalanine from Tyrosine. All the information that we need can be extracted from the unimod homepage (Figure 4) like the compositional change as well as the site specificity.\n\n\n\nFigure 4: Information extracted from unimod can be used to add modifications to the Andromeda search\n\n\nFirst click the “Add” button (left side of the ribbon on Figure 5). Then a new row will be added at the end of the table and a new modification form will appear on the right hand side that can be edited.\n\n\n\nFigure 5: Add modification to the andromeda search\n\n\nThen you just have to transfer the information from the unimod table to the form. Note that when your done you have to press the “Modify table” button to transfer the changes you made on the right side to the table on the left. And to save the table to the modifications.xml file you press the button “Save changes” (highlighted in Figure 6.\nTo have the added modification available in MaxQuant you have to open a new MaxQuant window.\n\n\n\nFigure 6: Save modification\n\n\n\n\n\nNote that the isotopic and isobaric labels are also defined in this table. Labels have written “Label” in the “Type” column and are the labels that can be used in MaxQuant for MS1 level quantification. Isobaric labels can also be identified by their term “isobaric Label” in the “Type” column.\nNow we define a new hypothetical SILAC label, His8 (\\(His^{13}C_5^{15}N_3\\)). (s. Figure 7) Therefore, we add a new entry to the modifications table.\n\n\n\nFigure 7: Add a new label\n\n\nThen the form on the right can be edited. It is important to keep in mind that we are not just adding heavy amino acids, but also have to remove the same amount of “normal” amino acids. Don’t forget to press the “Modify table” button when your done to transfer the changes you made on the right side to the table on the left. And to save the table to the modifications.xml file you have to click the “Save changes” button.\nTo have the added modification available in MaxQuant you have to open a new MaxQuant window.\n\n\n\nFigure 8: save the changed label"
  },
  {
    "objectID": "user_intereface.html",
    "href": "user_intereface.html",
    "title": "The Viewer",
    "section": "",
    "text": "The user interface of the Viewer, which is packed with MaxQuant is located at the “Viewer” tab of the MaxQuant software (highlighted by a red rectangle in Figure 1). The title bar along the top has the MaxQuant logo on the left, followed by an icon that can be used to rename the session, which will be displayed and is by default Session 1 - MaxQuant - name of the currently displayed raw file. The ribbon of the “Viewer” tab contains three groups of functions: the Map controls (highlighted by a petrol blue rectangle to the left), the Feature controls (highlighted by a yellow rectangle in the middle) and the Table navigation (highlighted by a red-brown rectangle on the right hand side). The functionalities of all buttons within each category can be obtained from their tool tips.\n\n\n\nFigure 1: Viewer Ribbon\n\n\nThe Viewer window (s. Figure 2) is split into four different areas: the map view (top left), the MS-feature view (bottom left), the table view (top right) and the protein view (bottome right). Each view can be manipulated with the corresponding buttons. The left window with the map view and the MS-feature view can be either split horizontally or vertically by the button highlighted by the blue rectangle in Figure 1.\n\n\n\nFigure 2: Viewer window"
  },
  {
    "objectID": "quantiles.html",
    "href": "quantiles.html",
    "title": "Quantiles",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: Quantiles.cs"
  },
  {
    "objectID": "quantiles.html#number-of-quantiles",
    "href": "quantiles.html#number-of-quantiles",
    "title": "Quantiles",
    "section": "3.1 Number of quantiles",
    "text": "3.1 Number of quantiles\nDefined number of quantiles (default: 5), each of the selected expression/numerical columns will be transformed into."
  },
  {
    "objectID": "quantiles.html#columns",
    "href": "quantiles.html#columns",
    "title": "Quantiles",
    "section": "3.2 Columns",
    "text": "3.2 Columns\nSelected expression/numerical columns that should be transformed into quantiles (default: all expression columns are selected)."
  },
  {
    "objectID": "reorderremovecolumns.html",
    "href": "reorderremovecolumns.html",
    "title": "Reorder / Remove Columns",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Rearrange\nSource code: ReorderRemoveAnnotRows.cs\n\n\n\n2 Brief description\nAnnotation rows can be removed with this activity.\nOutput: Same matrix but with annotation rows removed or in new order.\n\n\n3 Parameters\nA new matrix is generated with the specified columns by selecting/deselecting expression/numerical/multi numerical/categorical/text columns (default: all columns are selected).\n\n\n4 Parameter window\n\n\n\nReorder remove columns\n\n\n{{perseus:user:activities:matrixprocessing:rearrange:rearrange-reorder_remove_columns-edited.png?direct|Perseus pop-up window: Rearrange -&gt; Reorder/remove columns}}"
  },
  {
    "objectID": "histogram.html",
    "href": "histogram.html",
    "title": "Histogram",
    "section": "",
    "text": "Type: - Matrix Analysis\nHeading: - Visualization\nSource code: not public."
  },
  {
    "objectID": "histogram.html#columns",
    "href": "histogram.html#columns",
    "title": "Histogram",
    "section": "3.1 Columns",
    "text": "3.1 Columns\nSelected expression columns, whose values get displayed in separate histograms (default: all expression columns are selected)."
  },
  {
    "objectID": "replacemissingbyconstant.html",
    "href": "replacemissingbyconstant.html",
    "title": "Replace missing values by constant",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Imputation\nSource code: ReplaceMissingByConstant.cs"
  },
  {
    "objectID": "replacemissingbyconstant.html#value",
    "href": "replacemissingbyconstant.html#value",
    "title": "Replace missing values by constant",
    "section": "3.1 Value",
    "text": "3.1 Value\nSpecifies the value that is going to replace missing values in all expression columns."
  },
  {
    "objectID": "onedimannotationenrichment.html",
    "href": "onedimannotationenrichment.html",
    "title": "1D annotation enrichment",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. columns\nSource code: not public."
  },
  {
    "objectID": "onedimannotationenrichment.html#columns",
    "href": "onedimannotationenrichment.html#columns",
    "title": "1D annotation enrichment",
    "section": "3.1 Columns",
    "text": "3.1 Columns\nSelected expression/numerical columns that should be processed (default: no expression/numerical columns are selected)."
  },
  {
    "objectID": "onedimannotationenrichment.html#use-for-truncation",
    "href": "onedimannotationenrichment.html#use-for-truncation",
    "title": "1D annotation enrichment",
    "section": "3.2 Use for truncation",
    "text": "3.2 Use for truncation\nThe truncation can be based on p-values or the Benjamini-Hochberg correction for multiple hypothesis testing (default: Benjamini-Hochberg FDR). Rows with a test result below a specified value (“Threshold value”) are reported as significant."
  },
  {
    "objectID": "onedimannotationenrichment.html#side",
    "href": "onedimannotationenrichment.html#side",
    "title": "1D annotation enrichment",
    "section": "3.3 Side",
    "text": "3.3 Side\nTo apply a two-sided test, where the null hypothesis can be rejected regardless of the direction of the effect “both” has to be selected (default). “high” and “low” are the respective one-sided tests."
  },
  {
    "objectID": "onedimannotationenrichment.html#threshold-value",
    "href": "onedimannotationenrichment.html#threshold-value",
    "title": "1D annotation enrichment",
    "section": "3.4 Threshold value",
    "text": "3.4 Threshold value\nBased on a specified threshold (default: 0.02) a row is reported as significant, if its test result is below the defined value. Depending on the chosen truncation score this threshold value is applied to the p-value or to the Benjamini-Hochberg FDR"
  },
  {
    "objectID": "onedimannotationenrichment.html#relative-enrichment",
    "href": "onedimannotationenrichment.html#relative-enrichment",
    "title": "1D annotation enrichment",
    "section": "3.5 Relative enrichment",
    "text": "3.5 Relative enrichment\nSelected text column, where all rows having the same identifier will be counted as one entity in the 1D annotation enrichment analysis (default: &lt;None&gt;). The main application is for posttranslational modification sites. Then one should select here protein or gene identifiers. This will make sure that multiple sites from the same protein (or gene) are counted only once for the enrichment analysis."
  },
  {
    "objectID": "profileplot.html",
    "href": "profileplot.html",
    "title": "Profile plot",
    "section": "",
    "text": "1 General\n\nType: - Matrix Analysis\nHeading: - Visualization\nSource code: not public.\n\n\n\n2 Brief description\nDisplay each row of the matrix as a quantitative profile with each column defining a data point. Profiles can be queried by similarity to a reference profile.\n\n\n\n3 Parameters\n“Profile plot” has no parameters."
  },
  {
    "objectID": "filterrandomrows.html",
    "href": "filterrandomrows.html",
    "title": "Filter rows based on random sampling",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Filter rows\nSource code: FilterRandomRows.cs"
  },
  {
    "objectID": "filterrandomrows.html#number-of-rows",
    "href": "filterrandomrows.html#number-of-rows",
    "title": "Filter rows based on random sampling",
    "section": "3.1 Number of rows",
    "text": "3.1 Number of rows\nThe filtering is based on random decisions, where a given number of rows is kept (default: total number of rows in the matrix)."
  },
  {
    "objectID": "filterrandomrows.html#filter-mode",
    "href": "filterrandomrows.html#filter-mode",
    "title": "Filter rows based on random sampling",
    "section": "3.2 Filter mode",
    "text": "3.2 Filter mode\nThe “Filter mode” defines, whether the input matrix will be reduced (“Reduce matrix” = default) or a new categorical column called “Filter” will be generated containing the categories “Keep” and “Discard” (“Filter mode” = “Add categorical column”)."
  },
  {
    "objectID": "filtercategoricalcolumn.html",
    "href": "filtercategoricalcolumn.html",
    "title": "Filter rows based on categorical column",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Filter rows\nSource code: FilterCategoricalColumn.cs"
  },
  {
    "objectID": "filtercategoricalcolumn.html#column",
    "href": "filtercategoricalcolumn.html#column",
    "title": "Filter rows based on categorical column",
    "section": "3.1 Column",
    "text": "3.1 Column\nSelected categorical column the filtering is based on (default: first categorical column of the matrix)."
  },
  {
    "objectID": "filtercategoricalcolumn.html#values",
    "href": "filtercategoricalcolumn.html#values",
    "title": "Filter rows based on categorical column",
    "section": "3.2 Values",
    "text": "3.2 Values\nRows with the containing values will be kept/discarded depending on the selected “Mode” (default: no values are selected)."
  },
  {
    "objectID": "filtercategoricalcolumn.html#mode",
    "href": "filtercategoricalcolumn.html#mode",
    "title": "Filter rows based on categorical column",
    "section": "3.3 Mode",
    "text": "3.3 Mode\nThe rows with the selected values will be kept/discarded depending on the selected “Mode” (default: “Remove matching rows”). If “Remove matching rows” is selected, rows having the values will be removed while all other rows will be kept. If “Keep matching rows” is selected, the opposite will happen."
  },
  {
    "objectID": "filtercategoricalcolumn.html#filter-mode",
    "href": "filtercategoricalcolumn.html#filter-mode",
    "title": "Filter rows based on categorical column",
    "section": "3.4 Filter mode",
    "text": "3.4 Filter mode\nThe “Filter mode” defines, whether the input matrix will be reduced (“Reduce matrix” = default) or a new categorical column called “Filter” will be generated containing the categories “Keep” and “Discard” (“Filter mode” = “Add categorical column”)."
  },
  {
    "objectID": "coexpressionclustering.html",
    "href": "coexpressionclustering.html",
    "title": "Co-expression clustering",
    "section": "",
    "text": "This analysis is provided through the R-language integration into Perseus and therefore requires R as well as the WGCNA package to be installed. Visit WGCNA1 page for more information and installation instructions.\nMore information about co-expression clustering can be found at the following resources:\n\nofficial WGCNA tutorials.\nNetwork analysis with WGCNA\nWGCNA - RNA-seq\n\n\n1 Description\nThe co-expression network is created using the defined correlation function. The determined power is applied to the network (see Soft-threshold for more info). Topological overlap distance is used to create the hierarchical clustering dendrogram. The co-expression modules are determined using the dynamic tree-cut method. For each module, a module eigengene is reported, with its name corresponding to the color of the cluster.\n\n\n2 Output\n\nHierarchical clustering heatmap with a dendrogram and automatic cluster assignments.\nMatrix of module eigengenes that represent a cluster. See Correlate for identifying clusters that correlate with clinical/phenotype data.\n\n\n\n\n\n\nReferences\n\n1. Langfelder, P. & Horvath, S. WGCNA: an R package for weighted correlation network analysis. BMC Bioinformatics 9, (2008)."
  },
  {
    "objectID": "hierarchicalcluster.html",
    "href": "hierarchicalcluster.html",
    "title": "Hierarchical clustering",
    "section": "",
    "text": "Type: - Matrix Analysis\nHeading: - Clustering/PCA\nSource code: not public."
  },
  {
    "objectID": "hierarchicalcluster.html#row-tree",
    "href": "hierarchicalcluster.html#row-tree",
    "title": "Hierarchical clustering",
    "section": "3.1 Row tree",
    "text": "3.1 Row tree\nIf checked rows will be clustered and a tree (dendrogram) is generated (default: checked).\n\n3.1.1 Distance\nSelected distance that will be used for the clustering process (default: Euclidean). The distance can be selected from a predefined list:\n\nEuclidean\nL1\nMaximum\nLp\nPearson correlation\nSpearman correlation\nCosine\nCanberra\n\n\n\n3.1.2 Linkage\nSelected clustering method that will be applied (default: Average). It can be selected from a predefined list:\n\nAverage\nComplete\nSingle\n\n\n\n3.1.3 Constraint\nSelected constraint that should be preserved from the input data (default: None). The used constraint can be selected from a predefined list of constraints:\n\nNone\nPreserve order\nPreserve order (periodic)\n\n\n\n3.1.4 Preprocess with k-means\nSpecifies, whether the data should be pre-processed using k-means before applying clustering and generating a heatmap (default: checked).\n\n\n3.1.5 Number of clusters\nThis parameter is just relevant, if the parameter “Preprocess with k-means” is checked. Defines the number of clusters that will be created by the k-means algorithm (default: 300)."
  },
  {
    "objectID": "hierarchicalcluster.html#column-tree",
    "href": "hierarchicalcluster.html#column-tree",
    "title": "Hierarchical clustering",
    "section": "3.2 Column tree",
    "text": "3.2 Column tree\nIf checked, columns will be clustered and a tree (dendrogram) is generated (default: checked).\n\n3.2.1 Distance\nSelected distance that will be used for the clustering process (default: Euclidean). The distance can be selected from a predefined list:\n\nEuclidean\nL1\nMaximum\nLp\nPearson correlation\nSpearman correlation\n\n\n\n3.2.2 Linkage\nSelected clustering method that will be applied (default: Average). It can be selected from a predefined list:\n\nAverage\nComplete\nSingle\n\n\n\n3.2.3 Constraint\nSelected constraint that should be preserved from the input data (default: None). The used constraint can be selected from a predefined list of constraints:\n\nNone\nPreserve order\nPreserve order (periodic)\nPreserve grouping\n\n\n\n3.2.4 Preprocess with k-means\nSpecifies, whether the data should be pre-processed using k-means before applying clustering and generating a heatmap (default: checked).\n\n\n3.2.5 Number of clusters\nThis parameter is just relevant, if the parameter “Preprocess with k-means” is checked. Defines the number of clusters that will be created by the k-means algorithm (default: 300)."
  },
  {
    "objectID": "hierarchicalcluster.html#which-columns-to-use",
    "href": "hierarchicalcluster.html#which-columns-to-use",
    "title": "Hierarchical clustering",
    "section": "3.3 Which columns to use",
    "text": "3.3 Which columns to use\nList of all expression/numerical columns in the data set (default: all numerical columns; the expression columns are selected see parameter “Use for clustering”)."
  },
  {
    "objectID": "hierarchicalcluster.html#use-for-clustering",
    "href": "hierarchicalcluster.html#use-for-clustering",
    "title": "Hierarchical clustering",
    "section": "3.4 Use for clustering",
    "text": "3.4 Use for clustering\nSelected expression/numerical columns that should be used for the clustering (default: all expression columns are selected)."
  },
  {
    "objectID": "hierarchicalcluster.html#display-in-heat-map-but-do-not-use-for-clustering",
    "href": "hierarchicalcluster.html#display-in-heat-map-but-do-not-use-for-clustering",
    "title": "Hierarchical clustering",
    "section": "3.5 Display in heat map but do not use for clustering",
    "text": "3.5 Display in heat map but do not use for clustering\nSelected expression/numerical columns that should be displayed in the output heat map, but are not used for the clustering (default: empty)."
  },
  {
    "objectID": "replacestrings.html",
    "href": "replacestrings.html",
    "title": "Replace strings",
    "section": "",
    "text": "Type: - Matrix MultiProcessing\nHeading: - Basic (MultiProcessing)\nSource code: ReplaceStrings.cs"
  },
  {
    "objectID": "replacestrings.html#column-in-matrix-1-to-be-edited",
    "href": "replacestrings.html#column-in-matrix-1-to-be-edited",
    "title": "Replace strings",
    "section": "3.1 Column in matrix 1 to be edited",
    "text": "3.1 Column in matrix 1 to be edited\nThe column in the first matrix in which strings will be replaced according to the key-value table specified in matrix 2."
  },
  {
    "objectID": "replacestrings.html#keys-in-matrix-2",
    "href": "replacestrings.html#keys-in-matrix-2",
    "title": "Replace strings",
    "section": "3.2 Keys in matrix 2",
    "text": "3.2 Keys in matrix 2\nThe keys for the replacement table."
  },
  {
    "objectID": "replacestrings.html#values-in-matrix-2",
    "href": "replacestrings.html#values-in-matrix-2",
    "title": "Replace strings",
    "section": "3.3 Values in matrix 2",
    "text": "3.3 Values in matrix 2\nThe values for the replacement table."
  },
  {
    "objectID": "convertmultinumeric.html",
    "href": "convertmultinumeric.html",
    "title": "Convert multi-numeric column",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Rearrange\nSource code: ConvertMultiNumeric.cs"
  },
  {
    "objectID": "convertmultinumeric.html#operation",
    "href": "convertmultinumeric.html#operation",
    "title": "Convert multi-numeric column",
    "section": "3.1 Operation",
    "text": "3.1 Operation\nSelected operations that should be applied to multi-numeric columns to gain a numeric column with one value per row (default: no operations are selected). For each selected operation a separate numeric column is generated. The operations can be selected from a predefined list:\n\nCount\nSum\nProduct\nAverage\nMedian"
  },
  {
    "objectID": "convertmultinumeric.html#columns",
    "href": "convertmultinumeric.html#columns",
    "title": "Convert multi-numeric column",
    "section": "3.2 Columns",
    "text": "3.2 Columns\nSelected multi-numeric columns that will be transformed applying each of the selected operations (default: all multi-numeric columns are selected)."
  },
  {
    "objectID": "ClassificationFeatureSelection.html",
    "href": "ClassificationFeatureSelection.html",
    "title": "Classification feature optimization",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Learning\nSource code: not public."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#items-are-in",
    "href": "ClassificationFeatureSelection.html#items-are-in",
    "title": "Classification feature optimization",
    "section": "3.1 Items are in",
    "text": "3.1 Items are in\nIt specifies if the items that should be used for the cross-validation or the prediction can be found in “Columns” or “Rows” (default: Columns).\n\n3.1.1 Classes\nSelected categorical row or column that contains the class of the items (default: first categorical row/column in the matrix). If items are in columns then the classes are in a categorical row, and if items are in rows the classes are in a categorical column.\n\n\n3.1.2 Sub-classes\nThis parameter is just relevant, if the parameter “Items are in” is set to “Columns”. It specifies whether sub-classes should be taken into consideration for the classification process (default: &lt;None&gt;).\n\n\n3.1.3 Feature ranking method\nThis parameter is just relevant, if the parameter “Feature selection” is set to “From feature ranking”. It specifies which features method will be used to rank the features (default: ANOVA). The method can be selected from a predefined list:\n\nANOVA\nHybrid SVM\nMANOVA\nOne-sided t-test\nTwo-way ANOVA\nSVM\nRFE-SVM\nGolub\n\nDepending on the ranking method up to 4 parameters can be specified.\n\n3.1.3.1 S0\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “ANOVA”, “Hybrid SVM”, “One-sided t-test” or “MANOVA”. It defines the artificial within groups variance and controls the relative importance of resulted test p-values and difference between means (default: 0). At \\(s0=0\\) only the p-value matters, while at nonzero \\(s0\\) also the difference of means plays a role. See1 for details.\n\n\n3.1.3.2 C\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Hybrid SVM”, “SVM” or “RFE-SVM”. C is a penalty constant (default: 100). Large C corresponds to large penalties for misclassification and resembles a hard margin classifier.\n\n\n3.1.3.3 Reduction factor\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Hybrid SVM” or “RFE-SVM”. It defines the factor by what the number of features will be reduced step by step during the ranking process (default: 1.414).\n\n\n3.1.3.4 Number of top ANOVA features\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “MANOVA”. It defines how many of the selected features are top ANOVA features.\n\n\n3.1.3.5 Side\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “One-sided t-test”. It defines the “Left” or “Right” side, where the null hypothesis can be rejected (default: Right).\n\n\n3.1.3.6 Orthogonal grouping\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. It defines the grouping of the data according to a given categorical column or row to distinguish the effects of the groups.\n\n\n3.1.3.7 Min. orthogonal p-value\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. Test results above this p-value are defined as orthogonal. ( default: 0).\n\n\n3.1.3.8 Min. interaction p-value\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. Test results above this p-value are defined as interacting, hence the effects of one group do not depend on the other group (default: 0).\n\n\n3.1.3.9 Skip if orthog. P-value is better\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. It defines whether features with an orthogonal p-value better than the given value in “Min. interaction p-value” are filtered out (default: unchecked).\n\n\n3.1.3.10 Number of features\nDefines how many features should be selected (default: 100).\n\n\n3.1.3.11 Group-wise feature sel.\nIf checked, for each defined group in the data a different amount of features can be selected, which are then used for the classification (default: unchecked). The numbers can be defined either by typing in the text field in the form [Group,number] or by using the Edit button."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#classification-algorithm",
    "href": "ClassificationFeatureSelection.html#classification-algorithm",
    "title": "Classification feature optimization",
    "section": "3.2 Classification algorithm",
    "text": "3.2 Classification algorithm\nDefines the algorithm that should be used for the classification (default: Support vector machine). The algorithm can be selected from a predefined list:\n\nSupport vector machine\nFisher LDA\nKNN\n\n\n3.2.1 Kernel\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “Support vector machine”. It defines the kernel function that is used to classify items (default: linear).\nThe kernel function can be selected from a predefined list:\n \\[\\begin{align}\n\\text{Linear:} \\ K(x,y) &= x^{T}y \\\\\n\\text{RBF:}    \\  K(x,y) &= \\exp (-\\sigma\\|x-y\\|^2) , \\sigma \\&gt; 0 \\\\\n\\text{Polynomial:} \\ K(x,y) &= (\\gamma x^{T}y + r)^d , \\gamma \\&gt; 0 \\\\\n\\text{Sigmoid:} \\  K(x,y) &= tanh(\\gamma x^{T}y + r) \\\\\n\\end{align}\\] \nDepending on the chosen function 1 to 4 parameters must be specified.\n\n3.2.1.1 Sigma\nThis parameter is just relevant, if “Kernel” is set to “RBF”. It defines the slope of the function (see formula above, default: 1).\n\n\n3.2.1.2 Degree\nThis parameter is just relevant, if “Kernel” is set to “Polynomial”. It defines the degree of the polynom (see formula above, default: 3).\n\n\n3.2.1.3 Gamma\nThis parameter is just relevant, if “Kernel” is set to “Polynomial” or “Sigmoid”. It defines the slope of the function (see formula above, default: 0.01).\n\n\n3.2.1.4 Coef\nThis parameter is just relevant, if “Kernel” is set to “Polynomial” or “Sigmoid”. It defines a constant (see formula above, default: 0).\n\n\n3.2.1.5 C\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “Support vector machine”. C is a penalty constant (default: 10). Large C corresponds to large penalties for misclassification and resembles a hard margin classifier.\n\n\n\n3.2.2 Distance\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “KNN”. It defines the selected distance that will be used to assign the nearest neighbours to an item and therefore classify it (default: Euclidean). The distance can be selected from a predefined list:\n\nEuclidean\nL1\nMaximum\nLp\nPearson correlation\nSpearman correlation\nCosine\nCanberra\n\n\n\n3.2.3 Number of neighbours\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “KNN”. It specifies the number of closest neighbours that are taken into account for the classification of an item (default: 5)."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#cross-validation-type",
    "href": "ClassificationFeatureSelection.html#cross-validation-type",
    "title": "Classification feature optimization",
    "section": "3.3 Cross-validation type",
    "text": "3.3 Cross-validation type\nThis parameter is just relevant, if the parameter “Cross-validate assigned items” is checked. It defines the type of cross-validation that should be applied to the data set (default: n-fold). The type can be selected from a predefined list:\n\nLeave one out: As many predictors are built as there are items in the data set. Thus for each predictor one item is left out to train the model and the predictor will be evaluated using the left out item. In the end the average prediction performance will be returned.\nn-fold: The items of the data set are split into n equally sized chunks. n predictors will be generated. In each of these prediction models the union of n-1 of these chunks are taken as the training set and the remaining chunk is the test set. In the end the average prediction performance will be returned.\nRandom sampling: The number of predictors is specified by the “Number of repeats” parameter. The number of items taken out to form the test set (and not used for building the predictor) is specified by the “Test set percentage” parameter. In the end the average prediction performance will be returned.\n\nDepending on the cross-validation type 0 to 2 parameters have to specified:\n\n3.3.1 n\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “n-fold”. It defines the number of partitions the data is divided into (default: 4).\n\n\n3.3.2 Test set percentage\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “Random sampling”. It specifies the percentage of the data that is used for testing the trained model (default: 15). The remaining data is used for the training process.\n\n\n3.3.3 Number of repeats\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “Random sampling”. It specifies how often the cross-validation process is repeated (default: 250). In every round the data is again divided according to the previously defined percentage."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#size-reduction-factor",
    "href": "ClassificationFeatureSelection.html#size-reduction-factor",
    "title": "Classification feature optimization",
    "section": "3.4 Size reduction factor",
    "text": "3.4 Size reduction factor\nIt defines the factor by what the number of features will be reduced step by step during the ranking process (default: 1.414)."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#max.number-of-features",
    "href": "ClassificationFeatureSelection.html#max.number-of-features",
    "title": "Classification feature optimization",
    "section": "3.5 Max.number of features",
    "text": "3.5 Max.number of features\nSpecifies the maximal number of features that are kept after feature selection has been applied (default: 100000)."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#number-of-threads",
    "href": "ClassificationFeatureSelection.html#number-of-threads",
    "title": "Classification feature optimization",
    "section": "3.6 Number of threads",
    "text": "3.6 Number of threads\nDefines the number of threads that should be used for the process (default: 1). The number of threads is limited by number of available cores of the machine Perseus in running on."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#parameter-window",
    "href": "ClassificationFeatureSelection.html#parameter-window",
    "title": "Classification feature optimization",
    "section": "3.7 Parameter window",
    "text": "3.7 Parameter window\n\n\n\nClassification feature optimization"
  },
  {
    "objectID": "ClassificationFeatureSelection.html#support-vector-machines",
    "href": "ClassificationFeatureSelection.html#support-vector-machines",
    "title": "Classification feature optimization",
    "section": "4.1 Support vector machines",
    "text": "4.1 Support vector machines\nSupport vector machines (SVMs) were largely developed in the 1990s by Vapnik and co-workers on a basis of a separable bipartition problem at the AT & T Bell Laboratories (see2. SVMs are a family of data analysis algorithms, based on convex quadratic programming, whose successful use has been demonstrated in classification, regression and clustering problems. Thus, SVMs are now the state-of-the-art tools for non-linear input-output knowledge. The following section covers a brief and basic description of SVMs, but detailed explanations can be found in V. N. Vapniks3, N. Cristianinis and J. Shawe-Taylors4, V. N. Vapniks5, V. N. Vapniks6, B. E. Bosers, I. M. Guyons, and V. N. Vapniks7.\nSVMs are a particular class of supervised learning methods that are well suited for analyses of data in high-dimensional feature spaces. They are computationally efficient and capable of detecting biologically-relevant signals. SVMs revolve around the notion of a margin - either side of a data separating linear decision boundary (hyperplane). Maximizing this margin and thereby creating the largest distance between two classes as well as between the hyperplane and the instances on either side, is the main task in training SVMs (see figure below). Thus, these models have a binary nature to separate classes, but can be extended to multi-class problems by reducing the problem to a set of multiple binary classification problems. The hyperplane is defined by:\n \\[\\begin{align}\nD(x)  =  &lt;\\omega,x&gt;  +  b\n\\end{align}\\] \nwhere \\(ω\\) is the weights vector and \\(b\\) is a bias value (or \\(−b\\) the threshold).\nIn case an optimal separating hyperplane is found, data points on the margin are known as support vectors and the solution is a linear combination of them (red data points in figure below). Each new data point is then classified according to its optimal position relative to the model’s hyperplane. So the model complexity is unaffected by the number of features encountered in the training data, therefore SVMs are well suited to deal with learning tasks with a large number of features compared to the number of data points. In case no hyperplane can be found, the problem can be addressed using the so-called soft margin. The margin optimization constraints can be relaxed by allowing some misclassifications or margin violations in the training set, to get better generalization of the SVM than using a hard margin. The choice of appropriate penalties is mandatory:\n \\[\\begin{align}\nmin_{\\omega,b,\\xi} ~& \\frac{1}{2} \\ \\omega^{T}\\omega \\ + \\ C\\sum_{i=1}^{l}\\xi_{i} \\\\\n\\text{subject to} ~& y_{i}(\\omega^{T}x_{i}+b) \\ &lt; \\ 1-\\xi_{i} \\ \\ \\text{and} \\ \\ \\xi \\geq 0\n\\end{align}\\] \nwhere \\(\\omega\\) is the weights vector, \\(b\\) is a bias value, \\(C\\) is a penalty constant, and \\(\\xi\\) is a slack variable, which is the orthogonal distance between a data point and the hyperplane. Large C correspond to large penalties for misclassification and resemble a hard margin classifier, whereas \\(\\xi\\) measures the degree of misclassification or margin violation. This is a good way to deal with outliers in the data set without destroying the model by tailoring it perfectly to the input data.\nNevertheless, most real-world data sets involve separation problems that are linearly non-separable, which requires the definition of complex functions to build a good classifier. SVMs use kernels, a special class of functions to deal with such situations. Mapping the data points to a higher-dimensional space (transformed feature space) using kernels, enables the definition of a linear hyperplane, which results in a non-linear hyperplane in the original space. The hyperplanes in the higher-dimensional space are represented by all points defining a set, whose inner product with a vector is constant in that space. Training the classifier depends only on the data through dot products, which are possible to compute even at a high-dimension at low cost by applying the so-called kernel trick. The trick lies in working in an higher-dimensional space, without ever explicitly transforming the original data points into that space, but instead relying on algorithms that only need to compute inner products within that space. These algorithms are identical to kernels and can thus be cheaply computed in the original space. So, everything about linear cases can also be applied to non-linear ones using an appropriate kernel function. It is common practice to find the best suiting function by cross-validation. Some popular kernels, which are all included in Perseus, are:\n \\[\\begin{align}\n\\text{linear:} \\ K(x,y)         &= x^{T}y  \\\\\n\\text{sigmoid:} \\ K(x,y)    &= tanh(\\gamma x^{T}y \\ + \\ r) \\\\\n\\text{radial basis:} \\ K(x,y)   &= \\exp(-\\gamma|x \\ - \\ y|^{2}) , \\ \\gamma &gt; 0 \\\\\n\\text{polynomial:} \\ K(x,y)     &= (\\gamma x^{T}y \\ + \\ r)^{d}, \\ \\gamma &gt; 0\n\\end{align}\\] \nwhere \\(x\\) and \\(y\\) are two data points, \\(\\gamma\\) is the slope, \\(d\\) is the degree of the polynom, and \\(r\\) is a constant.\n\n\n\nIllustration of separating two classes using SVMs. Linear (A.) and non-linear (B.) perfect separation of two classes (green and orange) with a hyperplane (black) and maximal margin (blue and dotted gray lines). Support vectors defining the hyperplane are in red. No misclassifiactions or margin violations are included.\n\n\nFor more information you can also consult Wikipedia."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#fishers-linear-discriminant-analysis",
    "href": "ClassificationFeatureSelection.html#fishers-linear-discriminant-analysis",
    "title": "Classification feature optimization",
    "section": "4.2 Fisher’s linear discriminant analysis",
    "text": "4.2 Fisher’s linear discriminant analysis\nLinear Discriminant Analysis (LDA), is a well-known classification technique that has been used successfully in many statistical pattern recognition problems. It was developed by R.A. Fisher, a professor of statistics at University College London, and is sometimes called Fisher Discriminant Analysis (FDA). Its first description was in 1936 and can be found in8.\nThe primary purpose of LDA is to separate samples of two or multiple distinct groups while preserving as much of the class discriminatory information as possible to classify new unseen instances. The approach of the LDA is to project all the data points into new space, normally of lower dimension, which maximizes the between-class separability while minimizing their within-class variability. So the goal is to find the best projection axes for separating the classes. In general the number of axes that can be computed by the LDA method is one less than the number of classes in the problem.\n\n\n\nIllustration of separating two classes using LDA. Classes are separated perfectly and the dimensionality of the problem has been reduced from two features (x1,x2) to only a scalar value y.\n\n\nFor more information you can also consult Wikipedia."
  },
  {
    "objectID": "ClassificationFeatureSelection.html#k-nearest-neighbors",
    "href": "ClassificationFeatureSelection.html#k-nearest-neighbors",
    "title": "Classification feature optimization",
    "section": "4.3 k-nearest neighbors",
    "text": "4.3 k-nearest neighbors\nK-Nearest Neighbors (kNN) is a simple lazy learner algorithm that stores all available data points and classifies new instances based on a similarity measure (e.g., distance functions). It corresponds to the group of supervised learning algorithms and has been used in statistical estimation and pattern recognition already in the beginning of 1970’s as a non-parametric technique. During the training phase the algorithm simply stores the data points including their class labels and all computation is deferred until the classification process. So kNN is based on the principle that instances that are in close proximity to another have similar properties. Thus, to classify new unclassified instances, one simply has to look at their k-nearest neighbors, to figure out the classification label. The class membership can be defined by a majority vote of the k closest neighbors or the neighbors can be ranked and weighted according to their distance to the new instance. A common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\n\n\n\nIllustration of classifying a new item using kNN. Using a majority vote of the k nearest neighbors, the defined k can change the assigned class of the red star. If k = 3 (purple circle) the star corresponds to the blue polygon class, because the three closest neighbors include two blue polygons and one green rectangle. Whereas, if k = 5 (black circle) the star is assigned to the green class, because the five closest neighbors include more green rectangles than blue polygons (three green rectangles vs. two blue polygons).\n\n\nFor more information you can also consult Wikipedia."
  },
  {
    "objectID": "reordercolumnsbynumannotationrow.html",
    "href": "reordercolumnsbynumannotationrow.html",
    "title": "Reorder columns by numerical annotation row",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Rearrange\nSource code: ReorderColumnsByNumAnnotationRow.cs\n\n\n\n2 Brief description\nThe order of the columns as they appear in the matrix is changed according to the values in a numerical row in ascending order. This can be useful for displaying columns in a specific order in a heat map.\nOutput: Same matrix but with columns in the new order implied by a numerical row."
  },
  {
    "objectID": "widthadjustment.html",
    "href": "widthadjustment.html",
    "title": "Width adjustment",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Normalization\nSource code: WidthAdjustment.cs\n\n\n\n2 Brief description\nThe first, second and third quartiles (\\(q1\\), \\(q2\\), \\(q3\\)) are calculated from the distribution of all values. The second quartile (which is the median) is subtracted from each value to center the distribution. Then we divide by the width in an asymmetric way. All values that are positive after subtraction of the median are divided by \\(q3 - q2\\) while all negative values are divided by \\(q2 - q1\\).\n\n\n\n3 Parameters\n“Width adjustment” has no parameters."
  },
  {
    "objectID": "removeemptycolumns.html",
    "href": "removeemptycolumns.html",
    "title": "Remove Empty Columns",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Rearrange\nSource code: RemoveEmptyColumns.cs\n\n\n\n2 Brief description\nColumns containing no values or only invalid values will be removed.\nOutput: Same matrix but with empty columns removed.\n\n\n\n3 Parameters\n“Remove empty columns” has no parameters."
  },
  {
    "objectID": "rawFilesTab.html",
    "href": "rawFilesTab.html",
    "title": "Raw files tab",
    "section": "",
    "text": "The Raw files tab displays information about your raw data sources and offers functions for dealing with those sources and the specification of your experimental design. This page first describes the columns of the raw files table and then the functions available to change it, organized following the buttons and button groups.\nWe often refer to a source of raw data as a raw file, but in some data formats the source is actually a folder. Currently, the file-based raw data formats supported and their (case insensitive) extensions are:\nThe folder.based raw data formats and their extensions are:"
  },
  {
    "objectID": "rawFilesTab.html#raw-files-table",
    "href": "rawFilesTab.html#raw-files-table",
    "title": "Raw files tab",
    "section": "1 Raw files table",
    "text": "1 Raw files table\nThe table displaying information about the raw data sources has seven columns.\n\nFile: the absolute path to the file or folder containing the raw data.\nExists: whether the file or folder currently exists at that location on the file system.\nSize: if the path points to an existing file (not a folder), the size of that file.\nData format: the vendor or standard defining the format in which the data is stored, and whether it is file-based or folder-based.\nParameter group: The data files may be organized at the user’s discretion in groups, which may be given different parameters.\nExperiment: The experiment is text that the user can choose and use however convenient, for example to group raw data or to remind him of the characteristics of the samples from which the data was taken.\nFraction: Often the samples will be pre-processed with a procedure, for example centrifugation, that produces several sub-samples in a particular sequence. These sub-samples can be labeled with integers in the fractions column."
  },
  {
    "objectID": "rawFilesTab.html#raw-files-functions",
    "href": "rawFilesTab.html#raw-files-functions",
    "title": "Raw files tab",
    "section": "2 Raw files functions",
    "text": "2 Raw files functions\n\n2.1 Input data group\nThe Load and Load folder buttons are described here, but you should be aware that there are two other ways to load raw files.\nOne is by loading parameters stored in an *.xml file, usually named mqpar.xml. To do this, use the Load parameters item on the drop-down menu just left of the Raw files tab. A file browser will open so you can load the parameters you want, including the raw data files.\nThe other way to load raw files is by drag-and-drop from a file explorer. The behavior will depend on what you select.\n\nIf you select a single file with an *.xml extension (usually mqpar.xml), the raw data files and other parameters stored there will be loaded.\nIf all of the items you select are recognized as raw data files, then these will be loaded.\nIf any of the items you select is not recognized as a raw data file, then each folder among the selected items will be searched recursively for raw data files. All files found will be loaded. If there are also files which are not recognized as raw data, they will be silently passed over.\n\n\n\n2.2 Load button\nPressing the Load button will open a file chooser window. The filter at the lower right may be selected to choose which type of file-based raw data source is visible and may be selected. Folder-based raw data sources cannot be loaded with this button, but the Load folder button (Section 2.4) can be used for these. The folders may be browsed and single or multiple files can be chosen in the usual way. When you have made your selection, press Open. (Remember, if you have chosen a folder, it will be opened for browsing. To load files, you must have only files selected.) The file or files chosen will be added to the table, unless they are already present there.\n\n\n2.3 Remove button\nThis button removes the selected raw data rows from the table.\n\n\n2.4 Load folder button\nThe Load folder button can be used in three ways:\n\nto open a single, folder-based raw data source,\nto open all the raw data sources of any sort (whether file-based or folder based) in the chosen folder, or\nto open all the raw data sources of any sort anywhere under the chosen folder, that is, recursively.\n\nBecause there could be many items in a directory that are not raw data sources, all such items are silently ignored.\n\n\n2.5 Change folder button\nRaw data files loaded by the buttons or with drag-and-drop will, naturally, exist. When an mqpar.xml file is loaded, however, the files mentioned may no longer exist. For example, some processing of the files may have been done and a parameter file saved, but the files subsequently moved to another folder or drive. If the files are still together, the situation may be saved by telling MaxQuant where to find them now. When you press the Change folder button you will be presented with the part of the path that the files in the table have in common and given the opportunity to change it to the path that is currently correct, either by editing or by browsing."
  },
  {
    "objectID": "rawFilesTab.html#experimental-design-file-group",
    "href": "rawFilesTab.html#experimental-design-file-group",
    "title": "Raw files tab",
    "section": "3 Experimental design file group",
    "text": "3 Experimental design file group\n\n3.1 Write template button\nNo documentation yet.\n\n\n3.2 Read from file button\nNo documentation yet."
  },
  {
    "objectID": "rawFilesTab.html#edit-experimental-design-group",
    "href": "rawFilesTab.html#edit-experimental-design-group",
    "title": "Raw files tab",
    "section": "4 Edit experimental design group",
    "text": "4 Edit experimental design group\n\n4.1 Set experiment button\nNormally the experiment is left blank when raw data is loaded. The exception is when data is loaded recursively from a folder, in which case MaxQuant generates an value for the experiment from the paths to the files. To set or change the value of experiment, choose one or more files and press the Set experiment button. You will see a pop-up window where you can enter the new value.\n\n\n4.2 Set parameter group button\nThis button allows you to set or change the value of the parameter group for the selected files to anything from Group 0 to Group 19.\n\n\n4.3 Set fractions button\nWhen you press the Set fractions button, you will be given the opportunity to set two integer values. The value you put in the lower box must be greater than or equal to the value you put in the upper box. The raw data files you have selected will be assigned fraction numbers cyclically from the lower value to the higher value, e.g 1, 2, 3, 1, 2, 3, … .\n\n\n4.4 No fractions button\nPressing this button will remove all values in the fraction column. (Not just the values for the selected files!) It will also recalculate the values in the Experiment column."
  },
  {
    "objectID": "filtervalidvaluesrows.html",
    "href": "filtervalidvaluesrows.html",
    "title": "Filter rows based on valid values",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Filter rows\nSource code: FilterValidValuesRows.cs"
  },
  {
    "objectID": "filtervalidvaluesrows.html#min.-number-of-values",
    "href": "filtervalidvaluesrows.html#min.-number-of-values",
    "title": "Filter rows based on valid values",
    "section": "3.1 Min. number of values",
    "text": "3.1 Min. number of values\nDefines the minimal number of valid values each row needs to have in the expression columns to survive the filtering process (default: 3)."
  },
  {
    "objectID": "filtervalidvaluesrows.html#sec-mode",
    "href": "filtervalidvaluesrows.html#sec-mode",
    "title": "Filter rows based on valid values",
    "section": "3.2 Mode",
    "text": "3.2 Mode\nDefines, which expression columns are counted to define, if a row has enough valid values to survive the filtering process (default: In total). There are three options available:\n\nIn total (includes all expression columns)\nIn each group\nIn at least one group\n\n\n3.2.1 Grouping\nThis parameter is just relevant, if Mode (Section 3.2) is set to “In each group” or “In at least one group”. It defines the grouping of the expression columns that should be used by selecting a categorical row (default: first categorical row of the matrix)."
  },
  {
    "objectID": "filtervalidvaluesrows.html#values-should-be",
    "href": "filtervalidvaluesrows.html#values-should-be",
    "title": "Filter rows based on valid values",
    "section": "3.3 Values should be",
    "text": "3.3 Values should be\nDefines the restriction for a value to be classified as valid (default: Valid). There are seven different possibilities to specify, which entry is counted as a valid value (default: Valid):\n\nValid\nGreater than\nGreater or equal\nLess than\nLess or equal\nBetween\nOutside\n\n\n3.3.1 Minimum\nThis parameter is just relevant, if “Values should be” is set to “Greater than”, “Greater or equal”, “Between” or “Outside”. It defines a lower bound to apply the operation specified in “Values should be” (default: 0).\n\n\n3.3.2 Maximum\nThis parameter is just relevant, if “Values should be” is set to “Less than”, “Less or equal”, “Between” or “Outside”. It defines a upper bound to apply the operation specified in “Values should be” (default: 0)."
  },
  {
    "objectID": "filtervalidvaluesrows.html#filter-mode",
    "href": "filtervalidvaluesrows.html#filter-mode",
    "title": "Filter rows based on valid values",
    "section": "3.4 Filter mode",
    "text": "3.4 Filter mode\nThe “Filter mode” defines, whether the input matrix will be reduced (“Reduce matrix” = default) or a new categorical column called “Filter” will be generated containing the categories “Keep” and “Discard” (“Filter mode” = “Add categorical column”)."
  },
  {
    "objectID": "andromeda_instructions.html",
    "href": "andromeda_instructions.html",
    "title": "Andromeda - Start",
    "section": "",
    "text": "Andromeda1 is a peptide search engine based on probabilistic scoring. On proteome data, Andromeda performs as well as Mascot, a widely used commercial search engine, as judged by sensitivity and specificity analysis based on target decoy searches.\nIt can handle data with arbitrarily high fragment mass accuracy, it is also able to assign and score complex patterns of post-translational modifications, such as highly phosphorylated peptides, and accommodates extremely large databases. Andromeda can function independently or integrated into MaxQuant. This combination enables analysis of large datasets on a desktop computer. Identification of co-fragmented peptides improves the number of identified peptides.\nTo run Andromeda, you will need .NET framework 4.5.\n\n1 Documentation outline\nThis documentation is organized as a wiki, so if you find anything that is wrong or hard to understand, please let us know. The wiki can be browsed by using the links below or by searching with the magnifying glass in the upper right corner.\n\nmaxquant - Download and installation\nAndromeda User Interface\nAndromeda Configuration\nAndromeda Tutorial\nGoogle groups\nmaxquant Bug reporting\nGlossary\n\nYou can find raw file format examples to test Andromeda here.\nFor additional training, consider attending the annual MaxQuant Summer School. Also watching some MaxQuant videos provides more insight.\n\n\n\n\n\nReferences\n\n1. Cox, J. et al. Andromeda: A Peptide Search Engine Integrated into the MaxQuant Environment. Journal of Proteome Research 10, 1794–1805 (2011)."
  },
  {
    "objectID": "cloneprocessing.html",
    "href": "cloneprocessing.html",
    "title": "Clone Processing",
    "section": "",
    "text": "1 General =====\n\nType: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: CloneProcessing.cs\n\n\n\n2 Brief description\nA copy of the input matrix is generated.\nOutput: Same as input matrix.\n\n\n\n3 Parameters\n“Clone” has no parameters."
  },
  {
    "objectID": "sequencelogo.html",
    "href": "sequencelogo.html",
    "title": "Sequence logos",
    "section": "",
    "text": "Type: - Matrix Analysis\nHeading: - Misc. (Analysis)\nSource code: not public."
  },
  {
    "objectID": "sequencelogo.html#sequences",
    "href": "sequencelogo.html#sequences",
    "title": "Sequence logos",
    "section": "3.1 Sequences",
    "text": "3.1 Sequences\nSelected categorical column that contains the amino acid sequences for which a Sequence logo should be generated (default: first categorical column in the matrix).\nHint: The sequences need to have the same length."
  },
  {
    "objectID": "sequencelogo.html#column",
    "href": "sequencelogo.html#column",
    "title": "Sequence logos",
    "section": "3.2 Column",
    "text": "3.2 Column\nSelected categorical column that groups the rows according to their value in that column and generates one sequence logo for each value (default: &lt;None&gt;). If &lt;None&gt; is selected one Sequence logo is generated for all sequences in the column defined in the parameter “Sequences”."
  },
  {
    "objectID": "sequencelogo.html#compute-position-specific-p-values",
    "href": "sequencelogo.html#compute-position-specific-p-values",
    "title": "Sequence logos",
    "section": "3.3 Compute position-specific p-values",
    "text": "3.3 Compute position-specific p-values\nSpecifies the input to calculate the position-specific scoring matrix (PSSM) containing the p-values for each position (default: global occurrence). For each Sequence logo one PSSM is calculated containing the p-value for each amino acid at each position in the sequence. The PSSM can be obtained by clicking on the “Export aa p-values” button in the “Sequence logos” tab of the matrix that was used to generate the Sequence logo(s)."
  },
  {
    "objectID": "replaceimputedbynan.html",
    "href": "replaceimputedbynan.html",
    "title": "Replace missing values by NaN",
    "section": "",
    "text": "1 General =====\n\nType: - Matrix Processing\nHeading: - Imputation\nSource code: ReplaceImputedByNan.cs\n\n\n\n2 Brief description\nReplaces all values that have been imputed with NaN.\nOutput: Same matrix but with imputed values deleted.\n\n\n\n3 Parameters\n“Replace imputed values by NaN” has no parameters."
  },
  {
    "objectID": "normalizebycolumn.html",
    "href": "normalizebycolumn.html",
    "title": "Modify by column",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: not public."
  },
  {
    "objectID": "normalizebycolumn.html#columns",
    "href": "normalizebycolumn.html#columns",
    "title": "Modify by column",
    "section": "3.1 Columns",
    "text": "3.1 Columns\nSelected expression/numerical column(s) that should be modified by the values of the “Control column” using the the action specified in “Modify by” (default: all expression columns are selected)."
  },
  {
    "objectID": "normalizebycolumn.html#control-column",
    "href": "normalizebycolumn.html#control-column",
    "title": "Modify by column",
    "section": "3.2 Control column",
    "text": "3.2 Control column\nOne expression/numerical column that is used to modify all selected expression/numerical columns in the field “Columns” using the action specified in “Modify by” (default: first expression column in the matrix)."
  },
  {
    "objectID": "normalizebycolumn.html#modify-by",
    "href": "normalizebycolumn.html#modify-by",
    "title": "Modify by column",
    "section": "3.3 Modify by",
    "text": "3.3 Modify by\nDefines the operation that is applied between the selected “Columns” and the “Control column” (default: Subtraction). The operation can be chosen from a predefined list of operations:\n\nAddition\nMultiplication\nSubtraction\nDivision"
  },
  {
    "objectID": "jointermsincategoricalrow.html",
    "href": "jointermsincategoricalrow.html",
    "title": "Join terms in categorical row",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. rows\nSource code: JoinTermsInCategoricalRow.cs"
  },
  {
    "objectID": "jointermsincategoricalrow.html#row",
    "href": "jointermsincategoricalrow.html#row",
    "title": "Join terms in categorical row",
    "section": "3.1 Row",
    "text": "3.1 Row\nSelected categorical row that the filtering should be based on (default: first categorical row in the matrix)."
  },
  {
    "objectID": "jointermsincategoricalrow.html#values",
    "href": "jointermsincategoricalrow.html#values",
    "title": "Join terms in categorical row",
    "section": "3.2 Values",
    "text": "3.2 Values\nSelected values that should be joined to one group (default: no values are selected)."
  },
  {
    "objectID": "jointermsincategoricalrow.html#new-term",
    "href": "jointermsincategoricalrow.html#new-term",
    "title": "Join terms in categorical row",
    "section": "3.3 New term",
    "text": "3.3 New term\nDefines the new value the above joined values should get (default: empty)."
  },
  {
    "objectID": "timeseriesordering.html",
    "href": "timeseriesordering.html",
    "title": "Time series ordering",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Time series\nSource code: not public."
  },
  {
    "objectID": "timeseriesordering.html#periodic",
    "href": "timeseriesordering.html#periodic",
    "title": "Time series ordering",
    "section": "3.1 Periodic",
    "text": "3.1 Periodic\nIf checked, the analyzed time series are periodic (default: unchecked)."
  },
  {
    "objectID": "timeseriesordering.html#in-vivo-quantitative-proteomics-reveals-a-key-contribution-of-post-transcriptional-mechanisms-to-the-circadian-regulation-of-liver-metabolism",
    "href": "timeseriesordering.html#in-vivo-quantitative-proteomics-reveals-a-key-contribution-of-post-transcriptional-mechanisms-to-the-circadian-regulation-of-liver-metabolism",
    "title": "Time series ordering",
    "section": "5.1 In-Vivo Quantitative Proteomics Reveals a Key Contribution of Post-Transcriptional Mechanisms to the Circadian Regulation of Liver Metabolism",
    "text": "5.1 In-Vivo Quantitative Proteomics Reveals a Key Contribution of Post-Transcriptional Mechanisms to the Circadian Regulation of Liver Metabolism\nThe algorithms were first applied in 2014 by Robles et. al.1.\nAbstract\nCircadian clocks are endogenous oscillators that drive the rhythmic expression of a broad array of genes, orchestrating metabolism and physiology. Recent evidence indicates that post-transcriptional and post-translational mechanisms play essential roles in modulating temporal gene expression for proper circadian function, particularly for the molecular mechanism of the clock. Due to technical limitations in large-scale, quantitative protein measurements, it remains unresolved to what extent the circadian clock regulates metabolism by driving rhythms of protein abundance. Therefore, we aimed to identify global circadian oscillations of the proteome in the mouse liver by applying in vivo SILAC mouse technology in combination with state of the art mass spectrometry. Among the 3000 proteins accurately quantified across two consecutive cycles, 6% showed circadian oscillations with a defined phase of expression. Interestingly, daily rhythms of one fifth of the liver proteins were not accompanied by changes at the transcript level. The oscillations of almost half of the cycling proteome were delayed by more than six hours with respect to the corresponding, rhythmic mRNA. Strikingly we observed that the length of the time lag between mRNA and protein cycles varies across the day. Our analysis revealed a high temporal coordination in the abundance of proteins involved in the same metabolic process, such as xenobiotic detoxification. Apart from liver specific metabolic pathways, we identified many other essential cellular processes in which protein levels are under circadian control, for instance vesicle trafficking and protein folding. Our large-scale proteomic analysis reveals thus that circadian post-transcriptional and post-translational mechanisms play a key role in the temporal orchestration of liver metabolism and physiology."
  },
  {
    "objectID": "twodimannotationenrichment.html",
    "href": "twodimannotationenrichment.html",
    "title": "2D annotation enrichment",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. columns\nSource code: not public.\n\n\n\nFor every term in the categorical columns it is tested whether the corresponding expression values have a distribution in two-dimensional planes of expression values that is deviating from the global distribution. For details see Cox and Mann (2012) (Cox and Mann 2012) .\nOutput: The output matrix contains a list of terms from all categorical columns that are significantly biased compared to the global distribution."
  },
  {
    "objectID": "twodimannotationenrichment.html#brief-description",
    "href": "twodimannotationenrichment.html#brief-description",
    "title": "2D annotation enrichment",
    "section": "",
    "text": "For every term in the categorical columns it is tested whether the corresponding expression values have a distribution in two-dimensional planes of expression values that is deviating from the global distribution. For details see Cox and Mann (2012) (Cox and Mann 2012) .\nOutput: The output matrix contains a list of terms from all categorical columns that are significantly biased compared to the global distribution."
  },
  {
    "objectID": "twodimannotationenrichment.html#columns1",
    "href": "twodimannotationenrichment.html#columns1",
    "title": "2D annotation enrichment",
    "section": "2.1 Columns1",
    "text": "2.1 Columns1\nSelected numerical/expression columns that are used as x-axis for the testing of the 2D distributions (default: no expression/numerical columns are selected).\nHint: The selected number of columns in “Columns1” have to be equal to the selected ones in “Columns2”."
  },
  {
    "objectID": "twodimannotationenrichment.html#columns2",
    "href": "twodimannotationenrichment.html#columns2",
    "title": "2D annotation enrichment",
    "section": "2.2 Columns2",
    "text": "2.2 Columns2\nSelected numerical/expression columns that are used as y-axis for the testing of the 2D distributions (default: no expression/numerical columns are selected).\nHint: The selected number of columns in “Columns2” have to be equal to the selected ones in “Columns1”."
  },
  {
    "objectID": "twodimannotationenrichment.html#use-for-truncation",
    "href": "twodimannotationenrichment.html#use-for-truncation",
    "title": "2D annotation enrichment",
    "section": "2.3 Use for truncation",
    "text": "2.3 Use for truncation\nThe truncation can be based on p-values or the Benjamini-Hochberg correction for multiple hypothesis testing (default: Benjamini-Hochberg FDR). Rows with a test result below a specified value (“Threshold value”) are reported as significant."
  },
  {
    "objectID": "twodimannotationenrichment.html#threshold-value",
    "href": "twodimannotationenrichment.html#threshold-value",
    "title": "2D annotation enrichment",
    "section": "2.4 Threshold value",
    "text": "2.4 Threshold value\nBased on a specified threshold (default: 0.02) a row is reported as significant, if its test result is below the defined value. Depending on the chosen truncation score this threshold value is applied to the p-value or to the Benjamini-Hochberg FDR."
  },
  {
    "objectID": "twodimannotationenrichment.html#relative-enrichment",
    "href": "twodimannotationenrichment.html#relative-enrichment",
    "title": "2D annotation enrichment",
    "section": "2.5 Relative enrichment",
    "text": "2.5 Relative enrichment\nSelected text column, where all rows having the same identifier will be counted as one entity in the 2D annotation enrichment analysis (default: &lt;None&gt;). The main application is for post-translational modification sites. Then one should select here protein or gene identifiers.\nThis will make sure that multiple sites from the same protein (or gene) are counted only once for the enrichment analysis."
  },
  {
    "objectID": "twowayanova.html",
    "href": "twowayanova.html",
    "title": "Two-way ANOVA",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Tests\nSource code: not public."
  },
  {
    "objectID": "twowayanova.html#first-grouping",
    "href": "twowayanova.html#first-grouping",
    "title": "Two-way ANOVA",
    "section": "3.1 First Grouping",
    "text": "3.1 First Grouping\nSelected categorical row that defines the first grouping of columns that should be used in the test (default: first categorical row in the matrix)."
  },
  {
    "objectID": "twowayanova.html#second-grouping",
    "href": "twowayanova.html#second-grouping",
    "title": "Two-way ANOVA",
    "section": "3.2 Second Grouping",
    "text": "3.2 Second Grouping\nSelected categorical row that defines the second grouping of columns that should be used in the test (default: first categorical row in the matrix)."
  },
  {
    "objectID": "twowayanova.html#third-grouping",
    "href": "twowayanova.html#third-grouping",
    "title": "Two-way ANOVA",
    "section": "3.3 Third Grouping",
    "text": "3.3 Third Grouping\nSelected categorical row that defines the third grouping of columns that should be used in the test (default: first categorical row in the matrix)."
  },
  {
    "objectID": "twowayanova.html#log10",
    "href": "twowayanova.html#log10",
    "title": "Two-way ANOVA",
    "section": "3.4 -Log10",
    "text": "3.4 -Log10\nIf checked, \\(-Log_{10}(test\\ value)\\) is reported in the output matrix (default). Otherwise the test-value is reported."
  },
  {
    "objectID": "twowayanova.html#suffix",
    "href": "twowayanova.html#suffix",
    "title": "Two-way ANOVA",
    "section": "3.5 Suffix",
    "text": "3.5 Suffix\nThe entered suffix will be attached to newly generated columns (default: empty). That way columns from multiple runs of the test can be distinguished more easily."
  },
  {
    "objectID": "performancecurves.html",
    "href": "performancecurves.html",
    "title": "Performance curves",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: PerformanceCurves.cs"
  },
  {
    "objectID": "performancecurves.html#indicated-are",
    "href": "performancecurves.html#indicated-are",
    "title": "Performance curves",
    "section": "2.1 Indicated are",
    "text": "2.1 Indicated are\nSpecification whether rows containing the “Indicator” in the categorical column specified in “In column” correspond to the class under observation or not (default: False)."
  },
  {
    "objectID": "performancecurves.html#in-column",
    "href": "performancecurves.html#in-column",
    "title": "Performance curves",
    "section": "2.2 In column",
    "text": "2.2 In column\nSelected categorical column containing the class membership of each instance (row) of the class under observation (default: first categorical column in the matrix)."
  },
  {
    "objectID": "performancecurves.html#indicator",
    "href": "performancecurves.html#indicator",
    "title": "Performance curves",
    "section": "2.3 Indicator",
    "text": "2.3 Indicator\nRows containing the defined string are counted as true or false depending on the selection in “Indicated are” (default: \\(+\\))."
  },
  {
    "objectID": "performancecurves.html#scores",
    "href": "performancecurves.html#scores",
    "title": "Performance curves",
    "section": "2.4 Scores",
    "text": "2.4 Scores\nSelected expression columns containing the scores by which the rows are ranked to calculate the specified quantities (default: first expression column of the matrix is selected)."
  },
  {
    "objectID": "performancecurves.html#large-values-are-good",
    "href": "performancecurves.html#large-values-are-good",
    "title": "Performance curves",
    "section": "2.5 Large values are good",
    "text": "2.5 Large values are good\nIf checked the larger the score value the better (default: checked). Otherwise the lower the value the better."
  },
  {
    "objectID": "performancecurves.html#display-quantity",
    "href": "performancecurves.html#display-quantity",
    "title": "Performance curves",
    "section": "2.6 Display quantity",
    "text": "2.6 Display quantity\nSelected quantities that will be calculated (default: no quantities are selected). The quantities can be selected from a predefined list:\n\n\\(TP/(TP+FP)\\) (Precision)\n\\(TP/(TP+FN)\\) (Recall)\n\\(FP/TP\\)\n\\(TP/NP\\)\n\\(TP/(TP+FN)\\) (Sensitivity)\n\\(TN/(TN+FP)\\) (Specificity)"
  },
  {
    "objectID": "softthreshold.html",
    "href": "softthreshold.html",
    "title": "Soft threshold",
    "section": "",
    "text": "This activity is provided through the R-language integration into Perseus and therefore requires R as well as the WGCNA package to be installed. See WGCNA for more information and installation instructions.\nSoft-thresholding is covered by official WGCNA tutorials. Unfortunately, the tutorial is not online anymore.\nBut one possible option is the discussion on Bioconductor.org\n\n1 Description\nFind the best power parameter for co-expression analysis. The goal is to find a network with good scale-free fit.\n\n\n2 Output\nThe output matrix contains properties of co-expression networks constructed for each choice of the power parameter. It is best explored by plotting power against the other properties in a scatter plot. Select the lowest power with a high scale-free fit index SFT.R.sq for the analysis."
  },
  {
    "objectID": "addlinearmotifs.html",
    "href": "addlinearmotifs.html",
    "title": "Add linear motifs",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Modifications\nSource code: not public."
  },
  {
    "objectID": "addlinearmotifs.html#sequence-window",
    "href": "addlinearmotifs.html#sequence-window",
    "title": "Add linear motifs",
    "section": "3.1 Sequence window",
    "text": "3.1 Sequence window\nSelected text column that contains the sequence windows around the sites (default: first text column in the matrix)."
  },
  {
    "objectID": "proteomicruler.html",
    "href": "proteomicruler.html",
    "title": "Proteomic Ruler",
    "section": "",
    "text": "This Perseus plugin implements computational strategies for absolute protein quantification without spike-in standards. The concept was published by Wiśniewski et al.1 and builds on earlier work by Wiśniewski et al.2.\nUsually, determining copy numbers of proteins per cell requires the use of spike-in standards in combination with cell counting and accurate protein concentration measurements.\nIn a nutshell, the idea of the ‘proteomic ruler’ is to use a natural standard instead: Histones. Histones are wrapped around DNA at a defined ratio of ~1:1 (DNA mass/histone mass). The amount of DNA per cell is a function of genome size and ploidy and one usually has good prior knowledge of these factors. Moreover, the total protein concentration is remarkably similar in most cell types, around 200-300 g/l. One can use this to estimate not only copy numbers per cell, but also absolute protein concentrations.\n\n\n1 Requirements\nIf you want to use the proteomic ruler plugin on your dataset, you will need a deep, eukaryotic, whole proteome dataset from a nucleated cell type with known ploidy. It is important that you lysed the cells completely and that no cellular fraction was lost during sample preparation. In particular, the chromatin fraction must be included. Reasonably deep proteome coverage is required. From a depth 12,000+ peptides on, we found that the ratio of histone intensity to total intensity flattens out, assuring overall scaling accuracy. However, deeper coverage will be needed to obtain accurate copy numbers of most individual proteins.\nIf you fractionated your sample, make sure that your fractionation method does not underrepresent histones. We successfully tested SAX, SCX and in-gel fractionation, whereas isoelectric focusing (OFF-gel) fractions tends to underrepresent basic histone peptides, leading to a scaling bias.\nFor technical reasons, this plugin requires that you searched your data against a recent Uniprot fasta file. The plugin currently supports H. sapiens, M. musculus, D. melanogaster, C. elegans, S. cerevisiae and S. pombe.\n\n\n2 Installation\nAs with other Perseus plugins, installation is really easy:\n\nDownload the plugin .dll that is compatible with your Perseus version.\nCopy the .dll into the main Perseus folder.\nRestart Perseus.exe.\nYou should then see a new menu item ‘Proteomic ruler’ in the processing section.\n\n\n\n3 Usage\nImport the proteinGroups.txt of your dataset into Perseus. Make sure to import the following columns:\n\nIntensity (or LFQ intensity) columns as expression columns\nPeptides, unique, razor + unique peptides as numerical columns\nProtein IDs, Majority protein IDs as text columns\nSequence length, Molecular weight as numerical columns (optional)\n\nThe proteomic ruler plugin comes with three functions that should be executed in this order:\n\nAnnotate proteins\nEstimate copy numbers and concentrations\nEstimate absolute protein quantification accuracy\n\n\n\n\n\n\nReferences\n\n1. Wiśniewski, J. R., Hein, M. Y., Cox, J. & Mann, M. A “Proteomic Ruler” for Protein Copy Number and Concentration Estimation without Spike-in Standards. Molecular & Cellular Proteomics 13, 3497–3506 (2014).\n\n\n2. Wiśniewski, J. R. et al. Extensive quantitative remodeling of the proteome between normal colon tissue and adenocarcinoma. Molecular Systems Biology 8, (2012)."
  },
  {
    "objectID": "addregulatorysites.html",
    "href": "addregulatorysites.html",
    "title": "Add regulatory sites",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Modifications\nSource code: AddRegulatorySites.cs"
  },
  {
    "objectID": "addregulatorysites.html#uniprot-column",
    "href": "addregulatorysites.html#uniprot-column",
    "title": "Add regulatory sites",
    "section": "3.1 Uniprot column",
    "text": "3.1 Uniprot column\nSelected text column that contains the UniProt identifiers (default: first text column in the matrix)."
  },
  {
    "objectID": "addregulatorysites.html#sequence-window",
    "href": "addregulatorysites.html#sequence-window",
    "title": "Add regulatory sites",
    "section": "3.2 Sequence window",
    "text": "3.2 Sequence window\nSelected text column that contains the sequence windows around the sites (default: first text column in the matrix)."
  },
  {
    "objectID": "correlate.html",
    "href": "correlate.html",
    "title": "Correlate",
    "section": "",
    "text": "Correlate annotation rows with the main columns of a matrix using the provided correlation function."
  },
  {
    "objectID": "correlate.html#combine-with",
    "href": "correlate.html#combine-with",
    "title": "Correlate",
    "section": "2.1 Combine with",
    "text": "2.1 Combine with\n\nAverage categories\nCo-expression clustering"
  },
  {
    "objectID": "genericclusteringproc.html",
    "href": "genericclusteringproc.html",
    "title": "Generic clustering",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Clustering\nSource code: not public."
  },
  {
    "objectID": "genericclusteringproc.html#method",
    "href": "genericclusteringproc.html#method",
    "title": "Generic clustering",
    "section": "3.1 Method",
    "text": "3.1 Method\nSelected method that should be applied to cluster the data (default: K-means). The result will be displayed in newly generated categorical column named “Clusters”."
  },
  {
    "objectID": "genericclusteringproc.html#number-of-clusters",
    "href": "genericclusteringproc.html#number-of-clusters",
    "title": "Generic clustering",
    "section": "3.2 Number of clusters",
    "text": "3.2 Number of clusters\nDefines the number of clusters the data should be clustered into (default: 10)."
  },
  {
    "objectID": "filtervalidvaluescolumns.html",
    "href": "filtervalidvaluescolumns.html",
    "title": "Filter columns based on valid values",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Filter columns\nSource code: FilterValidValuesColumns.cs"
  },
  {
    "objectID": "filtervalidvaluescolumns.html#min.-number-of-values",
    "href": "filtervalidvaluescolumns.html#min.-number-of-values",
    "title": "Filter columns based on valid values",
    "section": "3.1 Min. number of values",
    "text": "3.1 Min. number of values\nSpecifies minimal number of valid values each expression column needs to contain to survive the filtering process (default: 3)."
  },
  {
    "objectID": "filtervalidvaluescolumns.html#mode",
    "href": "filtervalidvaluescolumns.html#mode",
    "title": "Filter columns based on valid values",
    "section": "3.2 Mode",
    "text": "3.2 Mode\nDefines, which expression columns are counted to define, if a row has enough valid values to survive the filtering process (default: In total)."
  },
  {
    "objectID": "filtervalidvaluescolumns.html#values-should-be",
    "href": "filtervalidvaluescolumns.html#values-should-be",
    "title": "Filter columns based on valid values",
    "section": "3.3 Values should be",
    "text": "3.3 Values should be\nDefines the restriction for a value to be classified as valid (default: Valid). There are seven different possibilities to specify, which entry is counted as a valid value (default: Valid):\n\nValid\nGreater than\nGreater or equal\nLess than\nLess or equal\nBetween\nOutside\n\n\n3.3.1 Minimum\nThis parameter is just relevant, if “Values should be” is set to “Greater than”, “Greater or equal”, “Between” or “Outside”. It defines a lower bound to apply the operation specified in “Values should be” (default: 0).\n\n\n3.3.2 Maximum\nThis parameter is just relevant, if “Values should be” is set to “Less than”, “Less or equal”, “Between” or “Outside”. It defines a upper bound to apply the operation specified in “Values should be” (default: 0)."
  },
  {
    "objectID": "filtervalidvaluescolumns.html#filter-mode",
    "href": "filtervalidvaluescolumns.html#filter-mode",
    "title": "Filter columns based on valid values",
    "section": "3.4 Filter mode",
    "text": "3.4 Filter mode\nThe “Filter mode” defines, whether the input matrix will be reduced (“Reduce matrix” = default) or a new categorical column called “Filter” will be generated containing the categories “Keep” and “Discard” (“Filter mode” = “Add categorical column”)."
  },
  {
    "objectID": "binaryupload.html",
    "href": "binaryupload.html",
    "title": "Binary Upload",
    "section": "",
    "text": "Type: - Matrix Upload\nSource code: BinaryUpload.cs"
  },
  {
    "objectID": "binaryupload.html#file",
    "href": "binaryupload.html#file",
    "title": "Binary Upload",
    "section": "3.1 File",
    "text": "3.1 File\nSpecifies the file path of the binary file that should be uploaded (default: empty). It can be specified manually by typing in the path or the file can be browsed by using the “Select” button. All file extensions are supported."
  },
  {
    "objectID": "significanceb.html",
    "href": "significanceb.html",
    "title": "Significance B",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Outliers\nSource code: SignificanceB.cs"
  },
  {
    "objectID": "significanceb.html#ratio-columns",
    "href": "significanceb.html#ratio-columns",
    "title": "Significance B",
    "section": "3.1 Ratio columns",
    "text": "3.1 Ratio columns\nSelected expression columns containing the ratios for which “Significance B” should be calculated (default: no columns are selected).\nHint: The number of selected ratio columns must be the same as the number of selected intensity columns."
  },
  {
    "objectID": "significanceb.html#intensity-columns",
    "href": "significanceb.html#intensity-columns",
    "title": "Significance B",
    "section": "3.2 Intensity columns",
    "text": "3.2 Intensity columns\nSelected expression/numerical columns containing the intensities for which “Significance B” should be calculated (default: no columns are selected).\nHint: The number of selected intensity columns must be the same as the number of selected ratio columns."
  },
  {
    "objectID": "significanceb.html#side",
    "href": "significanceb.html#side",
    "title": "Significance B",
    "section": "3.3 Side",
    "text": "3.3 Side\nTo apply a two-sided test, where the null hypothesis can be rejected regardless of the direction of the effect “both” has to be selected (default). “left” and “right” are the respective one-sided tests.\n\n3.3.1 Use for truncation\nThe truncation can be based on p-values or the Benjamini-Hochberg correction for multiple hypothesis testing (default: Benjamini-Hochberg FDR). Rows with a test result below a specified value (parameter below) are reported as significant."
  },
  {
    "objectID": "significanceb.html#threshold-value",
    "href": "significanceb.html#threshold-value",
    "title": "Significance B",
    "section": "3.4 Threshold value",
    "text": "3.4 Threshold value\nBased on a specified threshold a specific row is reported as significant (default: 0.05). Depending on the chosen truncation score this threshold value is applied to the p-value or to the Benjamini-Hochberg FDR."
  },
  {
    "objectID": "perseus_usecases.html",
    "href": "perseus_usecases.html",
    "title": "Persues use cases",
    "section": "",
    "text": "1 Use cases\nThis section provides some cases to demonstrate how to work on Perseus for typical workflows.\n\nLabel-free interaction data\nLabel-free phospho data\nSILAC data"
  },
  {
    "objectID": "developersguide.html",
    "href": "developersguide.html",
    "title": "Plugin Developers’s Guide",
    "section": "",
    "text": "1 Repository\nThe code of the plugins is maintained at the perseus-plugins repository\nIf you do not need to examine or contribute to the source code, a more convenient portal with basic information about plugin development can be found at our plug-In store, where the software can be downloaded after registration.\nThese will unpack to a folder named JurgenCox-perseus-plugins-6e9f957 (or similar). The c-sharp source code implementing the standard activities (i.e. plugins) are found in the folders under PerseusPluginLib.\n\n\n2 API\nIf you wish to program your own plugin, the api you need to program against is found under PerseusApi.Matrix. An activity/plugin must implement (currently) one of the five matrix activity interfaces, IMatrixAnalysis, IMatrixExport, IMatrixMultiProcessing, IMatrixProcessing, and IMatrixUpload. All of these are derived from IMatrixActivity, which is nothing more than a marker interface indicating activities that operate on IMatrixData. IActivity is a simple interface derived from INamedListItem with only three methods:\n\nHasButton { get; }\nint GetMaxThreads(Parameters parameters)\nstring Url { get; }\n\nIMatrixData : IData\nIMatrixActivity : IActivity\n\nIMatrixAnalysis : IMatrixActivity, IAnalysis\nIMatrixExport : IMatrixActivity, IExport\nIMatrixMultiProcessing : IMatrixActivity, IMultiProcessing\nIMatrixProcessing : IMatrixActivity, IProcessing\nIMatrixUpload : IMatrixActivity, IUpload\n\n\n\nIn contrast to the simplicity of the activity interfaces, the IMatrixData interface is very rich. Its documentation text reads,\n\nThe data structure representing an augmented data matrix which is the main data object that is flowing through the Perseus workflow. Note that plugin programmers are not supposed to write implementations of //IMatrixData//. The interface only serves to encapsulate the complexity of the implementation for the purpose of plugin programming.\nThe IMatrixData interface specifies 5 signatures of the void SetData method. Most of the parameters are the same in each signature. The ones which are not are written in bold.\n\n\n\n\n\n\n\n\nstring name\n\n\n\n\n\nList&lt;string&gt; expressionColumnNames\nfloat[,] expressionValues\n\n\nList&lt;string&gt; stringColumnNames\nList&lt;string[]&gt; stringColumns\n\n\nList&lt;string&gt; categoryColumnNames\nList&lt;string[][]&gt; categoryColumns\n\n\nList&lt;string&gt; numericColumnNames\nList&lt;double[]&gt; numericColumns\n\n\nList&lt;string&gt; multiNumericColumnNames\nList&lt;double[][]&gt; multiNumericColumns\n\n\n\n\n\n\n\n\n\n\nstring name\n\n\n\n\n\nList&lt;string&gt; expressionColumnNames\nfloat[,] expressionValues\n\n\nbool[,]isImputed\n\n\n\nList&lt;string&gt; stringColumnNames\nList&lt;string[]&gt; stringColumns\n\n\nList&lt;string&gt; categoryColumnNames\nList&lt;string[][]&gt; categoryColumns\n\n\nList&lt;string&gt; numericColumnNames\nList&lt;double[]&gt; numericColumns\n\n\nList&lt;string&gt; multiNumericColumnNames\nList&lt;double[][]&gt; multiNumericColumns\n\n\n\n\n\n\n\n\n\n\nstring name\n\n\n\n\n\nList&lt;string&gt; expressionColumnNames\nfloat[,] expressionValues\n\n\nList&lt;string&gt; stringColumnNames\nList&lt;string[]&gt; stringColumns\n\n\nList&lt;string&gt; categoryColumnNames\nList&lt;string[][]&gt; categoryColumns\n\n\nList&lt;string&gt; numericColumnNames\nList&lt;double[]&gt; numericColumns\n\n\nList&lt;string&gt; multiNumericColumnNames\nList&lt;double[][]&gt; multiNumericColumns\n\n\nList&lt;string&gt; categoryRowNames\nList&lt;string[][]&gt; categoryRows\n\n\nList&lt;string&gt; numericRowNames\nList&lt;double[]&gt; numericRows\n\n\n\n\n\n\n\n\n\n\nstring name\n\n\n\n\n\nList&lt;string&gt; expressionColumnNames\nfloat[,] expressionValues\n\n\nbool[,] isImputed\n\n\n\nList&lt;string&gt; stringColumnNames\nList&lt;string[]&gt; stringColumns\n\n\nList&lt;string&gt; categoryColumnNames\nList&lt;string[][]&gt; categoryColumns\n\n\nList&lt;string&gt; numericColumnNames\nList&lt;double[]&gt; numericColumns\n\n\nList&lt;string&gt; multiNumericColumnNames\nList&lt;double[][]&gt; multiNumericColumns\n\n\nList&lt;string&gt; categoryRowNames\nList&lt;string[][]&gt; categoryRows\n\n\nList&lt;string&gt; numericRowNames\nList&lt;double[]&gt; numericRows\n\n\n\n\n\n\n\n\n\n\n\nstring name\nstring description\n\n\n\n\n\nList&lt;string&gt; expressionColumnNames\nList&lt;string&gt; expressionColumnDescriptions\nfloat[,] expressionValues\n\n\nbool[,] isImputed\n\n\n\n\nfloat[,] qualityValues\nstring qualityName\nbool qualityBiggerIsBetter\n\n\nList&lt;string&gt; stringColumnNames\nList&lt;string&gt; stringColumnDescriptions\nList&lt;string[]&gt; stringColumns\n\n\nList&lt;string&gt; categoryColumnNames\nList&lt;string&gt; categoryColumnDescriptions | List&lt;string[][]&gt; categoryColumns\n\n\n\nList&lt;string&gt; numericColumnNames\nList&lt;string&gt; numericColumnDescriptions\nList&lt;double[]&gt; numericColumns\n\n\nList&lt;string&gt; multiNumericColumnNames\nList&lt;string&gt; multiNumericColumnDescriptions\nList&lt;double[][]&gt; multiNumericColumns\n\n\nList&lt;string&gt; categoryRowNames\nList&lt;string&gt; categoryRowDescriptions\nList&lt;string[][]&gt; categoryRows\n\n\nList&lt;string&gt; numericRowNames\n`List&lt;string&gt; numericRowDescriptions\nList&lt;double[]&gt; numericRows\n\n\n\nThe pattern is that a data matrix, in its simplest form, can have 5 types of columns: expression, string, category, numeric, and multinumeric. The List type of the column names, and, except for expressions, of the values, reflects the fact that there may be multiple columns of any given type. Since for any given column, each row has its own value, the values are lists (col) of arrays (row). Two value types, categories and multiNumeric, are lists of 2d arrays, because any given element (col,row) may have multiple categories or numbers. The 2d array nature of isImputed mirrors that of expressionValues because each entry for an expression has either been imputed or not."
  },
  {
    "objectID": "subtract.html",
    "href": "subtract.html",
    "title": "Subtract",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: Subtract.cs"
  },
  {
    "objectID": "subtract.html#matrix-access",
    "href": "subtract.html#matrix-access",
    "title": "Subtract",
    "section": "3.1 Matrix access",
    "text": "3.1 Matrix access\nSpecifies whether the subtraction is performed on rows or columns (default: Rows). If the subtraction is performed on rows, the “Grouping” can be specified as well (default: ).\n\n3.1.1 Grouping\nThis parameter is just relevant, if the “Matrix access” is set to “Rows”. It specifies, whether the normalization should be applied separately on groups (default: )."
  },
  {
    "objectID": "subtract.html#subtract-what",
    "href": "subtract.html#subtract-what",
    "title": "Subtract",
    "section": "3.2 Subtract what",
    "text": "3.2 Subtract what\nDefines what value should be subtracted from all entries in expression columns (default: Median). The subtrahend can be selected from a predefined list:\n\nMean\nMedian\nMost frequent value\nTukey’s biweight"
  },
  {
    "objectID": "averagecategories.html",
    "href": "averagecategories.html",
    "title": "Average categories",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Annot. columns\nSource code: AverageCategories.cs"
  },
  {
    "objectID": "averagecategories.html#categories",
    "href": "averagecategories.html#categories",
    "title": "Average categories",
    "section": "3.1 Categories",
    "text": "3.1 Categories\nSelected categorical columns that are used to specify, which rows should be summarized (default: all categorical columns are selected)."
  },
  {
    "objectID": "averagecategories.html#average-type",
    "href": "averagecategories.html#average-type",
    "title": "Average categories",
    "section": "3.2 Average type",
    "text": "3.2 Average type\nSelected operation that should be used to average the values of the expression columns (default: median). The operation can be selected from a predefined list of operations:\n\nMedian\nMean\nSum\nStandard deviation"
  },
  {
    "objectID": "averagecategories.html#min.-size",
    "href": "averagecategories.html#min.-size",
    "title": "Average categories",
    "section": "3.3 Min. size",
    "text": "3.3 Min. size\nThe minimal occurrence of a certain term within a categorical column so that the rows will be summarized into one row (default: 3)."
  },
  {
    "objectID": "perseus_activities.html",
    "href": "perseus_activities.html",
    "title": "Activities",
    "section": "",
    "text": "Data processing in Perseus generally takes the form of loading a matrix of data and then applying a series of transformations to that matrix. These transformations are called “activities”. They are organized into a series of “Types”, and possibly a number of “Headings” under the Type.\nYou can search this page for the name of the activity of interest, or you can browse the Type or Heading of interest using the Table of Contents on the right."
  },
  {
    "objectID": "perseus_activities.html#clusteringpca",
    "href": "perseus_activities.html#clusteringpca",
    "title": "Activities",
    "section": "3.1 Clustering/PCA",
    "text": "3.1 Clustering/PCA\n\nHierarchical clustering\nPrincipal component analysis\nCo-expression clustering"
  },
  {
    "objectID": "perseus_activities.html#misc.-analysis",
    "href": "perseus_activities.html#misc.-analysis",
    "title": "Activities",
    "section": "3.2 Misc. (Analysis)",
    "text": "3.2 Misc. (Analysis)\n\nNumeric venn diagram\nSelect rows manually\nSequence logos\nVolcano plot\nHawaii plot"
  },
  {
    "objectID": "perseus_activities.html#visualization",
    "href": "perseus_activities.html#visualization",
    "title": "Activities",
    "section": "3.3 Visualization",
    "text": "3.3 Visualization\n\n3D plot\nHistogram\nMulti scatter plot\nProfile plot\nScatter plot"
  },
  {
    "objectID": "perseus_activities.html#basic-multiprocessing",
    "href": "perseus_activities.html#basic-multiprocessing",
    "title": "Activities",
    "section": "4.1 Basic (MultiProcessing)",
    "text": "4.1 Basic (MultiProcessing)\n\nMatching columns by name\nMatching rows by name\nReplace strings\nRow correlation"
  },
  {
    "objectID": "perseus_activities.html#ci",
    "href": "perseus_activities.html#ci",
    "title": "Activities",
    "section": "4.2 CI",
    "text": "4.2 CI\n\nAssert matrix equals"
  },
  {
    "objectID": "perseus_activities.html#annot.-columns",
    "href": "perseus_activities.html#annot.-columns",
    "title": "Activities",
    "section": "5.1 Annot. columns",
    "text": "5.1 Annot. columns\n\n1D annotation enrichment\n2D annotation enrichment\nAdd annotation\nAnnotation matrix\nAverage categories\nCategory counting\nFisher exact test\nTo base identifiers"
  },
  {
    "objectID": "perseus_activities.html#annot.-rows",
    "href": "perseus_activities.html#annot.-rows",
    "title": "Activities",
    "section": "5.2 Annot. rows",
    "text": "5.2 Annot. rows\n\nAverage groups\nCategorical annotation rows\nJoin terms in categorical row\nNumerical annotation rows"
  },
  {
    "objectID": "perseus_activities.html#basic-processing",
    "href": "perseus_activities.html#basic-processing",
    "title": "Activities",
    "section": "5.3 Basic (Processing)",
    "text": "5.3 Basic (Processing)\n\nAdd noise\nClone\nColumn correlation\nCombine main columns\nCombine rows by identifiers\nDensity estimation\nPerformance curves\nQuantiles\nRow correlation\nSummary statistics (columns)\nSummary statistics (rows)\nTransform"
  },
  {
    "objectID": "perseus_activities.html#clustering",
    "href": "perseus_activities.html#clustering",
    "title": "Activities",
    "section": "5.4 Clustering",
    "text": "5.4 Clustering\n\nGeneric clustering"
  },
  {
    "objectID": "perseus_activities.html#filter-columns",
    "href": "perseus_activities.html#filter-columns",
    "title": "Activities",
    "section": "5.5 Filter columns",
    "text": "5.5 Filter columns\n\nFilter columns based on categorical row\nFilter columns based on valid values"
  },
  {
    "objectID": "perseus_activities.html#filter-rows",
    "href": "perseus_activities.html#filter-rows",
    "title": "Activities",
    "section": "5.6 Filter rows",
    "text": "5.6 Filter rows\n\nFilter rows based on categorical column\nFilter rows based on numerical/main column\nFilter rows based on random sampling\nFilter rows based on text column\nFilter rows based on valid values"
  },
  {
    "objectID": "perseus_activities.html#imputation",
    "href": "perseus_activities.html#imputation",
    "title": "Activities",
    "section": "5.7 Imputation",
    "text": "5.7 Imputation\n\nReplace missing values by constant\nReplace missing\nReplace imputed values by NaN"
  },
  {
    "objectID": "perseus_activities.html#learning",
    "href": "perseus_activities.html#learning",
    "title": "Activities",
    "section": "5.8 Learning",
    "text": "5.8 Learning\n\nClassification (cross-validation and prediction\nClassification feature optimization\nClassification parameter optimization"
  },
  {
    "objectID": "perseus_activities.html#modifications",
    "href": "perseus_activities.html#modifications",
    "title": "Activities",
    "section": "5.9 Modifications",
    "text": "5.9 Modifications\n\nAdd known sites\nAdd linear motifs\nAdd modification counts\nAdd regulatory sites\nAdd sequence features\nExpand site table\nKinase-substrate relations\nShorten motif length"
  },
  {
    "objectID": "perseus_activities.html#normalization",
    "href": "perseus_activities.html#normalization",
    "title": "Activities",
    "section": "5.10 Normalization",
    "text": "5.10 Normalization\n\nCluster normalization\nDivide\nModify by column\nRank\nScale to interval\nSubtract\nSubtract row cluster\nUn-Z-score\nUnit vectors\nWidth adjustment\nZ-score"
  },
  {
    "objectID": "perseus_activities.html#outliers",
    "href": "perseus_activities.html#outliers",
    "title": "Activities",
    "section": "5.11 Outliers",
    "text": "5.11 Outliers\n\nSignificance A\nSignificance B"
  },
  {
    "objectID": "perseus_activities.html#quality",
    "href": "perseus_activities.html#quality",
    "title": "Activities",
    "section": "5.12 Quality",
    "text": "5.12 Quality\n\nConvert to NaN\nCreate quality matrix\nFilter quality"
  },
  {
    "objectID": "perseus_activities.html#rearrange",
    "href": "perseus_activities.html#rearrange",
    "title": "Activities",
    "section": "5.13 Rearrange",
    "text": "5.13 Rearrange\n\nChange column type\nCombine annotations\nCombine categorical columns\nConvert multi-numeric column\nDe-hyphenate ids\nDuplicate columns\nExpand multi-numeric and text columns\nFill categorical columns\nProcess text column\nRemove empty columns\nRename columns\nRename columns [reg. ex.]\nReorder columns by numerical annotation row\nReorder/remove annotation rows\nremove columns\nSearch text column\nSort by column\nTranspose\nUnique rows\nUnique values"
  },
  {
    "objectID": "perseus_activities.html#tests",
    "href": "perseus_activities.html#tests",
    "title": "Activities",
    "section": "5.14 Tests",
    "text": "5.14 Tests\n\nMultiple-sample tests\nOne-sample tests\nPost hoc tests\nThree-way ANOVA\nTwo-sample tests\nTwo-way ANOVA"
  },
  {
    "objectID": "perseus_activities.html#time-series",
    "href": "perseus_activities.html#time-series",
    "title": "Activities",
    "section": "5.15 Time series",
    "text": "5.15 Time series\n\nCyclic annotation enrichment\nPeriodicity analysis\nPeriodogram\nTime series ordering"
  },
  {
    "objectID": "scatterplot3danalysis.html",
    "href": "scatterplot3danalysis.html",
    "title": "3D plot",
    "section": "",
    "text": "1 General\n\nType: - Matrix Analysis\nHeading: - Visualization\nSource code: not public.\n\n\n\n2 Brief description\nPlot three columns or three rows against each other as x, y and z values in a scatter plot.\n\n\n\n3 Parameters\n“3D plot” has no parameters."
  },
  {
    "objectID": "modifications.html",
    "href": "modifications.html",
    "title": "Label-free phospho data",
    "section": "",
    "text": "For the here shown use case Perseus 1.5.1.6 was used."
  },
  {
    "objectID": "modifications.html#loading",
    "href": "modifications.html#loading",
    "title": "Label-free phospho data",
    "section": "2.1 Loading",
    "text": "2.1 Loading\nLoad the file “Phospho (STY)Sites.txt” from the “combined/txt” folder of the MaxQuant output. Load → Generic matrix upload is denoted by the green arrow on the top left corner of the Perseus window or load the file using the drag and drop function of Perseus.\n\nMake sure to load the following 42 columns as main columns:\n\nIntensity C1_1, Intensity C1_2, Intensity C1_3,\nIntensity C2_1, Intensity C2_2, Intensity C2_3,\nIntensity C3_1, Intensity C3_2, Intensity C3_3,\nIntensity C4_1, Intensity C4_2, Intensity C4_3,\nIntensity C5_1, Intensity C5_2, Intensity C5_3,\nIntensity C6_1, Intensity C6_2, Intensity C6_3,\nIntensity E15_1_1, Intensity E15_1_2, Intensity E15_1_3,\nIntensity E15_2_1, Intensity E15_2_2, Intensity E15_2_3,\nIntensity E15_3_1, Intensity E15_3_2, Intensity E15_3_3,\nIntensity E15_4_1, Intensity E15_4_2, Intensity E15_4_3,\nIntensity N1_1, Intensity N1_2, Intensity N1_3,\nIntensity N2_1, Intensity N2_2, Intensity N2_3,\nIntensity N3_1, Intensity N3_2, Intensity N3_3,\nIntensity N4_1, Intensity N4_2, Intensity N4_3\n\nAlso make sure that the “Localization prob” is loaded as numerical column. (Only the global one, not one for each sample.)\nComment: After each applied operation a new matrix will be generated. Every matrix that you are creating can be saved with Export → Generic matrix export and re-imported later with Load → Generic matrix upload."
  },
  {
    "objectID": "modifications.html#renaming-columns",
    "href": "modifications.html#renaming-columns",
    "title": "Label-free phospho data",
    "section": "4.1 Renaming columns",
    "text": "4.1 Renaming columns\nNext, we are doing some cosmetics by shortening the names of the main columns and remove the prefix “Intensity” with Processing → Rearrange → Rename columns [reg. ex.].\n\nThe regular expression that you may use to remove the repetitive part of the name is “Intensity (.*)”. The general concept of regular expressions can be found under http://en.wikipedia.org/wiki/Regular_expression. If you already know generally how regular expressions work, you may only need to glance at this Quick Reference or at an even quicker Cheat Sheet. This results in a matrix, where the names of the main columns are the replica.\n\nIf you want to rename the columns manually without the help of regular expressions you can use Processing → Rearrange → Rename columns.\n\nThen you can type the new names in the predefined text field."
  },
  {
    "objectID": "matchingrowsbyname.html",
    "href": "matchingrowsbyname.html",
    "title": "Matching rows by name",
    "section": "",
    "text": "Type: - Matrix MultiProcessing\nHeading: - Basic (MultiProcessing)\nSource code: MatchingRowsByName.cs"
  },
  {
    "objectID": "matchingrowsbyname.html#base-matrix",
    "href": "matchingrowsbyname.html#base-matrix",
    "title": "Matching rows by name",
    "section": "3.1 Base matrix",
    "text": "3.1 Base matrix\nSpecifies the matrix, whose rows are used as basis for the matching of the entries in a defined column. If no matrix is selected, the default value is empty, otherwise the default is the currently selected matrix. The selection can be changed by first clicking on “Base matrix” line of the pop-up window and then clicking on the matrix of choice in the middle pane (see parameter window A)."
  },
  {
    "objectID": "matchingrowsbyname.html#other-matrix",
    "href": "matchingrowsbyname.html#other-matrix",
    "title": "Matching rows by name",
    "section": "3.2 Other matrix",
    "text": "3.2 Other matrix\nSpecifies the matrix, whose rows are matched to the ones of the “Base matrix”. If no matrix is selected, the default value is empty, otherwise the default is the currently selected matrix. The selection can be changed by first clicking on “Other matrix” line of the pop-up window and then clicking on the matrix of choice in the middle pane (see parameter window A)."
  },
  {
    "objectID": "matchingrowsbyname.html#matching-column-1",
    "href": "matchingrowsbyname.html#matching-column-1",
    "title": "Matching rows by name",
    "section": "3.3 Matching column 1",
    "text": "3.3 Matching column 1\nSelected text column of the “Base matrix” that is used for matching (default: first text column in the matrix)."
  },
  {
    "objectID": "matchingrowsbyname.html#matching-column-2",
    "href": "matchingrowsbyname.html#matching-column-2",
    "title": "Matching rows by name",
    "section": "3.4 Matching column 2",
    "text": "3.4 Matching column 2\nSelected text column of the “Other matrix” that is used for matching (default: first text column in the matrix)."
  },
  {
    "objectID": "matchingrowsbyname.html#indicator",
    "href": "matchingrowsbyname.html#indicator",
    "title": "Matching rows by name",
    "section": "3.5 Indicator",
    "text": "3.5 Indicator\nIf checked, a categorical column called as the “Other matrix” will be added (default: unchecked). A “+” in the column indicates that at least one row of the second matrix matches."
  },
  {
    "objectID": "matchingrowsbyname.html#expression-columns",
    "href": "matchingrowsbyname.html#expression-columns",
    "title": "Matching rows by name",
    "section": "3.6 Expression columns",
    "text": "3.6 Expression columns\nSelected expression columns of the “Other matrix” that should be added to the “Base matrix” (default: no expression columns are selected)."
  },
  {
    "objectID": "matchingrowsbyname.html#combine-expression-column-values",
    "href": "matchingrowsbyname.html#combine-expression-column-values",
    "title": "Matching rows by name",
    "section": "3.7 Combine expression column values",
    "text": "3.7 Combine expression column values\nDefines the combination of multiple expression values, in case multiple rows of the “Other matrix” match to a row of the “Base matrix” (default: Median). The combination can be selected from a predefined list of operations:\n\nMedian\nMean\nMinimum\nMaximum\nSum"
  },
  {
    "objectID": "matchingrowsbyname.html#categorical-columns",
    "href": "matchingrowsbyname.html#categorical-columns",
    "title": "Matching rows by name",
    "section": "3.8 Categorical columns",
    "text": "3.8 Categorical columns\nSelected categorical columns of the “Other matrix” that should be added to the “Base matrix” (default: no categorical columns are selected)."
  },
  {
    "objectID": "matchingrowsbyname.html#text-columns",
    "href": "matchingrowsbyname.html#text-columns",
    "title": "Matching rows by name",
    "section": "3.9 Text columns",
    "text": "3.9 Text columns\nSelected text columns of the “Other matrix” that should be added to the “Base matrix” (default: no text columns are selected)."
  },
  {
    "objectID": "matchingrowsbyname.html#numerical-columns",
    "href": "matchingrowsbyname.html#numerical-columns",
    "title": "Matching rows by name",
    "section": "3.10 Numerical columns",
    "text": "3.10 Numerical columns\nSelected numerical columns of the “Other matrix” that should be added to the “Base matrix” (default: no numerical columns are selected)."
  },
  {
    "objectID": "matchingrowsbyname.html#combine-numerical-values",
    "href": "matchingrowsbyname.html#combine-numerical-values",
    "title": "Matching rows by name",
    "section": "3.11 Combine numerical values",
    "text": "3.11 Combine numerical values\nDefines the combination of multiple numerical values, in case multiple rows of the “Other matrix” match to a row of the “Base matrix” (default: Median). The combination can be selected from a predefined list of operations:\n\nMedian\nMean\nMinimum\nMaximum\nSum\nKeep separate"
  },
  {
    "objectID": "rowcorrelations.html",
    "href": "rowcorrelations.html",
    "title": "Row Correlations",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: not public."
  },
  {
    "objectID": "rowcorrelations.html#type",
    "href": "rowcorrelations.html#type",
    "title": "Row Correlations",
    "section": "3.1 Type",
    "text": "3.1 Type\nDefines the measure of correlation that should be calculated between the selected rows (default: Pearson correlation). It can be selected from a list of correlation coefficients:\n\nLog2(Absence-presence enrichment factor)\nAbsence-presence -Log10(p-value)\nNumbers of valid pairs\nValid pairs percentage\nPearson correlation\n-Log10(Pearson p-value)\n-Log10(Pearson p-value) [correlation]\n-Log10(Pearson p-value) [anti-correlation]\nR squared\nSpearman rank correlation\n-Log10(Spearman p-value)\n-Log10(Spearman p-value) [correlation]\n-Log10(Spearman p-value) [anti-correlation]\nKendall rank correlation\nDistance correlation\nMutual information\nEuclidean distance\nManhattan distance\nMaximum distance"
  },
  {
    "objectID": "rowcorrelations.html#select-columns-based-on",
    "href": "rowcorrelations.html#select-columns-based-on",
    "title": "Row Correlations",
    "section": "3.2 Select columns based on",
    "text": "3.2 Select columns based on\nSelected categorical/text column that filters the rows of the input matrix (default: ). If  is chosen, all rows of the original matrix will become columns of the correlation matrix ."
  },
  {
    "objectID": "rowcorrelations.html#column-terms",
    "href": "rowcorrelations.html#column-terms",
    "title": "Row Correlations",
    "section": "3.3 Column terms",
    "text": "3.3 Column terms\nFiltering option to only include rows, which match the specified input term(s) in the selected column in “Select columns based on” (default: empty). Inclusion of multiple terms is possible in the predefined field and have to be separated by new lines. Each row is one term."
  },
  {
    "objectID": "rowcorrelations.html#select-rows-based-on",
    "href": "rowcorrelations.html#select-rows-based-on",
    "title": "Row Correlations",
    "section": "3.4 Select rows based on",
    "text": "3.4 Select rows based on\nSelected categorical/text column that filters the rows of the input matrix (default: ). If  is chosen, all rows of the original matrix will become rows of the correlation matrix ."
  },
  {
    "objectID": "rowcorrelations.html#row-terms",
    "href": "rowcorrelations.html#row-terms",
    "title": "Row Correlations",
    "section": "3.5 Row terms",
    "text": "3.5 Row terms\nFiltering option to only include rows, which match the specified input term(s) in the previously selected column in “Select rows based on” (default: empty). Inclusion of multiple terms is possible in the predefined field and have to be separated by new lines. Each row is one term."
  },
  {
    "objectID": "periodicityanalysis.html",
    "href": "periodicityanalysis.html",
    "title": "Periodicity analysis",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Time series\nSource code: not public."
  },
  {
    "objectID": "periodicityanalysis.html#period",
    "href": "periodicityanalysis.html#period",
    "title": "Periodicity analysis",
    "section": "3.1 Period",
    "text": "3.1 Period\nDefines when a full period is passed (default: 24 according to hours of the day)."
  },
  {
    "objectID": "periodicityanalysis.html#time-column",
    "href": "periodicityanalysis.html#time-column",
    "title": "Periodicity analysis",
    "section": "3.2 Time column",
    "text": "3.2 Time column\nSelected numerical row, which defines the time points (default: first numerical row). Proteins are listed in rows, whereas columns represent a specific point in time."
  },
  {
    "objectID": "periodicityanalysis.html#separate-time-series",
    "href": "periodicityanalysis.html#separate-time-series",
    "title": "Periodicity analysis",
    "section": "3.3 Separate time series",
    "text": "3.3 Separate time series\nDefines whether separate time series analyses should be applied according to a pre-defined grouping, which is specified in a categorical row (default: &lt;None&gt;). It can be selected from a list including all available groupings of the matrix."
  },
  {
    "objectID": "periodicityanalysis.html#sort-by",
    "href": "periodicityanalysis.html#sort-by",
    "title": "Periodicity analysis",
    "section": "3.4 Sort by",
    "text": "3.4 Sort by\nThis parameter is just relevant, if the parameter “Separate time series” is set to a grouping. It specifies, which group will be used for sorting."
  },
  {
    "objectID": "periodicityanalysis.html#criterion",
    "href": "periodicityanalysis.html#criterion",
    "title": "Periodicity analysis",
    "section": "3.5 Criterion",
    "text": "3.5 Criterion\nDefines the function to what the periodic time series are compared to (default: Cosine fit). The function can be selected from a pre-defined list."
  },
  {
    "objectID": "periodicityanalysis.html#fdr",
    "href": "periodicityanalysis.html#fdr",
    "title": "Periodicity analysis",
    "section": "3.6 FDR",
    "text": "3.6 FDR\nRows (proteins) with a test result below this value are reported as significant (default: 0.05)."
  },
  {
    "objectID": "periodicityanalysis.html#number-of-randomizations",
    "href": "periodicityanalysis.html#number-of-randomizations",
    "title": "Periodicity analysis",
    "section": "3.7 Number of randomizations",
    "text": "3.7 Number of randomizations\nSpecifies the number of randomized periods (default: 1000)."
  },
  {
    "objectID": "periodicityanalysis.html#preserve-grouping-in-randomizations",
    "href": "periodicityanalysis.html#preserve-grouping-in-randomizations",
    "title": "Periodicity analysis",
    "section": "3.8 Preserve grouping in randomizations",
    "text": "3.8 Preserve grouping in randomizations\nDefines, whether the grouping specified in a categorical row should be preserved in the randomizations (default: &lt;None&gt;). It can be selected from a list including all available groupings of the matrix."
  },
  {
    "objectID": "periodicityanalysis.html#in-vivo-quantitative-proteomics-reveals-a-key-contribution-of-post-transcriptional-mechanisms-to-the-circadian-regulation-of-liver-metabolism",
    "href": "periodicityanalysis.html#in-vivo-quantitative-proteomics-reveals-a-key-contribution-of-post-transcriptional-mechanisms-to-the-circadian-regulation-of-liver-metabolism",
    "title": "Periodicity analysis",
    "section": "5.1 In-Vivo Quantitative Proteomics Reveals a Key Contribution of Post-Transcriptional Mechanisms to the Circadian Regulation of Liver Metabolism",
    "text": "5.1 In-Vivo Quantitative Proteomics Reveals a Key Contribution of Post-Transcriptional Mechanisms to the Circadian Regulation of Liver Metabolism\nThe algorithms were first applied in 2014 by Robles et. al.1.\nAbstract\nCircadian clocks are endogenous oscillators that drive the rhythmic expression of a broad array of genes, orchestrating metabolism and physiology. Recent evidence indicates that post-transcriptional and post-translational mechanisms play essential roles in modulating temporal gene expression for proper circadian function, particularly for the molecular mechanism of the clock. Due to technical limitations in large-scale, quantitative protein measurements, it remains unresolved to what extent the circadian clock regulates metabolism by driving rhythms of protein abundance. Therefore, we aimed to identify global circadian oscillations of the proteome in the mouse liver by applying in vivo SILAC mouse technology in combination with state of the art mass spectrometry. Among the 3000 proteins accurately quantified across two consecutive cycles, 6% showed circadian oscillations with a defined phase of expression. Interestingly, daily rhythms of one fifth of the liver proteins were not accompanied by changes at the transcript level. The oscillations of almost half of the cycling proteome were delayed by more than six hours with respect to the corresponding, rhythmic mRNA. Strikingly we observed that the length of the time lag between mRNA and protein cycles varies across the day. Our analysis revealed a high temporal coordination in the abundance of proteins involved in the same metabolic process, such as xenobiotic detoxification. Apart from liver specific metabolic pathways, we identified many other essential cellular processes in which protein levels are under circadian control, for instance vesicle trafficking and protein folding. Our large-scale proteomic analysis reveals thus that circadian post-transcriptional and post-translational mechanisms play a key role in the temporal orchestration of liver metabolism and physiology."
  },
  {
    "objectID": "creategenelist.html",
    "href": "creategenelist.html",
    "title": "Create Gene list",
    "section": "",
    "text": "Type: - Matrix Upload\nSource code: CreateGeneList.cs\n\n===== Brief description =====\nStart with a list of all protein-coding genes from an organism."
  },
  {
    "objectID": "creategenelist.html#organism",
    "href": "creategenelist.html#organism",
    "title": "Create Gene list",
    "section": "2.1 Organism",
    "text": "2.1 Organism\nSpecification of the organism for which a gene list should be created (default: first file in \\conf\\perseus\\genelists). The organisms or the corresponding text files that should be uploaded can be selected from all files, which are located in \\conf\\perseus\\genelists in your Perseus folder."
  },
  {
    "objectID": "creategenelist.html#parameter-window",
    "href": "creategenelist.html#parameter-window",
    "title": "Create Gene list",
    "section": "2.2 Parameter window",
    "text": "2.2 Parameter window\n\n\n\nCreate Gene list windows"
  },
  {
    "objectID": "unzscore.html",
    "href": "unzscore.html",
    "title": "Un Z-Score",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Normalization\nSource code: UnZScore.cs"
  },
  {
    "objectID": "unzscore.html#matrix-access",
    "href": "unzscore.html#matrix-access",
    "title": "Un Z-Score",
    "section": "3.1 Matrix access",
    "text": "3.1 Matrix access\nSpecifies whether the un-z-scoring is performed on the rows or columns (default: Rows). The selection depends on what was used before in the normalization process."
  },
  {
    "objectID": "unzscore.html#mean",
    "href": "unzscore.html#mean",
    "title": "Un Z-Score",
    "section": "3.2 Mean",
    "text": "3.2 Mean\nSelected numerical row/column containing the mean values that were used for the z-scoring (default: column called “Mean” or first numerical column in the matrix)."
  },
  {
    "objectID": "unzscore.html#std.-dev.",
    "href": "unzscore.html#std.-dev.",
    "title": "Un Z-Score",
    "section": "3.3 Std. dev.",
    "text": "3.3 Std. dev.\nSelected numerical row/column containing the standard deviation values that were used for the z-scoring (default: column called “Std.dev.” or first numerical column in the matrix)."
  },
  {
    "objectID": "interactions.html",
    "href": "interactions.html",
    "title": "Label-free interaction data",
    "section": "",
    "text": "For the here shown use case Perseus 1.5.1.6 was used."
  },
  {
    "objectID": "interactions.html#loading",
    "href": "interactions.html#loading",
    "title": "Label-free interaction data",
    "section": "2.1 Loading",
    "text": "2.1 Loading\nLoad the file “proteinGroups.txt” from the “combined/txt” folder of the MaxQuant output. Load Generic matrix upload is denoted by the green arrow on the top left corner of the Perseus window or load the file using the drag and drop function of Perseus.\n\nMake sure to load the six LFQ intensities as main columns, which correspond to three repeats of a pulldown and three controls."
  },
  {
    "objectID": "interactions.html#filtering",
    "href": "interactions.html#filtering",
    "title": "Label-free interaction data",
    "section": "2.2 Filtering",
    "text": "2.2 Filtering\nAfter loading the matrix, we filter out the reverse proteins and the proteins that are only identified by site. The contaminants are deliberately not filtered out to show some things later. The filtering is done using the function Processing → Filter rows → Filter rows based on categorical column, because both the “Reverse” column and the “Only identified by site” column are categorical.\n\nFirst, we filter out the reverse hits. Reverse hits are indicated by a “+” in the “Reverse” column, so to filter out these hits all rows containing a “+” will be removed from the matrix. To do so the column “Reverse” needs to be selected, “+” is the value we are looking for and is selected by default. No further changes are necessary, because we want to remove the matching rows from the matrix. This results in a matrix, where the value in the “Reverse” column of all rows is empty.\n\nSecond, we filter out the hits that are only identified by site. These are indicated by a “+” in the “Only identified by site” column, so to filter out these hits all rows containing a “+” will be removed from the matrix. To do so the column “Only identified by site” needs to be selected, “+” is the value we are looking for and is selected by default. No further changes are necessary, because we want to remove the matching rows from the matrix. This results in a matrix, where the value in the “Only identified by site” column of all rows are empty."
  },
  {
    "objectID": "interactions.html#transformation",
    "href": "interactions.html#transformation",
    "title": "Label-free interaction data",
    "section": "2.3 Transformation",
    "text": "2.3 Transformation\nTo gain better behavior of the data in statistical tests and also in imputation, normally distributed data is advantageous. Therefore we logarithmize the data using Processing → Basic → Transform.\n\nSince the default formula is \\(log_2(x)\\) and all expression columns are selected, nothing needs to be changed.\n\nTo verify, if the data is now more or less normally distributed, we plot histograms of each intensity column separately with Analysis → Visualization → Histogram.\n\n\nThe results will be displayed in an extra tab on the same matrix containing the histogram functionalities. And indeed the intensities are each more or less normally distributed."
  },
  {
    "objectID": "interactions.html#grouping-of-samples",
    "href": "interactions.html#grouping-of-samples",
    "title": "Label-free interaction data",
    "section": "2.4 Grouping of samples",
    "text": "2.4 Grouping of samples\nBefore filtering the data for valid values, we group the samples according to replicates using Processing → Annot. rows → Categorical annotation rows.\n\nYou can leave the name of the grouping to the default value “Group1” and just shorten the rest of the column names according to replicates. Also giving each group a different name is possible, this is a matter of taste."
  },
  {
    "objectID": "interactions.html#filtering-according-to-groups",
    "href": "interactions.html#filtering-according-to-groups",
    "title": "Label-free interaction data",
    "section": "2.5 Filtering according to groups",
    "text": "2.5 Filtering according to groups\nIdentifications with just one reported intensity are usually not very useful for further analyses. This is why we filter for valid values with Processing → Filter rows → Filter rows based on valid values.\n\nWe want to have “3” valid values “in at least one group”, because interaction partners without any background affinity would not appear in the control, but should still appear with three valid values in the actual pulldown."
  },
  {
    "objectID": "interactions.html#imputation",
    "href": "interactions.html#imputation",
    "title": "Label-free interaction data",
    "section": "2.6 Imputation",
    "text": "2.6 Imputation\nThen we impute missing values from normal distribution with Processing → Imputation → Replace missing values from normal distribution.\n\nThe imputation function will look at the distribution of values, assuming that they are normally distributed and calculates width and center of the distribution. Then Perseus will shrink the distributions to a factor of “0.3” (width), shift it down by “1.8” (down shift) standard deviations and simulate some random values that make up values to fill up the missing values. Also we take the whole matrix (mode).\n\nUsing histograms as before Analysis → Visualization → Histogram, we are able to look at the distributions of the imputed values. Therefore we generate histograms for all LFQ intensity columns. The results will be displayed in an extra tab on the same matrix containing the histogram functionalities as before. It is important that the imputed values do not form a separate normal distribution, start around the same point in the replicates and are narrower than the distributions of the measured values."
  },
  {
    "objectID": "interactions.html#multi-scatter-plot",
    "href": "interactions.html#multi-scatter-plot",
    "title": "Label-free interaction data",
    "section": "3.1 Multi-scatter plot",
    "text": "3.1 Multi-scatter plot\nTo be sure, that the experiment worked one can use multi-scatter plots with Analysis → Visualization → Multi scatter plot to analyze the correlation between the samples.\n\nTherefore we have to plot all six samples against each other.\n\nWe can observe, that the Pearson correlation (calculation option highlighted by a red rectangle) is good between the samples. Also that the correlation within the groups is a bit higher than between the groups of replicates."
  },
  {
    "objectID": "interactions.html#hierarchical-clustering",
    "href": "interactions.html#hierarchical-clustering",
    "title": "Label-free interaction data",
    "section": "3.2 Hierarchical clustering",
    "text": "3.2 Hierarchical clustering\nAnother option to figure out, if the experiment worked is hierarchical clustering in Analysis → Clustering/PCA → Hierarchical clustering.\n\nSince we just have six known samples, we are not interested in the column tree. So we are more curious about the intensity profile of all the samples. The results will be displayed in an extra tab on the same matrix containing the hierarchical clustering functionalities.\n\nThe default color gradient is from green (low values) to red (high values), which is very nice for ratios, but for intensities another color gradient could be used, which is called “rainbow gradient”. It goes from dark cold colors to bright warm colors. To change it click on the “change color gradient” button in the “Clustering” tab (highlighted by a red rectangle) and the current gradient will be displayed in a pop-up window.\n\nTo change the colors just click on the position in the gradient you want to change and select a color in the “Color” window that pops up until you have the gradient you desire.\n\nAt first glance all the samples look more or less the same, which is good. But, if we take a closer look at the top of the heatmap, we can find a group of proteins that show distinct behavior between the groups. To figure out which proteins these are we have to zoom in by holding the left mouse key and dragging. And what we can see is that the highlighted cluster of proteins belong mainly to anaphase promoting complex.\n\nNow we can investigate the behavior of the contaminants, since we deliberately did not filter them in the beginning. To highlight them we have to click on the “Configure row names” button in the “Clustering” tab\n\nand change “Row color bar” to “Contaminant”.\n\nThis results in an additional column on the left hand side of the heatmap, where contaminants are highlighted in blue (the rest is grey). These contaminants do not behave like a contaminant and are thus potential interaction partners."
  },
  {
    "objectID": "interactions.html#two-sample-t-test",
    "href": "interactions.html#two-sample-t-test",
    "title": "Label-free interaction data",
    "section": "4.1 Two sample t-test",
    "text": "4.1 Two sample t-test\nNow we want to find the interactors using Processing → Tests → Two-samples tests.\n\nThe T-test assumes that you have equal variance within the groups of replicates. s0 is in essence a minimal fold change, which means even if a protein gives you a very good p-value in case the fold change is below that value (s0) it won’t be significant. As a rule of thumb we use s0 = 2 and FDR = 0.01, what usually gives nice results. This results in a new matrix with additional columns: “t-test Significant”, “-Log t-test p-value”, “t-test Difference” and “test statistic”."
  },
  {
    "objectID": "interactions.html#scatter-plot",
    "href": "interactions.html#scatter-plot",
    "title": "Label-free interaction data",
    "section": "4.2 Scatter plot",
    "text": "4.2 Scatter plot\nNow to view the interactors we have to generate a scatter plot Analysis → Visualization → Scatter plot.\n\nSo we have to plot the columns against each other. This results in an extra tab on the same matrix containing the scatter plot functionalities.\n\nWe are interested in the “t-test Difference” vs. “-Log t-test p-value” (highlighted by a blue rectangle). Then select “t-test Significant” in the “Categories” tab and show the “Gene Names” of these hits (highlighted by green rectangles). Since we used “both” sides in the significance test, we are able to find significant hits on both sides.\n\nIt is important to keep in mind that there is no universal s0 and FDR parameter value that guarantees good results. One has to optimize the values for every experiment. This optimization process is covered in the following section."
  },
  {
    "objectID": "interactions.html#volcano-plot",
    "href": "interactions.html#volcano-plot",
    "title": "Label-free interaction data",
    "section": "4.3 Volcano plot",
    "text": "4.3 Volcano plot\nThe volcano plot is the unified function of the two sample t-test and the scatter plot with the additional option to easily optimize the s0 and FDR parameter. So to identify and visualize the interactors in one step use Analysis → Misc. → Volcano plot.\n\nAs in the two sample t-test one has to specify the grouping that should be used for the significance test, which test should be applied and the parameters of the test. Since we have just one grouping and want to use the t-test we just have to change s0 to 2 and FDR to 0.01. This results in an extra tab on the same matrix containing the volcano plot functionalities.\n\nThe results are shown in a volcano plot with the cutoff curve indicating which hits are significant. To highlight the test significant hits we have to select “t-test Significant” in the “Categories” tab and choose “Gene Names” as labels (highlighted by green rectangles). Also other cutoff parameters, s0 and FDR, can be applied by typing in the new chosen values (blue rectangle). Don’t forget to refresh the plot (red rectangle)."
  },
  {
    "objectID": "interactions.html#profile-plot",
    "href": "interactions.html#profile-plot",
    "title": "Label-free interaction data",
    "section": "4.4 Profile plot",
    "text": "4.4 Profile plot\nAnother option to find and visualize interactors is to use Analysis → Visualization → Profile plot. This function has no input parameters and the results are shown in an extra tab on the same matrix containing the profile plot functionalities.\n\nFirst we have to search for the bait protein CDC23 and get its profile. This is achieved by clicking the right mouse button in the “Profiles” tab (orange rectangle in the figure at the end), selecting “Find…” and searching for “CDC23” in the whole table.\n\nThen go to the “Reference profile” tab (green rectangle) and search for the 20 next neighbors (red rectangle) and press the start button (blue rectangle). If you go now back to the “Profiles plot” (orange rectangle) the selected proteins are at the top of the matrix and are waiting to be investigated."
  },
  {
    "objectID": "estimatecopynumbers.html",
    "href": "estimatecopynumbers.html",
    "title": "Proteomic ruler: Estimate copy numbers and concentrations",
    "section": "",
    "text": "If you arrived here directly, it is a good idea to read the Proteomic ruler overview first."
  },
  {
    "objectID": "estimatecopynumbers.html#input",
    "href": "estimatecopynumbers.html#input",
    "title": "Proteomic ruler: Estimate copy numbers and concentrations",
    "section": "2.1 Input",
    "text": "2.1 Input\n\n2.1.1 Protein IDs\nSelect the column containing your semicolon-separated protein group IDs (uniprot format). It is recommended to use the ‘Majority protein IDs’ when coming from MaxQuant.\n\n\n2.1.2 Intensities\nSelect a number of intensity columns as input for the absolute quantification. Each of them must represent a complete proteome (not individual fractions).\n\nThe default is to use the Intensity xyz columns in the proteinGroups.txt coming from MaxQuant.\nYou can also work with LFQ intensity xyz columns if you intend to compare resulting copy numbers across samples. Here, it is recommended to use ‘1’ as min. LFQ ratio count in MaxQuant and to change the averaging mode (see below).\nDo not use iBAQ values! These values are already normalized for protein size and using them as input will give you wrong results, as size-normalization will be applied again.\n\n\n\n2.1.3 Logarithmized\nSpecify whether the intensities are logarithmized in the selected Intensities columns.\n\n\n2.1.4 Averaging mode\nThis parameter controls how different columns will be handled.\n\nAll columns separately: Each column will be treated independently. Any sample-to-sample normalization will be lost. Resulting copy numbers should only be compared within each sample, not across. This is useful if you want to analyse changes in cell sizes across conditions.\nSame normalization for all columns: The plugin will calculate the total protein mass per cell for all columns and then use the median thereof as scaling factor for all columns. Any sample-to-sample normalization will be retained. Therefore, this setting is useful in when working with LFQ intensities that are normalized for comparability across samples. Note that any changes in cell size across samples will not be represented.\nSame normalization within groups: as described for the previous option, but only within groups of samples.\nAverage all columns: Instead of reporting copy numbers for individual samples, only the median across all samples will be reported. This is useful if you are using a set of replicates as input.\n\n\n\n2.1.5 Molecular masses\nSelect the column containing the molecular masses of the proteins. This can either be the Molecular weight column from the MaxQuant output or one of the columns mapped by the Annotate proteins function.\n\n\n2.1.6 Detectability correction\nIn its simplest form, the plugin will assume linearity between the Intensity values and the cumulative mass of each protein. In other words, the molecular mass of each protein serves as the correction factor if one wants to calculate copy numbers. Alternatively, one can use other detectability correction factors, such as the number of theoretical peptides. In contrast to the molecular mass, this takes the combination of sequence features and the protease into account. Theoretical peptides are not reported by MaxQuant directly, but can be calculated using Annotate proteins.\n\n\n2.1.7 Scaling mode\nThere are two options of how to scale copy numbers globally:\n\nThe total protein amount per cell. In this case the total intensity will be considered proportional to the total protein amount per cell.\nThe histone proteomic ruler. Here, the cumulative histone amount will be considered proportional to the expected DNA amount per cell (calculated from the genome size and the user-defined ploidy.) Note that the organism will be auto-detected from the uniprot IDs.\n\n\n\n2.1.8 Total cellular protein concentration\nSpecify the total cellular protein concentration. This value is remarkably invariant across many cell types (typically between 200 and 300 g/l. If unknown, use the default value. This parameter is irrelevant for copy numbers, but will be used for concentration estimations."
  },
  {
    "objectID": "estimatecopynumbers.html#output",
    "href": "estimatecopynumbers.html#output",
    "title": "Proteomic ruler: Estimate copy numbers and concentrations",
    "section": "2.2 Output",
    "text": "2.2 Output\nYou can select between different output columns and matrices:\n\nCopy number per cell\nConcentration [nM]\nRelative abundance, calculated either as fraction of the cumulative mass of a protein to the total cumulative mass, or as the fraction of the cumulative number of molecules of one protein to the total number of protein molecules.\nCopy number rank, with 1 representing the most highly abundant protein\nRelative copy number rank, with values ranging from 0-1. This value is useful for ‘s curve’ plots of rank vs. logarithmized copy numbers.\nSample summaries, either in the form of row annotations in the output matrix, or a separate matrix."
  },
  {
    "objectID": "Download_Installation.html",
    "href": "Download_Installation.html",
    "title": "Download & Installation",
    "section": "",
    "text": "Downloading and using the software is free of charge.\nSimply download from here and unpack the compressed file MaxQuant.zip."
  },
  {
    "objectID": "Download_Installation.html#sec-mq-windows",
    "href": "Download_Installation.html#sec-mq-windows",
    "title": "Download & Installation",
    "section": "2.1 MaxQuant on Windows",
    "text": "2.1 MaxQuant on Windows\nSupported operation system versions (64 bit is required) are Windows 10 or 11 or Windows Server 2016, 2019, 2022.\n\nInstall .NET Core 7.0 or higher: To find out whether you already have it, you can either open the following path\n\nC:\\Program Files\\dotnet\\sdk\nor run the following command in the terminal (Win+R -&gt; cmd):\ndotnat --version\nIf you need to, you can download the software and get installation instructions at the for .NET 7.0 here.\n.NET 3.1 and older version are not supported anymore.\n\nRun MaxQuant GUI Double click on MaxQuant.exe in the MaxQuant folder and specify your RAW files, experimental design and fasta files.\nStart MaxQuant Click on the Start button."
  },
  {
    "objectID": "Download_Installation.html#maxquant-on-linux",
    "href": "Download_Installation.html#maxquant-on-linux",
    "title": "Download & Installation",
    "section": "2.2 MaxQuant on Linux",
    "text": "2.2 MaxQuant on Linux\nWe are supporting MaxQuant on Ubuntu 20.04 or higher, but MaxQuant should also work on another distribution of Linux. You may run MaxQuant using Graphical User Interface. In this case, you should install Mono and follow instructions in Section 2.1 for MaxQuant on Windows starting from second point. This tutorial is focused on running MaxQuant using command line.\n\nInstall .NET Core 7.0 To find out whether you already have it, type in the command line\n\ndotnet --version\nIf you see at least version \\(7.0.407\\), then everything is ready to start MaxQuant.\nOtherwise you need to follow the installation instructions at .NET Core SDK 7.0 for your operating system.\nUbuntu 22.04 (for example)\nwget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\nsudo dpkg -i packages-microsoft-prod.deb\nsudo apt-get update; \\\nsudo apt-get install -y apt-transport-https && \\\nsudo apt-get update && \\\nsudo apt-get install -y dotnet-sdk-2.1\n\nEdit mqpar.xml file\n\nCurrently we highly recommend to preconfigure the mqpar.xml file in MaxQuant GUI. After transferring the file on Linux machine, do not forget to update file addresses accordingly. You can also use a programmatic way to change that.\ndotnet MaxQuant/bin/MaxQuantCmd.exe --changeFolder=new_mqpar.xml &lt;new folder with fasta files&gt; &lt;new folder with raw files&gt; old_mqpar.xml\nHowever if you feel familiar with a structure of mqpar.xml, you may create a template of mqpar.xml and edit it directly.\ndotnet MaxQuant/bin/MaxQuantCmd.exe --create new_mqpar.xml\n\nRun MaxQuant\n\ndotnet MaxQuant/bin/MaxQuantCmd.exe mqpar.xml"
  },
  {
    "objectID": "cyclicannotation.html",
    "href": "cyclicannotation.html",
    "title": "Cyclic Annotation enrichment",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Time series\nSource code: not public."
  },
  {
    "objectID": "cyclicannotation.html#phase-column",
    "href": "cyclicannotation.html#phase-column",
    "title": "Cyclic Annotation enrichment",
    "section": "3.1 Phase column",
    "text": "3.1 Phase column\nSelected numerical column that contains information about the phases of each protein (default: first numerical column in the matrix)."
  },
  {
    "objectID": "cyclicannotation.html#benj.-hochb.-fdr",
    "href": "cyclicannotation.html#benj.-hochb.-fdr",
    "title": "Cyclic Annotation enrichment",
    "section": "3.2 Benj. Hochb. FDR",
    "text": "3.2 Benj. Hochb. FDR\nApplied Benjamini-Hochberg correction for multiple hypotheses testing to find significantly enriched annotation terms (default: 0.05)."
  },
  {
    "objectID": "cyclicannotation.html#in-vivo-quantitative-proteomics-reveals-a-key-contribution-of-post-transcriptional-mechanisms-to-the-circadian-regulation-of-liver-metabolism",
    "href": "cyclicannotation.html#in-vivo-quantitative-proteomics-reveals-a-key-contribution-of-post-transcriptional-mechanisms-to-the-circadian-regulation-of-liver-metabolism",
    "title": "Cyclic Annotation enrichment",
    "section": "5.1 In-Vivo Quantitative Proteomics Reveals a Key Contribution of Post-Transcriptional Mechanisms to the Circadian Regulation of Liver Metabolism",
    "text": "5.1 In-Vivo Quantitative Proteomics Reveals a Key Contribution of Post-Transcriptional Mechanisms to the Circadian Regulation of Liver Metabolism\nThe algorithms were first applied in 2014 by Robles et. al.1.\nAbstract\nCircadian clocks are endogenous oscillators that drive the rhythmic expression of a broad array of genes, orchestrating metabolism and physiology. Recent evidence indicates that post-transcriptional and post-translational mechanisms play essential roles in modulating temporal gene expression for proper circadian function, particularly for the molecular mechanism of the clock. Due to technical limitations in large-scale, quantitative protein measurements, it remains unresolved to what extent the circadian clock regulates metabolism by driving rhythms of protein abundance. Therefore, we aimed to identify global circadian oscillations of the proteome in the mouse liver by applying in vivo SILAC mouse technology in combination with state of the art mass spectrometry. Among the 3000 proteins accurately quantified across two consecutive cycles, 6% showed circadian oscillations with a defined phase of expression. Interestingly, daily rhythms of one fifth of the liver proteins were not accompanied by changes at the transcript level. The oscillations of almost half of the cycling proteome were delayed by more than six hours with respect to the corresponding, rhythmic mRNA. Strikingly we observed that the length of the time lag between mRNA and protein cycles varies across the day. Our analysis revealed a high temporal coordination in the abundance of proteins involved in the same metabolic process, such as xenobiotic detoxification. Apart from liver specific metabolic pathways, we identified many other essential cellular processes in which protein levels are under circadian control, for instance vesicle trafficking and protein folding. Our large-scale proteomic analysis reveals thus that circadian post-transcriptional and post-translational mechanisms play a key role in the temporal orchestration of liver metabolism and physiology."
  },
  {
    "objectID": "processtextcolumns.html",
    "href": "processtextcolumns.html",
    "title": "Process text column",
    "section": "",
    "text": "===== General =====\n===== Brief description =====\nValues in string columns can be manipulated according to a regular expression."
  },
  {
    "objectID": "processtextcolumns.html#columns",
    "href": "processtextcolumns.html#columns",
    "title": "Process text column",
    "section": "1.1 Columns",
    "text": "1.1 Columns\nSelected text columns, whose values should be manipulated by the defined regular expression (default: no columns are selected)."
  },
  {
    "objectID": "processtextcolumns.html#regular-expression",
    "href": "processtextcolumns.html#regular-expression",
    "title": "Process text column",
    "section": "1.2 Regular expression",
    "text": "1.2 Regular expression\nSpecified regular expression that is applied to the selected text columns (default: “^([^;]+)”).\nA regular expression is a sequence of characters that forms a search pattern with a special syntax. A good general introduction can be found, as always, on Wikipedia. If you already know generally how regular expressions work, you may only need to glance at the quick reference or at an even quicker one.\nHere are a few examples:\n\n\n\n\n\n\n\nRegular expression\nEffect\n\n\n\n\n^([^;]+)\nSelect all the characters from the beginning of the line, up to but not including the first semicolon. This is the default.\n\n\nTAG = ([^,; ]*)\nLook for the first instance of “TAG =”, with any amount of whitespace (or none) around the equal sign, and return what follows after the whitespace until a comma or semicolon is reached.\n\n\n([ACTG]+)\nreturn the first string consisting only of the letters A, C, T, and G.\n\n\n(20[01][0-9]-[01][0-9]-[0-3][0-9])\nSelect a date between 2000 and 2019 of the form 2014-08-19."
  },
  {
    "objectID": "processtextcolumns.html#replacement-string",
    "href": "processtextcolumns.html#replacement-string",
    "title": "Process text column",
    "section": "1.3 Replacement string",
    "text": "1.3 Replacement string\nYou can provide a replacement string here for more flexibility. Leave empty if unsure.\nExamples:\n\n\n\n\n\n\n\nRegular expression\nEffect\n\n\n\n\n$1\nReplace the original string with the first capture group, i.e. the part of the original string inside the first parentheses ’‘(..)’’."
  },
  {
    "objectID": "processtextcolumns.html#keep-original-columns",
    "href": "processtextcolumns.html#keep-original-columns",
    "title": "Process text column",
    "section": "1.4 Keep original columns",
    "text": "1.4 Keep original columns\nIf checked, the original columns are retained unchanged, and new columns are appended to hold the results (default: unchecked). The name of a new column is created by appending underscores to the name of the original column until it is unique. If this box is not checked, then the strings in the original columns are overwritten by the results."
  },
  {
    "objectID": "processtextcolumns.html#strings-separated-by-semicolons-are-independent",
    "href": "processtextcolumns.html#strings-separated-by-semicolons-are-independent",
    "title": "Process text column",
    "section": "1.5 Strings separated by semicolons are independent",
    "text": "1.5 Strings separated by semicolons are independent\nIf checked, each string is split into substrings at the semicolons, and the regular expression is applied independently to each substring (default: unchecked). The results are separated by semicolons and concatenated into a single string, which is returned. This is useful for columns where any row may contain multiple entries. If not checked, the string is evaluated as a whole and the only first match returned."
  },
  {
    "objectID": "numericvenndiagram.html",
    "href": "numericvenndiagram.html",
    "title": "Numeric venn diagram",
    "section": "",
    "text": "Type: - Matrix Analysis\nHeading: - Misc. (Analysis)\nSource code: not public."
  },
  {
    "objectID": "numericvenndiagram.html#columns",
    "href": "numericvenndiagram.html#columns",
    "title": "Numeric venn diagram",
    "section": "3.1 Columns",
    "text": "3.1 Columns\nSelected expression columns, for which a numeric Venn diagram should be calculated (default: all expression columns are selected)."
  },
  {
    "objectID": "renamecolumns.html",
    "href": "renamecolumns.html",
    "title": "Rename Columns",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Rearrange\nSource code: RenameColumns.cs\n\n===== Brief description =====\nNew names can be specified for each expression column. The new names are typed in explicitly.\nOutput: Same matrix but with the new expression column names.\n\n\n2 Parameters\nFor each column in the matrix regardless of the type, the name can be changed manually by typing the new name for the column in the predefined text field.\n\n\n3 Parameter window\n\n{{perseus:user:activities:matrixprocessing:rearrange:rearrange-rename_columns-edited.png?direct|Perseus pop-up window: Rearrange -&gt; Rename columns}}"
  },
  {
    "objectID": "transformationproc.html",
    "href": "transformationproc.html",
    "title": "Transformation processes",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: not public.\n\n===== Brief description =====\nAll values in the specified columns are transformed according to the given formula.\nOutput: The output matrix has the same structure as the input matrix. Values in the selected columns will be transformed."
  },
  {
    "objectID": "transformationproc.html#transformation",
    "href": "transformationproc.html#transformation",
    "title": "Transformation processes",
    "section": "2.1 Transformation",
    "text": "2.1 Transformation\nThe specified transformation formula (default: \\(log_2(x)\\)) that should be applied to all cells in the selected columns. The variable name is \\(x\\). Numbers with \\(.\\) are reagrded decimal point, \\(+\\), \\(-\\), \\(*\\), \\(/\\) and ^ can be used as well as scientific notation (e.g. \\(2.9e^{-15}\\)) is supported. Predefined functions can also be used, whose argument has to be enclosed by round brackets, e.g. \\(sin(2*x)\\)."
  },
  {
    "objectID": "transformationproc.html#columns",
    "href": "transformationproc.html#columns",
    "title": "Transformation processes",
    "section": "2.2 Columns",
    "text": "2.2 Columns\nSelected expression and/or numerical columns, where the transformation should be applied (default: all columns are selected).\n#Parameter window\n\n\n\nPerseus pop-up window: Basic -&gt; Transform"
  },
  {
    "objectID": "perseus_instructions.html",
    "href": "perseus_instructions.html",
    "title": "Perseus",
    "section": "",
    "text": "The Perseus software platform supports biological and biomedical researchers in interpreting protein quantification, interaction and post-translational modification data. Perseus contains a comprehensive portfolio of statistical tools for high-dimensional omics data analysis covering normalization, pattern recognition, time-series analysis, cross-omics comparisons and multiple-hypothesis testing (for an overview see Figure 1). A machine learning module supports the classification and validation of patient groups for diagnosis and prognosis, and it also detects predictive protein signatures. Central to Perseus is a user-friendly, interactive workflow environment that provides complete documentation of computational methods used in a publication(Tyanova, Temu, and Cox 2016)."
  },
  {
    "objectID": "perseus_instructions.html#report-a-bug",
    "href": "perseus_instructions.html#report-a-bug",
    "title": "Perseus",
    "section": "4.1 Report a bug",
    "text": "4.1 Report a bug\nFor question we would love to hear from you under Contact"
  },
  {
    "objectID": "combineexpressioncolumns.html",
    "href": "combineexpressioncolumns.html",
    "title": "Combine Expression Columns",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Basic (Processing)\nSource code: not public."
  },
  {
    "objectID": "combineexpressioncolumns.html#operation",
    "href": "combineexpressioncolumns.html#operation",
    "title": "Combine Expression Columns",
    "section": "3.1 Operation",
    "text": "3.1 Operation\nSpecifies the operation (default: \\(x-y\\)), how a pair of columns should be combined into a single column. Numbers with \\(.\\) as decimal point, \\(+\\), \\(-\\), \\(*\\), \\(/\\) and ^ as well as scientific notation (e.g. \\(5.4e{-12}\\)) can be used. A set of predefined functions are also available, whose argument has to be enclosed by round brackets, e.g. \\(sin(2*x-y)\\)."
  },
  {
    "objectID": "combineexpressioncolumns.html#x",
    "href": "combineexpressioncolumns.html#x",
    "title": "Combine Expression Columns",
    "section": "3.2 x",
    "text": "3.2 x\nSelected expression columns that are the first partners of pairs of columns (default: no expression columns are selected). The number of selected columns in the “x” and “y” field must be the same."
  },
  {
    "objectID": "combineexpressioncolumns.html#y",
    "href": "combineexpressioncolumns.html#y",
    "title": "Combine Expression Columns",
    "section": "3.3 y",
    "text": "3.3 y\nSelected expression columns that are the second partners of pairs of columns (default: no expression columns are selected). The number of selected columns in the “x” and “y” field must be the same."
  },
  {
    "objectID": "combineexpressioncolumns.html#keep-original-columns",
    "href": "combineexpressioncolumns.html#keep-original-columns",
    "title": "Combine Expression Columns",
    "section": "3.4 Keep original columns",
    "text": "3.4 Keep original columns\nIf checked the original columns will be retained in the output matrix (default: checked)."
  },
  {
    "objectID": "viewer_gettingStarted.html",
    "href": "viewer_gettingStarted.html",
    "title": "Viewer - Getting started",
    "section": "",
    "text": "The Viewer module can either be used to get some prior information out of loaded raw files or to find some follow up hypotheses after raw files have already been processed.\n\n1 Raw files prior processing\nTo view MS spectra of raw files, load the files using the Load (to load single files) or Load folder (to load a whole folder) options.\n\n\n\nFigure 1: Load raw files\n\n\n\n\n2 Raw files post processing with MaxQuant\nFile organization is important for the correct display of annotated spectra. Raw files should be in the same folder as all index files, the mqpar.xml file and all output folders created by MaxQuant during the processing (that includes the combined folder). The easiest way to load the entire data is to load the mqpar.xml file. Make sure that the file paths are correct if using this option.\n\n\n\nFigure 2: Load mqpar.xml"
  },
  {
    "objectID": "combinebyidentifiersprocessing.html",
    "href": "combinebyidentifiersprocessing.html",
    "title": "Combine rows by identifiers",
    "section": "",
    "text": "#General"
  },
  {
    "objectID": "combinebyidentifiersprocessing.html#id-column",
    "href": "combinebyidentifiersprocessing.html#id-column",
    "title": "Combine rows by identifiers",
    "section": "2.1 ID column",
    "text": "2.1 ID column\nSelected text column containing the IDs that are going to be clustered (default: first text column in the matrix)"
  },
  {
    "objectID": "combinebyidentifiersprocessing.html#keep-rows-without-id",
    "href": "combinebyidentifiersprocessing.html#keep-rows-without-id",
    "title": "Combine rows by identifiers",
    "section": "2.2 Keep rows without ID",
    "text": "2.2 Keep rows without ID\nIf checked, rows without ID are kept and each as a separate row (default: unchecked)."
  },
  {
    "objectID": "combinebyidentifiersprocessing.html#average-type-for-expression-columns",
    "href": "combinebyidentifiersprocessing.html#average-type-for-expression-columns",
    "title": "Combine rows by identifiers",
    "section": "2.3 Average type for expression columns",
    "text": "2.3 Average type for expression columns\nDefinition how numeric values in expression columns of clustered rows should be averaged (default: Median). The average operation can be selected from a predefined list:\n\nSum\nMean\nMedian\nMaximum\nMinimum"
  },
  {
    "objectID": "shortenmotifs.html",
    "href": "shortenmotifs.html",
    "title": "Shorten motif length",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Modifications\nSource code: ShortenMotifs.cs"
  },
  {
    "objectID": "shortenmotifs.html#sequence-window",
    "href": "shortenmotifs.html#sequence-window",
    "title": "Shorten motif length",
    "section": "3.1 Sequence window",
    "text": "3.1 Sequence window\nSelected text column that contains the amino acid sequences (default: first text column in the matrix)."
  },
  {
    "objectID": "shortenmotifs.html#start",
    "href": "shortenmotifs.html#start",
    "title": "Shorten motif length",
    "section": "3.2 Start",
    "text": "3.2 Start\nStart position of the newly generated shorter sequences, which are displayed in a new generated text column called “Short sequence window” (default: 6). The flanks will be measured with respect to this position.\nHint: Start + Length cannot exceed the length of any protein sequence in the selected text column."
  },
  {
    "objectID": "shortenmotifs.html#length",
    "href": "shortenmotifs.html#length",
    "title": "Shorten motif length",
    "section": "3.3 Length",
    "text": "3.3 Length\nLength of the newly generated sequences (default: 11). Flanking regions of this length will be kept surrounding the central position."
  },
  {
    "objectID": "filtertextualcolumn.html",
    "href": "filtertextualcolumn.html",
    "title": "Filter rows based on text column",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Filter rows\nSource code: FilterTextualColumn.cs"
  },
  {
    "objectID": "filtertextualcolumn.html#column",
    "href": "filtertextualcolumn.html#column",
    "title": "Filter rows based on text column",
    "section": "3.1 Column",
    "text": "3.1 Column\nSelected text column the filtering is based on (default: first text column in the matrix)."
  },
  {
    "objectID": "filtertextualcolumn.html#search-string",
    "href": "filtertextualcolumn.html#search-string",
    "title": "Filter rows based on text column",
    "section": "3.2 Search string",
    "text": "3.2 Search string\nSpecified text string that should be searched in the previously defined text column (default: empty)."
  },
  {
    "objectID": "filtertextualcolumn.html#match-case",
    "href": "filtertextualcolumn.html#match-case",
    "title": "Filter rows based on text column",
    "section": "3.3 Match case",
    "text": "3.3 Match case\nIf checked, the cells of the text column will be searched for a matching substring (default: unchecked). The results will be in a new generated categorical column called “Search: original column name”. “+” indicates, whether a match was successful."
  },
  {
    "objectID": "filtertextualcolumn.html#match-whole-word",
    "href": "filtertextualcolumn.html#match-whole-word",
    "title": "Filter rows based on text column",
    "section": "3.4 Match whole word",
    "text": "3.4 Match whole word\nIf checked, the text column will be searched to match the whole word of the specified term (default: checked). The results will be in a new generated categorical column called “Search: original column name”. “+” indicates, whether a match was successful."
  },
  {
    "objectID": "filtertextualcolumn.html#mode",
    "href": "filtertextualcolumn.html#mode",
    "title": "Filter rows based on text column",
    "section": "3.5 Mode",
    "text": "3.5 Mode\nThe rows with the selected values will be kept/discarded depending on the selected “Mode” (default: “Remove matching rows”). If “Remove matching rows” is selected, rows having the values will be removed while all other rows will be kept. If “Keep matching rows” is selected, the opposite will happen."
  },
  {
    "objectID": "filtertextualcolumn.html#filter-mode",
    "href": "filtertextualcolumn.html#filter-mode",
    "title": "Filter rows based on text column",
    "section": "3.6 Filter mode",
    "text": "3.6 Filter mode\nThe “Filter mode” defines, whether the input matrix will be reduced (“Reduce matrix” = default) or a new categorical column called “Filter” will be generated containing the categories “Keep” and “Discard” (“Filter mode” = “Add categorical column”)."
  },
  {
    "objectID": "ClassificationParameterOptimization.html",
    "href": "ClassificationParameterOptimization.html",
    "title": "Classification parameter optimization",
    "section": "",
    "text": "Type: - Matrix Processing\nHeading: - Learning\nSource code: not public."
  },
  {
    "objectID": "ClassificationParameterOptimization.html#items-are-in",
    "href": "ClassificationParameterOptimization.html#items-are-in",
    "title": "Classification parameter optimization",
    "section": "3.1 Items are in",
    "text": "3.1 Items are in\nIt specifies if the items that should be used for the cross-validation or the prediction can be found in “Columns” or “Rows” (default: Columns).\n\n3.1.1 Classes\nSelected categorical row or column that contains the class of the items (default: first categorical row/column in the matrix). If items are in columns then the classes are in a categorical row, and if items are in rows the classes are in a categorical column.\n\n\n3.1.2 Sub-classes\nThis parameter is just relevant, if the parameter “Items are in” is set to “Columns”. It specifies whether sub-classes should be taken into consideration for the classification process (default: &lt;None&gt;)."
  },
  {
    "objectID": "ClassificationParameterOptimization.html#classification-algorithm",
    "href": "ClassificationParameterOptimization.html#classification-algorithm",
    "title": "Classification parameter optimization",
    "section": "3.2 Classification algorithm",
    "text": "3.2 Classification algorithm\nDefines the algorithm that should be used for the classification (default: Support vector machine). The algorithm can be selected from a predefined list:\n\nSupport vector machine\nFisher LDA\nKNN\n\n\n3.2.1 Kernel\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “Support vector machine”. It defines the kernel function that is used to classify items (default: linear). The kernel function can be selected from a predefined list:\n \\[\\begin{align}\n\\text{Linear:} \\ K(x,y) &= x^{T}y \\\\\n\\text{RBF:}    \\  K(x,y) &= \\exp (-\\sigma\\|x-y\\|^2) , \\sigma \\&gt; 0 \\\\\n\\text{Polynomial:} \\ K(x,y) &= (\\gamma x^{T}y + r)^d , \\gamma \\&gt; 0 \\\\\n\\text{Sigmoid:} \\  K(x,y) &= tanh(\\gamma x^{T}y + r) \\\\\n\\end{align}\\] \nDepending on the chosen function 1 to 4 parameters must be specified.\n\n3.2.1.1 Sigma\nThis parameter is just relevant, if “Kernel” is set to “RBF”. It defines the slope of the function (see formula above, default: 1).\n\n\n3.2.1.2 Degree\nThis parameter is just relevant, if “Kernel” is set to “Polynomial”. It defines the degree of the polynom (see formula above, default: 3).\n\n\n3.2.1.3 Gamma\nThis parameter is just relevant, if “Kernel” is set to “Polynomial” or “Sigmoid”. It defines the slope of the function (see formula above, default: 0.01).\n\n\n3.2.1.4 Coef\nThis parameter is just relevant, if “Kernel” is set to “Polynomial” or “Sigmoid”. It defines a constant (see formula above, default: 0).\n\n\n3.2.1.5 C\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “Support vector machine”. C is a penalty constant (default: 10). Large C corresponds to large penalties for misclassification and resembles a hard margin classifier.\n\n\n\n3.2.2 Distance\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “KNN”. It defines the selected distance that will be used to assign the nearest neighbours to an item and therefore classify it (default: Euclidean). The distance can be selected from a predefined list:\n\nEuclidean\nL1\nMaximum\nLp\nPearson correlation\nSpearman correlation\nCosine\nCanberra\n\n\n\n3.2.3 Number of neighbours\nThis parameter is just relevant, if the parameter “Classification algorithm” is set to “KNN”. It specifies the number of closest neighbours that are taken into account for the classification of an item (default: 5).\n\n\n3.2.4 Feature selection\nDefines whether feature selection should be applied by ranking and reducing the features before the classification process (default: None).\n\n\n3.2.5 Feature ranking method\nThis parameter is just relevant, if the parameter “Feature selection” is set to “From feature ranking”. It specifies which features method will be used to rank the features (default: ANOVA). The method can be selected from a predefined list:\n\nANOVA\nHybrid SVM\nMANOVA\nOne-sided t-test\nTwo-way ANOVA\nSVM\nRFE-SVM\nGolub\n\nDepending on the ranking method up to 4 parameters can be specified.\n\n3.2.5.1 S0\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “ANOVA”, “Hybrid SVM”, “One-sided t-test” or “MANOVA”. It defines the artificial within groups variance and controls the relative importance of resulted test p-values and difference between means (default: 0). At s0=0 only the p-value matters, while at nonzero s0 also the difference of means plays a role. See1 for details.\n\n\n3.2.5.2 C\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Hybrid SVM”, “SVM” or “RFE-SVM”. C is a penalty constant (default: 100). Large C corresponds to large penalties for misclassification and resembles a hard margin classifier.\n\n\n3.2.5.3 Reduction factor\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Hybrid SVM” or “RFE-SVM”. It defines the factor by what the number of features will be reduced step by step during the ranking process (default: 1.414).\n\n\n3.2.5.4 Number of top ANOVA features\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “MANOVA”. It defines how many of the selected features are top ANOVA features.\n\n\n3.2.5.5 Side\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “One-sided t-test”. It defines the “Left” or “Right” side, where the null hypothesis can be rejected (default: Right).\n\n\n3.2.5.6 Orthogonal grouping\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. It defines the grouping of the data according to a given categorical column or row to distinguish the effects of the groups.\n\n\n3.2.5.7 Min. orthogonal p-value\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. Test results above this p-value are defined as orthogonal (default: 0).\n\n\n3.2.5.8 Min. interaction p-value\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. Test results above this p-value are defined as interacting, hence the effects of one group do not depend on the other group (default: 0).\n\n\n3.2.5.9 Skip if orthog. P-value is better\nThis parameter is just relevant, if the parameter “Feature ranking method” is set to “Two-way ANOVA”. It defines whether features with an orthogonal p-value better than the given value in “Min. interaction p-value” are filtered out (default: unchecked).\n\n\n3.2.5.10 Number of features\nDefines how many features should be selected (default: 100).\n\n\n3.2.5.11 Group-wise feature sel.\nIf checked, for each defined group in the data a different amount of features can be selected, which are then used for the classification (default: unchecked). The numbers can be defined either by typing in the text field in the form [Group,number] or by using the Edit button."
  },
  {
    "objectID": "ClassificationParameterOptimization.html#parameter-scan-type",
    "href": "ClassificationParameterOptimization.html#parameter-scan-type",
    "title": "Classification parameter optimization",
    "section": "3.3 Parameter scan type",
    "text": "3.3 Parameter scan type\nDefines whether a “One-dimensional scan” or a “Two-dimensional scan” should be applied (default: One-dimensional scan). A “One-dimensional scan” just scans through different parameter values of one parameter, whereas in a “Two-dimensional scan” two parameters can be chosen for which different values will be tested to find the optimal combination.\n\n3.3.1 Parameter\nSelected parameter, for which different values should be used to figure out the best setting for the classification problem. The parameter can be selected from a predefined list including the parameters of the selected classification algorithm as well as the parameters of the selected feature ranking method (default: first parameter available of “Classification algorithm” and “Feature selection”).\n\n\n3.3.2 Number of values\nDefines the number of values that should be tested (default: 5).\n\n\n3.3.3 Step size\nDefines the size by which the parameter should be increased step by step (default: 1). The operation how to increase the parameter values can be defined in “Step type”.\n\n\n3.3.4 Step type\nSpecifies whether the value defined in “Step size” is added to the parameter value to calculate the new value for the parameter, or if the values are multiplied with each other to get the new parameter value (default: Additive)."
  },
  {
    "objectID": "ClassificationParameterOptimization.html#cross-validation-type",
    "href": "ClassificationParameterOptimization.html#cross-validation-type",
    "title": "Classification parameter optimization",
    "section": "3.4 Cross-validation type",
    "text": "3.4 Cross-validation type\nThis parameter is just relevant, if the parameter “Cross-validate assigned items” is checked. It defines the type of cross-validation that should be applied to the data set (default: n-fold). The type can be selected from a predefined list:\n\nLeave one out: As many predictors are built as there are items in the data set. Thus for each predictor one item is left out to train the model and the predictor will be evaluated using the left out item. In the end the average prediction performance will be returned.\nn-fold: The items of the data set are split into n equally sized chunks. n predictors will be generated. In each of these prediction models the union of n-1 of these chunks are taken as the training set and the remaining chunk is the test set. In the end the average prediction performance will be returned.\nRandom sampling: The number of predictors is specified by the “Number of repeats” parameter. The number of items taken out to form the test set (and not used for building the predictor) is specified by the “Test set percentage” parameter. In the end the average prediction performance will be returned.\n\nDepending on the cross-validation type 0 to 2 parameters have to specified:\n\n3.4.1 n\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “n-fold”. It defines the number of partitions the data is divided into (default: 4).\n\n\n3.4.2 Test set percentage\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “Random sampling”. It specifies the percentage of the data that is used for testing the trained model (default: 15). The remaining data is used for the training process.\n\n\n3.4.3 Number of repeats\nThis parameter is just relevant, if the parameter “Cross-validation type” is set to “Random sampling”. It specifies how often the cross-validation process is repeated (default: 250). In every round the data is again divided according to the previously defined percentage."
  },
  {
    "objectID": "ClassificationParameterOptimization.html#number-of-threads",
    "href": "ClassificationParameterOptimization.html#number-of-threads",
    "title": "Classification parameter optimization",
    "section": "3.5 Number of threads",
    "text": "3.5 Number of threads\nDefines the number of threads that should be used for the process (default: 1). The number of threads is limited by number of available cores of the machine Perseus in running on."
  },
  {
    "objectID": "ClassificationParameterOptimization.html#support-vector-machines",
    "href": "ClassificationParameterOptimization.html#support-vector-machines",
    "title": "Classification parameter optimization",
    "section": "5.1 Support vector machines",
    "text": "5.1 Support vector machines\nSupport vector machines (SVMs) were largely developed in the 1990s by Vapnik and co-workers on a basis of a separable bipartition problem at the AT & T Bell Laboratories (see2. SVMs are a family of data analysis algorithms, based on convex quadratic programming, whose successful use has been demonstrated in classification, regression and clustering problems. Thus, SVMs are now the state-of-the-art tools for non-linear input-output knowledge. The following section covers a brief and basic description of SVMs, but detailed explanations can be found in V. N. Vapniks3, N. Cristianinis and J. Shawe-Taylors4, V. N. Vapniks5, V. N. Vapniks6, B. E. Bosers, I. M. Guyons, and V. N. Vapniks7.\nSVMs are a particular class of supervised learning methods that are well suited for analyses of data in high-dimensional feature spaces. They are computationally efficient and capable of detecting biologically-relevant signals. SVMs revolve around the notion of a margin - either side of a data separating linear decision boundary (hyperplane). Maximizing this margin and thereby creating the largest distance between two classes as well as between the hyperplane and the instances on either side, is the main task in training SVMs (see figure below). Thus, these models have a binary nature to separate classes, but can be extended to multi-class problems by reducing the problem to a set of multiple binary classification problems. The hyperplane is defined by:\n \\[\\begin{align}\nD(x)  =  &lt;\\omega,x&gt;  +  b\n\\end{align}\\] \nwhere \\(ω\\) is the weights vector and \\(b\\) is a bias value (or \\(−b\\) the threshold).\nIn case an optimal separating hyperplane is found, data points on the margin are known as support vectors and the solution is a linear combination of them (red data points in figure below). Each new data point is then classified according to its optimal position relative to the model’s hyperplane. So the model complexity is unaffected by the number of features encountered in the training data, therefore SVMs are well suited to deal with learning tasks with a large number of features compared to the number of data points. In case no hyperplane can be found, the problem can be addressed using the so-called soft margin. The margin optimization constraints can be relaxed by allowing some misclassifications or margin violations in the training set, to get better generalization of the SVM than using a hard margin. The choice of appropriate penalties is mandatory:\n \\[\\begin{align}\nmin_{\\omega,b,\\xi} ~& \\frac{1}{2} \\ \\omega^{T}\\omega \\ + \\ C\\sum_{i=1}^{l}\\xi_{i} \\\\\n\\text{subject to} ~& y_{i}(\\omega^{T}x_{i}+b) \\ &lt; \\ 1-\\xi_{i} \\ \\ \\text{and} \\ \\ \\xi \\geq 0\n\\end{align}\\] \nwhere \\(\\omega\\) is the weights vector, \\(b\\) is a bias value, \\(C\\) is a penalty constant, and \\(\\xi\\) is a slack variable, which is the orthogonal distance between a data point and the hyperplane. Large C correspond to large penalties for misclassification and resemble a hard margin classifier, whereas \\(\\xi\\) measures the degree of misclassification or margin violation. This is a good way to deal with outliers in the data set without destroying the model by tailoring it perfectly to the input data.\nNevertheless, most real-world data sets involve separation problems that are linearly non-separable, which requires the definition of complex functions to build a good classifier. SVMs use kernels, a special class of functions to deal with such situations. Mapping the data points to a higher-dimensional space (transformed feature space) using kernels, enables the definition of a linear hyperplane, which results in a non-linear hyperplane in the original space. The hyperplanes in the higher-dimensional space are represented by all points defining a set, whose inner product with a vector is constant in that space. Training the classifier depends only on the data through dot products, which are possible to compute even at a high-dimension at low cost by applying the so-called kernel trick. The trick lies in working in an higher-dimensional space, without ever explicitly transforming the original data points into that space, but instead relying on algorithms that only need to compute inner products within that space. These algorithms are identical to kernels and can thus be cheaply computed in the original space. So, everything about linear cases can also be applied to non-linear ones using an appropriate kernel function. It is common practice to find the best suiting function by cross-validation. Some popular kernels, which are all included in Perseus, are:\n \\[\\begin{align}\n\\text{linear:} \\ K(x,y)         &= x^{T}y  \\\\\n\\text{sigmoid:} \\ K(x,y)    &= tanh(\\gamma x^{T}y \\ + \\ r) \\\\\n\\text{radial basis:} \\ K(x,y)   &= \\exp(-\\gamma|x \\ - \\ y|^{2}) , \\ \\gamma &gt; 0 \\\\\n\\text{polynomial:} \\ K(x,y)     &= (\\gamma x^{T}y \\ + \\ r)^{d}, \\ \\gamma &gt; 0\n\\end{align}\\] \nwhere \\(x\\) and \\(y\\) are two data points, \\(\\gamma\\) is the slope, \\(d\\) is the degree of the polynom, and \\(r\\) is a constant.\n\n\n\nIllustration of separating two classes using SVMs. Linear (A.) and non-linear (B.) perfect separation of two classes (green and orange) with a hyperplane (black) and maximal margin (blue and dotted gray lines). Support vectors defining the hyperplane are in red. No misclassifiactions or margin violations are included.\n\n\nFor more information you can also consult Wikipedia."
  },
  {
    "objectID": "ClassificationParameterOptimization.html#fishers-linear-discriminant-analysis",
    "href": "ClassificationParameterOptimization.html#fishers-linear-discriminant-analysis",
    "title": "Classification parameter optimization",
    "section": "5.2 Fisher’s linear discriminant analysis",
    "text": "5.2 Fisher’s linear discriminant analysis\nLinear Discriminant Analysis (LDA), is a well-known classification technique that has been used successfully in many statistical pattern recognition problems. It was developed by R.A. Fisher, a professor of statistics at University College London, and is sometimes called Fisher Discriminant Analysis (FDA). Its first description was in 1936 and can be found in8.\nThe primary purpose of LDA is to separate samples of two or multiple distinct groups while preserving as much of the class discriminatory information as possible to classify new unseen instances. The approach of the LDA is to project all the data points into new space, normally of lower dimension, which maximizes the between-class separability while minimizing their within-class variability. So the goal is to find the best projection axes for separating the classes. In general the number of axes that can be computed by the LDA method is one less than the number of classes in the problem.\n\n\n\nIllustration of separating two classes using LDA. Classes are separated perfectly and the dimensionality of the problem has been reduced from two features (x1,x2) to only a scalar value y.\n\n\nFor more information you can also consult Wikipedia."
  },
  {
    "objectID": "ClassificationParameterOptimization.html#k-nearest-neighbors",
    "href": "ClassificationParameterOptimization.html#k-nearest-neighbors",
    "title": "Classification parameter optimization",
    "section": "5.3 k-nearest neighbors",
    "text": "5.3 k-nearest neighbors\nK-Nearest Neighbors (kNN) is a simple lazy learner algorithm that stores all available data points and classifies new instances based on a similarity measure (e.g., distance functions). It corresponds to the group of supervised learning algorithms and has been used in statistical estimation and pattern recognition already in the beginning of 1970’s as a non-parametric technique. During the training phase the algorithm simply stores the data points including their class labels and all computation is deferred until the classification process. So kNN is based on the principle that instances that are in close proximity to another have similar properties. Thus, to classify new unclassified instances, one simply has to look at their k-nearest neighbors, to figure out the classification label. The class membership can be defined by a majority vote of the k closest neighbors or the neighbors can be ranked and weighted according to their distance to the new instance. A common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\n\n\n\nIllustration of classifying a new item using kNN. Using a majority vote of the k nearest neighbors, the defined k can change the assigned class of the red star. If k = 3 (purple circle) the star corresponds to the blue polygon class, because the three closest neighbors include two blue polygons and one green rectangle. Whereas, if k = 5 (black circle) the star is assigned to the green class, because the five closest neighbors include more green rectangles than blue polygons (three green rectangles vs. two blue polygons).\n\n\nFor more information you can also consult Wikipedia."
  },
  {
    "objectID": "ngsupload.html",
    "href": "ngsupload.html",
    "title": "NGS Upload",
    "section": "",
    "text": "Type: - Matrix Upload\nSource code: not public."
  },
  {
    "objectID": "ngsupload.html#experiment",
    "href": "ngsupload.html#experiment",
    "title": "NGS Upload",
    "section": "4.1 Experiment",
    "text": "4.1 Experiment\nIt specifies the type of NGS experiment the data is derived from (default: RNA sequencing and ribosome profiling)."
  },
  {
    "objectID": "ngsupload.html#files",
    "href": "ngsupload.html#files",
    "title": "NGS Upload",
    "section": "4.2 Files",
    "text": "4.2 Files\nWith the two buttons “Add file” and “Remove file” it is possible to select BAM Binary-sequence Alignment Format files. This format is generally used as a compact way to save an alignment and at the same time allowing efficient random access.\nPerseus doesn’t require an index file."
  },
  {
    "objectID": "ngsupload.html#strand-specificity",
    "href": "ngsupload.html#strand-specificity",
    "title": "NGS Upload",
    "section": "4.3 Strand specificity",
    "text": "4.3 Strand specificity\nIt describes which RNA library preparation method was used. Depending on this, reads will mostly be aligned to the same (sense) or opposite (anti-sense) strand as the feature. Alternatively, reads can be aligned to both strands (not stranded).\nLet’s demonstrate an expected distribution of reads according to an imaginary gene, consisting of one transcript, using each type of strand specificity. All reads marked red are those which we will take into account. Reads marked grey will be excluded from the coverage calculation.\n\nWe rule out reads with a mark because there is no intersection between them and the gene.\nAlso, we skip b marked reads because they have an opposite direction to that which we expect.\nLastly, we eliminate reads which don’t fit with the annotation (c) although such reads can potentially be evidence for another isoform of the gene.\n\n\n\n\nFigure 1: Stranded Single End Reads. Expected distribution of reads in a case of stranded and sense assay\n\n\n\n\n\nFigure 2: Stranded (anti-sense). Expected distribution of reads in a case of stranded and anti-sense assay\n\n\n\n\n\nFigure 3: Not-Stranded reads. Expected distribution of reads in a case of non-stranded assay\n\n\nPaired end sequencing produces two reads from one fragment and they should be from different strands (if reads is aligned to the same strand, we exclude such pairs).\nFor simplicity of visualization let’s represent paired end reads like this (Figure 4)\n\n\n\nFigure 4: Paired-end reads\n\n\nWe represent the paired end read like this for short. Definition of a, b and c marked reads is similar to single end reads’ case.\n\n\n\nFigure 5: Paired End Reads, Stranded (sense) or irst read from the pair is on sense strand. Expected distribution of reads in a case of stranded and sense assay\n\n\n\n\n\nFigure 6: Stranded (anti-sense)or first read is on anti-sense strand. Expected distribution of reads in a case of stranded and anti-sense assay\n\n\n\n\n\nFigure 7: Not-Stranded. Expected distribution of reads in a case of non-stranded assay\n\n\nIt’s worth to notice that for paired end reads the Persues by default calculates number of fragments for each gene, in other words it doesn’t count twice two reads of one pair.\nHint: In case the experimental design isn’t known, we recommend to use “Not stranded” as “Strand specificity”."
  },
  {
    "objectID": "ngsupload.html#genome-annotation",
    "href": "ngsupload.html#genome-annotation",
    "title": "NGS Upload",
    "section": "4.4 Genome annotation",
    "text": "4.4 Genome annotation\nCurrently the plugin supports GTF file format containing coordinates of genome regions for which the coverage will be calculated. We strongly recommend to download an annotation from the ensemble FTP server.\nGTF (General Feature Format): GTF file can start from several browser/track lines (information specific to genome browser) and comment lines (line should begin with the # character). The rest of the file consists of one line per feature, each consists of nine columns\n\nseqname - name of chromosome of scaffold\nsource - name of the program that generated this feature, or the data source (database or project name)\nfeature - the current version of Perseus takes into account just “cds” and “exon” features\nstart - start position of the feature (1-based coordinate)\nend - end position of the feature (1-based coordinate)\nscore - a floating point value\nstrand - valid entries include ‘+’ (forward) or ‘-’ (reverse)\nattributes - a semicolon-separated list of tag-value pairs\n\n“empty” columns are denoted with a “.”. Each line with “cds” and “exon” feature should contain “gene_id” or “transcript_id” tags."
  },
  {
    "objectID": "ngsupload.html#file",
    "href": "ngsupload.html#file",
    "title": "NGS Upload",
    "section": "4.5 File",
    "text": "4.5 File\nSpecified file path to the genome annotation."
  },
  {
    "objectID": "ngsupload.html#feature-type-name",
    "href": "ngsupload.html#feature-type-name",
    "title": "NGS Upload",
    "section": "4.6 Feature type name",
    "text": "4.6 Feature type name\nIt is possible to specify a feature name that will be used (third column of GTF).\nHint: It makes sense to set “Feature type name” parameter to “Exons” for RNA-seq analysis and choose “CDS” for ribosome profiling."
  },
  {
    "objectID": "ngsupload.html#number-of-threads",
    "href": "ngsupload.html#number-of-threads",
    "title": "NGS Upload",
    "section": "4.7 Number of threads",
    "text": "4.7 Number of threads\nSpecifies the number of used threads for uploading NGS data."
  },
  {
    "objectID": "ngsupload.html#calculate-rpkmfpkm",
    "href": "ngsupload.html#calculate-rpkmfpkm",
    "title": "NGS Upload",
    "section": "6.1 Calculate RPKM/FPKM",
    "text": "6.1 Calculate RPKM/FPKM\n\nUpload bam files to Perseus. The Required annotation (gtf file) can be found on ensemble’ FTP server (annotation should be in agreement with version of genome which was used for alignment). For the example we took publically available RNA-seq data of cultured primary human lung fibroblast which done in two replicates GSM759890 and GSM759891\nFilter rows based on categorical ‘Gene biotype’ column\nselect just protein_coding group.\nMake normalization dividing columns by $sum(Normalization) -&gt; Divide $\nBy definition of RPKM, divide previous result by gene length, multiply on \\(10^9\\) and finally take the \\(log_2(Basic -&gt; Combine\\ main\\ columns)\\).\nTo get TPM, normalise columns of RPKM/FPKM by sum and multiply on \\(10^6\\) .\n\n\n\n\nFigure 8: Upload bam files to Perseus"
  },
  {
    "objectID": "maxquant_instructions.html",
    "href": "maxquant_instructions.html",
    "title": "MaxQuant",
    "section": "",
    "text": "MaxQuant is a proteomics software package designed for analyzing large mass-spectrometric data sets. It is specifically aimed at high-resolution MS data. Several labeling techniques as well as label-free quantification are supported. MaxQuant is freely available and can be downloaded from this site. The download includes the search engine Andromeda, which is integrated into MaxQuant as well as the viewer application for inspection of raw data and identification and quantification results. For statistical analysis of MaxQuant output, we offer the Perseus framework.\n\n1 Documentation outline\n\nDownload and installation\nFirst steps in MaxQuant\nViewer\nAndromeda\nOutput Tables\nGoogle groups\nmaxquant Bug reporting\nGlossary\n\nFor additional training, consider attending the annual MaxQuant Summer School. Also watching some MaxQuant videos provides more insight.\n\nFor question we would love to hear from you under Contact\n\n\n2 Bibliography\nFor a deep-dive into the technique and progress behind MaxQuant, many papers of varying degrees of details were published over the last decades. For further reading, you can look at a small list of the more important corner stones of the MaxQuant development history.1 (Note that the paper has a large supplement containing in-depth descriptions of algorithms),2,3,4,5,6 (Note that this paper explains how to run MaxQuant in detail.),7\n\n\n\n\n\nReferences\n\n1. Cox, J. & Mann, M. MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification. Nature Biotechnology 26, 1367–1372 (2008).\n\n\n2. Cox, J., Michalski, A. & Mann, M. Software Lock Mass by Two-Dimensional Minimization of Peptide Mass Errors. Journal of the American Society for Mass Spectrometry 22, 1373–1380 (2011).\n\n\n3. Schaab, C., Geiger, T., Stoehr, G., Cox, J. & Mann, M. Analysis of High Accuracy, Quantitative Proteomics Data in the MaxQB Database. Molecular & Cellular Proteomics 11, M111.014068 (2012).\n\n\n4. Cox, J. et al. Accurate Proteome-wide Label-free Quantification by Delayed Normalization and Maximal Peptide Ratio Extraction, Termed MaxLFQ. Molecular & Cellular Proteomics 13, 2513–2526 (2014).\n\n\n5. Tyanova, S. et al. Visualization of LC-MS/MS proteomics data in MaxQuant. PROTEOMICS 15, 1453–1456 (2015).\n\n\n6. Tyanova, S., Temu, T. & Cox, J. The MaxQuant computational platform for mass spectrometry-based shotgun proteomics. Nature Protocols 11, 2301–2319 (2016).\n\n\n7. Sinitcyn, P. et al. MaxQuant goes Linux. Nature Methods 15, 401–401 (2018)."
  },
  {
    "objectID": "onesampletestprocessing.html",
    "href": "onesampletestprocessing.html",
    "title": "One-sample tests",
    "section": "",
    "text": "1 General\n\nType: - Matrix Processing\nHeading: - Tests\nSource code: not public.\n\n\n\n2 Brief description\nOne sample-test for determining if the mean is significantly different from a fixed value (typically 0).\nOutput: Two numerical columns are added, one containing the p-value, the other containing the difference between the mean and the fixed value (which is usually 0). In addition there is a categorical column added in which it is indicated by a ‘+’ when the row is significant with respect to the specified criteria."
  }
]